{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma 장점\n",
    "\n",
    "- 다양한 모델 제공: Gemma는 Gemma 2B와 Gemma 7B 두 가지 유형으로 제공, 각 유형은 pre-trained 및 instruction-tuned 된 두가지 타입의 모델 제공\n",
    "- 책임감 있는 AI 툴킷: Gemma를 활용하여 안전한 AI 애플리케이션을 제작할 수 있는 가이드와 필수 도구를 제공하는 새로운 책임감 있는 생성형 AI 툴킷이 함께 제공\n",
    "- 프레임워크 지원: Kera JAX, PyTorch, TensorFlow와 같은 주요 프레임워크에서 추론 및 SFT을 위한 툴체인을 제공\n",
    "- 다양한 하드웨어 지원: NVIDIA GPU와 구글 클라우드 TPU 등에 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma를 위한 라이브러리\n",
    "\n",
    "- pip install -q -U transformers==4.38.2\n",
    "- pip install -q -U datasets==2.18.0\n",
    "- pip install -q -U bitsandbytes==0.42.0\n",
    "- pip install -q -U peft==0.9.0\n",
    "- pip install -q -U trl==0.7.11\n",
    "- pip install -q -U accelerate==0.27.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma 기타\n",
    "\n",
    "- 트랜스포머 디코더 쪽이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TrainingArguments\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 허깅페이스 로그인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066d10836f674d589a942e544d724aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_NAME = \"beomi-gemma-ko-2b-01\"\n",
    "TEST_NAME = \"google-gemma-ko-2b-01\"\n",
    "\n",
    "import os\n",
    "import platform\n",
    "\n",
    "# 데이터 관련\n",
    "os_name = platform.system()\n",
    "if os_name == 'Windows':\n",
    "    PRE_PATH = ''\n",
    "elif os_name == 'Linux':\n",
    "    PRE_PATH = '/kkh/'\n",
    "elif os_name == 'Darwin': # 맥\n",
    "    PRE_PATH = '/kkh/'\n",
    "DATA_PATH = PRE_PATH + \"data/\" # 대회에서 제공한 데이터\n",
    "TRAIN_PATH = DATA_PATH + \"train_kkh_new.csv\"\n",
    "VALID_PATH = DATA_PATH + \"dev_kkh_new.csv\"\n",
    "TEST_PATH = DATA_PATH + \"test.csv\"\n",
    "PREDICTION_PATH = PRE_PATH + \"prediction/\" # 최종 예측 값\n",
    "SUBMIT_PATH = PREDICTION_PATH + \"submission_\" + TEST_NAME + \".csv\"\n",
    "\n",
    "BASE_MODEL = \"google/gemma-2b-it\"\n",
    "# BASE_MODEL = \"beomi/gemma-ko-2b\"\n",
    "FINETUNE_MODEL = PRE_PATH + \"gemma_model/gemma-2b-it-sum-ko\"\n",
    "\n",
    "DATASET_TRAIN = load_dataset('csv', data_files={'train': TRAIN_PATH})\n",
    "DATASET_TEST = load_dataset('csv', data_files={'test': TEST_PATH})\n",
    "TRAIN_DATA = DATASET_TRAIN['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fname': 'train_0', 'dialogue': '#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\\n#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\\n#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\\n#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\\n#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\\n#Person2#: 알겠습니다.\\n#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\\n#Person2#: 네.\\n#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \\n#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\\n#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\\n#Person2#: 알겠습니다, 감사합니다, 의사선생님.', 'summary': '스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니다. 호킨스 의사는 스미스씨가 담배를 끊는 데 도움이 될 수 있는 수업과 약물에 대한 정보를 제공할 것입니다.', 'topic': '건강검진 받기'}\n",
      "=============================\n",
      "train 대화문:\n",
      "#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\n",
      "#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\n",
      "#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\n",
      "#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\n",
      "#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\n",
      "#Person2#: 알겠습니다.\n",
      "#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\n",
      "#Person2#: 네.\n",
      "#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \n",
      "#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\n",
      "#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\n",
      "#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n",
      "=============================\n",
      "train 요약문:\n",
      "스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니다. 호킨스 의사는 스미스씨가 담배를 끊는 데 도움이 될 수 있는 수업과 약물에 대한 정보를 제공할 것입니다.\n",
      "=============================\n",
      "test 대화문:\n",
      "#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\n",
      "#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\n",
      "#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\n",
      "#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\n",
      "#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\n",
      "#Person2#: 알겠습니다.\n",
      "#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\n",
      "#Person2#: 네.\n",
      "#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \n",
      "#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\n",
      "#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\n",
      "#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 확인\n",
    "print(TRAIN_DATA[0])\n",
    "print('=============================')\n",
    "print(f\"train 대화문:\\n{TRAIN_DATA[0]['dialogue']}\")\n",
    "print('=============================')\n",
    "print(f\"train 요약문:\\n{TRAIN_DATA[0]['summary']}\")\n",
    "print('=============================')\n",
    "print(f\"test 대화문:\\n{TRAIN_DATA[0]['dialogue']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72339ac47377482396076ac0d8131686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# 모델 자체를 몇 비트로 로딩할 것인지를 정하는 것이다.\n",
    "# 32, 16, 8에 비해 4비트는 메모리를 적게 사용하면서 속도가 빠르지만, 정확도가 떨어진다.\n",
    "# 서버 자원이 적을 경우 주로 사용한다.\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# device_map=\"auto\"는 데이터를 CPU 또는 GPU로 자동 연결해주는 기능이다. GPU가 여러개 이면, 자동 분산도 해준다.\n",
    "# quantization_config=quantization_config 에 대한 설명은 위 코드에 있다.\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Gemma-it의 프롬프트 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\n다음 글을 해설자가 설명하듯이 요약해주세요.:\\n\\n#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\\n#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\\n#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\\n#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\\n#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\\n#Person2#: 알겠습니다.\\n#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\\n#Person2#: 네.\\n#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \\n#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\\n#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\\n#Person2#: 알겠습니다, 감사합니다, 의사선생님.<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = TRAIN_DATA[0]['dialogue']\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"다음 글을 해설자가 설명하듯이 요약해주세요.:\\n\\n{}\".format(ex)\n",
    "    }\n",
    "]\n",
    "\n",
    "# tokenize: 문장을 토큰 단위를 자를 것인지 지정한다.\n",
    "# add_generation_prompt: 다음 발화자가 말할 수 있도록, 프롬프트를 적어준다. 예를 들어, user와 model이 대화를 하는 중이라면, user 발화 끝 부분에 \"<start_of_turn>model\" 과 같이 붙여준다.\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Gemma-it 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nlp/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_k=50,\n",
    "    top_p=0.92\n",
    ")\n",
    "\n",
    "# do_sample=True: 생성할 때 확률적인 샘플링을 사용하여 보다 다양한 출력을 생성합니다. False로 설정 시 가장 높은 확률의 토큰을 선택합니다.\n",
    "# temperature=0.5: 모델의 출력 확률 분포를 조정합니다. 값이 낮을수록 보수적이고, 높을수록 다양하게 생성합니다.\n",
    "# top_k=50: 모델이 선택할 수 있는 상위 50개의 토큰 중 하나를 샘플링합니다.\n",
    "# top_p=0.95: 확률 분포의 상위 95%에 해당하는 토큰을 선택해 생성하는 방식입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**해설:**\n",
      "\n",
      "이 문의에서 주요 내용은 다음과 같습니다.\n",
      "\n",
      "- 건강검진은 5년 동안 받아야 할 수 있는 기회가 있다고 얘기합니다.\n",
      "\n",
      "- 심각한 건강 문제를 예방하기 위해서는 질병을 조기에 발견하는 것이 가장 중요합니다.\n",
      "\n",
      "- 담배는 폐암과 심장병의 주요 원인이므로, 끊임없이 피우는 것이 중요합니다.\n",
      "\n",
      "- 의사에게 의문이 있으면 언제든지 질문해 보세요.\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 학습용 프롬프트 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(example):\n",
    "    prompt_list = []\n",
    "    for i in range(len(example['dialogue'])):\n",
    "        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n",
    "다음 글을 요약해주세요:\n",
    "\n",
    "{}\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\n",
    "{}\n",
    "\n",
    "<end_of_turn><eos>\"\"\".format(example['dialogue'][i], example['summary'][i]))\n",
    "    return prompt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "다음 글을 요약해주세요:\n",
      "\n",
      "#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\n",
      "#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\n",
      "#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\n",
      "#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\n",
      "#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\n",
      "#Person2#: 알겠습니다.\n",
      "#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\n",
      "#Person2#: 네.\n",
      "#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \n",
      "#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\n",
      "#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\n",
      "#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n",
      "\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니다. 호킨스 의사는 스미스씨가 담배를 끊는 데 도움이 될 수 있는 수업과 약물에 대한 정보를 제공할 것입니다.\n",
      "\n",
      "<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "prompt_ex_one = generate_prompt(TRAIN_DATA[:1])[0]\n",
    "print(prompt_ex_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 QLoRA 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0.05,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# r (8로 증가):\n",
    "# 장점: 더 복잡한 표현이 가능해져 성능이 개선될 수 있음.\n",
    "# 단점: 메모리 사용량이 늘어나고, 계산 비용이 증가.\n",
    "# lora_alpha (16로 증가):\n",
    "# 장점: LoRA의 출력 강도가 커져서, 모델 성능이 좋아질 가능성.\n",
    "# 단점: 오버피팅 가능성 증가, 훈련 시간이 더 길어질 수 있음.\n",
    "# lora_dropout (0.1로 증가):\n",
    "# 장점: 과적합 방지에 도움이 되어, 모델의 일반화 성능이 향상.\n",
    "# 단점: 너무 큰 드롭아웃 값은 학습 성능 저하로 이어질 수 있음.\n",
    "# target_modules: LoRA를 적용할 모델 레이어들입니다. 주로 트랜스포머의 어텐션 및 피드포워드 관련 레이어에 적용됩니다.\n",
    "# target_modules: LoRA가 적용되는 모듈의 범위를 줄이면 연산량 감소, 특정 성능 유지. 하지만, 너무 적은 모듈에 적용하면 학습 성능 저하.\n",
    "# ???task_type=\"CAUSAL_LM\": 이 설정은 모델이 Causal Language Modeling 작업에 적합하도록 맞춰집니다.\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# load_in_4bit=True:\n",
    "# 장점: 메모리 사용량이 줄어들어 대형 모델을 작은 GPU에서도 실행 가능.\n",
    "# 단점: 양자화로 인해 성능이 약간 떨어질 수 있음.\n",
    "\n",
    "# bnb_4bit_quant_type=\"fp4\" (fp4로 변경):\n",
    "# 장점: 더 높은 계산 정밀도를 제공, 양자화로 인한 성능 손실을 줄임.\n",
    "# 단점: 메모리 사용량이 약간 늘어날 수 있음.\n",
    "\n",
    "# bnb_4bit_compute_dtype=torch.bfloat16 (bfloat16으로 변경):\n",
    "# 장점: float16보다 더 넓은 범위를 처리해, 안정성 향상.\n",
    "# 단점: 연산 속도가 약간 느려질 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d7e56ac2804dddbc89bfca3bf944f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\", quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "# 오른쪽에만 패딩을 넣는 이유는 주로 Causal Language Modeling (예측 기반 언어 모델)에서 모델이 입력의 왼쪽에서 오른쪽으로 순차적으로 문맥을 처리하기 때문입니다. 오른쪽에 패딩을 넣으면 패딩된 부분이 미래의 토큰처럼 처리되지 않으며, 자연스러운 문맥 예측이 가능합니다. 특히 GPT 계열 모델처럼 시퀀스 생성 방식의 모델에서 이러한 방식은 매우 중요합니다.\n",
    "# 왼쪽에 패딩을 넣으면 패딩이 문맥에 영향을 줄 수 있어 예측 성능이 떨어질 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Trainer 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nlp/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=TRAIN_DATA,\n",
    "    max_seq_length=512,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"outputs\",\n",
    "        num_train_epochs = 10,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        warmup_ratio=1,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        fp16_full_eval=True,  # kkh 추가함\n",
    "        logging_steps=100,\n",
    "        push_to_hub=False,\n",
    "        report_to='none',\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=generate_prompt,\n",
    ")\n",
    "\n",
    "# model=model: 미리 학습된 모델을 트레이너에 전달합니다.\n",
    "# train_dataset=TRAIN_DATA: 훈련에 사용할 데이터셋.\n",
    "# max_seq_length=512: 입력 시퀀스의 최대 길이.\n",
    "# output_dir=\"outputs\": 훈련 후 결과를 저장할 디렉토리.\n",
    "# num_train_epochs=1: 전체 데이터셋에 대해 1번의 에포크로 훈련.\n",
    "# max_steps=100: 최대 100 스텝까지 훈련.\n",
    "# per_device_train_batch_size=1: 장치당 배치 크기.\n",
    "# gradient_accumulation_steps=4: 4 스텝마다 경사도를 누적.\n",
    "# optim=\"paged_adamw_8bit\": 8비트 AdamW 옵티마이저 사용.\n",
    "# warmup_ratio=0.02: 학습률 워밍업 비율.\n",
    "# learning_rate=2e-4: 학습률.\n",
    "# fp16=True: 16비트 부동소수점 연산 사용.\n",
    "# fp16_full_eval=True: 평가 시에도 FP16 사용.\n",
    "# logging_steps=100: 100 스텝마다 로그 기록.\n",
    "# push_to_hub=False: 허브로 푸시하지 않음.\n",
    "# report_to='none': 로깅을 사용하지 않음.\n",
    "# peft_config=lora_config: LoRA(매개변수 효율적인 미세 조정) 설정.\n",
    "# formatting_func=generate_prompt: 데이터셋 전처리용 함수."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15560' max='15560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15560/15560 5:50:49, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.599600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.483400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.603800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.846400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.813200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.751800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.704900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.689500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.647400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.648600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.626300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.601900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.581300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.572200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.558800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.569900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.517200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.541900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.528900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.521200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.509500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.502700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.506300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.513300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.505600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.465800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.457400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.450900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.455200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.472600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.450200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.459000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.451800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.448800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.439600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.468700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.457500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.452400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.424400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.386500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.377100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.392100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.401900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.392700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.372800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.404200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.378500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.391300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.366700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.373700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.372300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.393700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.395700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.318400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.305100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.296200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.302900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.320100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.298100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.326200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.318900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.305300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.325800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.307700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.303800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.220400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.236500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.254900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.237300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.261900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.254700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>1.259500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.272700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>1.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.258900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>1.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>1.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.172800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>1.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>1.174200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>1.177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>1.176600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>1.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>1.193300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>1.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>1.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>1.232100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>1.209900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>1.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>1.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>1.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>1.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>1.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>1.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.155300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>1.148300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>1.156500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>1.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>1.158300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.190900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>1.171700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>1.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>1.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>1.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.128100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>1.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>1.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>1.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>1.097000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.108800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>1.124600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>1.124300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>1.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>1.135100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>1.141100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>1.156800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>1.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>1.139400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>1.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>1.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>1.037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>1.076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>1.074700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>1.102500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>1.110900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>1.087800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>1.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>1.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>1.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>1.146200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.145200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with train loss: 1.3659310024624312\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15560' max='15560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15560/15560 5:50:35, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.980300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.983900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.990400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.974100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.945500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.960600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.945900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.920400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.937200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.945500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.926100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.914500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.901300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.904000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.896800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.913300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.928400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.878900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.908600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.899900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.898600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.902500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.904200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.906800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.904500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.843600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.852400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.860500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.875500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.864400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.882000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.869200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.863700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.869900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.900600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.898300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.890500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.892500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.863700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.814700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.817700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.824100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.842800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.859200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.880700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.877400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.863900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.871900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.878800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.897900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.908700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.819600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.806400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.803500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.815200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.840200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.848800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.831700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.865100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.859300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.874500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.872400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.876300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.871100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.891300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.882300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.881400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.793000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.814300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.807300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.824100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.844100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.851900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.840100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.877700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.857600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.871500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.874900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.889000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.900200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.890400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.903700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.818200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.811500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.803200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.826900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.833100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.851500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.872100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.853400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.867900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.885700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.894700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.914800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.928300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.785800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.792700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.815200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.846800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.844700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.875100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.875300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.893200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.894300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.866900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.897800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.901500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.911600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.920600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.932200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.832300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>0.822200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.840700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.858800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.875600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.879200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.881800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.881100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.899800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.913300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>0.934100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.935600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.827500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.829600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.831900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.855700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.861400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.871100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.897200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>0.922100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.942200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>0.917800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.942200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.938200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.927800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with train loss: 0.8828608392749776\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15560' max='15560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15560/15560 5:50:26, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.822800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.823600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.813300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.786600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.792400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.797700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.778200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.742600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.762100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.746100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.719700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.737300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.710900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.724900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.717100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.703900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.705900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.696700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.712000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.728200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.713700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.704000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.695600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.705100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.697200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.707300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.706900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.708900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.681100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.649200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.673300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.674800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.692800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.682200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.674800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.683300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.710700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.711900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.704500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.704300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.679200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.640300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.639800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.648000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.666200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.680400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.678300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.667800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.704000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.705400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.693300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.699100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.709600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.725400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.736300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.656900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.644100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.644900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.657800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.677700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.688100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.672300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.705300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.699100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.716100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.717900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.722400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.718600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.731000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.643400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.664200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.660400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.679400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.700900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.700100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.733500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.716800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.730400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.734600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.744800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.759200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.753900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.768500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.673500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.703700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.717200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.740300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.728300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.733300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.740300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.765800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.787800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.767900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.782200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.790600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.673200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.705600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.736700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.732200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.763600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.759800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.780600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.783400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.767800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.809400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.783700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.795500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.807000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.818100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.752200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.711200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>0.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.737700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.747700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.768400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.776700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.780700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.784800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.799700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.826900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.814800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>0.835400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.838100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.866800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.734200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.738400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.743400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.785400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.778100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.766400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.803800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.840300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>0.835200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.863500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>0.831600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.833200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.858200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.871700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with train loss: 0.7343147777959437\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15560' max='15560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15560/15560 5:50:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.754400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.756200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.744700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.718600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.731100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.707400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.674700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.689700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.675200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.637900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.648100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.619400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.646400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.601600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.635300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.622000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.621900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.618000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.616700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.626300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.623800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.600100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.571300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.583500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.593400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.601800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.595200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.609700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.601100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.598700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.601300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.624500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.629300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.621800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.619700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.595700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.560200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.560100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.571800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.586500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.601200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.597900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.590100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.607100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.624100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.616900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.629500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.641300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.653500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.571500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.569400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.580400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.602100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.609100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.596800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.628500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.639900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.642400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.645100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.648200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.577900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.599300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.590600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.606400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.625700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.632200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.628400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.659600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.639900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.659200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.664000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.672500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.684500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.678600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.624800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.609700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.608300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.627700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.635200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.650600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.670800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.659200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.673300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.687700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.688600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.714600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.724300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.734300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.617700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.631200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.641200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.668800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.664500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.695400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.697700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.715100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.717800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.701100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.742200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.724100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.733300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.745900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.754800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.707800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.667900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>0.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.696800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.711500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.716600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.721400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.721900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.740400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.761100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>0.773200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.779600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.769100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.802200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.679500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.690300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.724800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.726100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.729100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.751700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.781400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>0.779100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.808800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>0.777000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.779500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.801700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.820800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.817800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with train loss: 0.6639174851169929\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='827' max='15560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  827/15560 18:40 < 5:33:35, 0.74 it/s, Epoch 0.53/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.706600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.690200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.666500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.671100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.677600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.654700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.621300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m best_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# 가장 낮은 train loss를 추적하기 위한 변수\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(trainer\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_train_epochs)):  \u001b[38;5;66;03m# 에포크 루프\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     train_output \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 한 에포크의 학습 진행\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_output\u001b[38;5;241m.\u001b[39mtraining_loss  \u001b[38;5;66;03m# 현재 에포크의 train loss\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# 가장 낮은 train loss가 발견되면 모델 저장\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:361\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 361\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3324\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3363\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3362\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3363\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/utils/operations.py:825\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/utils/operations.py:813\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/utils/operations.py:825\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/utils/operations.py:813\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: ConvertOutputsToFp32.__call__ at line 813 (2 times), autocast_decorator.<locals>.decorate_autocast at line 16 (2 times), convert_outputs_to_fp32.<locals>.forward at line 825 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/utils/operations.py:825\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/utils/operations.py:813\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/peft/peft_model.py:1129\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1128\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:1074\u001b[0m, in \u001b[0;36mGemmaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1071\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1074\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1087\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1088\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:888\u001b[0m, in \u001b[0;36mGemmaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    877\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    878\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    879\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         cache_position,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    898\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:624\u001b[0m, in \u001b[0;36mGemmaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    623\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 624\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_attention_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    626\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:119\u001b[0m, in \u001b[0;36mGemmaRMSNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 119\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# Llama does x.to(float16) * w whilst Gemma is (x * w).to(float16)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# See https://github.com/huggingface/transformers/pull/29402\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     output \u001b[38;5;241m=\u001b[39m output \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mfloat())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # trainer.train()\n",
    "\n",
    "# best_train_loss = float('inf')  # 가장 낮은 train loss를 추적하기 위한 변수\n",
    "\n",
    "# for epoch in range(int(trainer.args.num_train_epochs)):  # 에포크 루프\n",
    "#     train_output = trainer.train()  # 한 에포크의 학습 진행\n",
    "#     train_loss = train_output.training_loss  # 현재 에포크의 train loss\n",
    "\n",
    "#     # 가장 낮은 train loss가 발견되면 모델 저장\n",
    "#     if train_loss < best_train_loss:\n",
    "#         best_train_loss = train_loss\n",
    "#         trainer.model.save_pretrained(\"best_model\")  # 가장 낮은 train loss 시점의 모델 저장\n",
    "#         print(f\"Best model saved with train loss: {best_train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Finetuned Model 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8679582804af4b55a1fc11fb11ab4d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ADAPTER_MODEL = \"/best_model\"\n",
    "# # trainer.model.save_pretrained(ADAPTER_MODEL)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map='auto', torch_dtype=torch.float16)\n",
    "# model = PeftModel.from_pretrained(model, ADAPTER_MODEL, device_map='auto', torch_dtype=torch.float16)\n",
    "\n",
    "# model = model.merge_and_unload()\n",
    "# FINETUNE_MODEL = PRE_PATH + \"gemma_model/gemma-2b-it-sum-ko-10epc\"\n",
    "# model.save_pretrained(FINETUNE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Fine-tuned 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa15433225046389413ede1c85fc5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m finetune_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFINETUNE_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(BASE_MODEL)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/modeling_utils.py:3960\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3950\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3951\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3953\u001b[0m     (\n\u001b[1;32m   3954\u001b[0m         model,\n\u001b[1;32m   3955\u001b[0m         missing_keys,\n\u001b[1;32m   3956\u001b[0m         unexpected_keys,\n\u001b[1;32m   3957\u001b[0m         mismatched_keys,\n\u001b[1;32m   3958\u001b[0m         offload_index,\n\u001b[1;32m   3959\u001b[0m         error_msgs,\n\u001b[0;32m-> 3960\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3980\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/modeling_utils.py:4434\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4430\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4431\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4432\u001b[0m                 )\n\u001b[1;32m   4433\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4434\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4435\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4436\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4437\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4438\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4439\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4440\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4441\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4442\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4444\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4445\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4446\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4447\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4448\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4449\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4450\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4451\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4453\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/modeling_utils.py:961\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    950\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m is_quantized\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mrequires_parameters_quantization)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    959\u001b[0m ):\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 961\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/utils/modeling.py:399\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    397\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 399\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "finetune_model = AutoModelForCausalLM.from_pretrained(FINETUNE_MODEL, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Fine-tuned 모델 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex = DATASET_TEST['test']['dialogue'][0]\n",
    "# # ex = DATASET_TRAIN['train']['dialogue'][0]\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": \"다음 글을 요약해주세요:\\n\\n{}\".format(ex)\n",
    "#     }\n",
    "# ]\n",
    "# prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# pipe_finetuned = pipeline(\"text-generation\", model=finetune_model, tokenizer=tokenizer, max_new_tokens=1024)\n",
    "# outputs = pipe_finetuned(\n",
    "#     prompt,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.2,\n",
    "#     top_k=50,\n",
    "#     top_p=0.92,\n",
    "#     add_special_tokens=True\n",
    "# )\n",
    "# print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 테스트 데이터셋 전체 요약 코드\n",
    "# summaries = []\n",
    "\n",
    "# # 데이터셋 전체 요약 처리\n",
    "# for i, ex in enumerate(DATASET_TEST['test']['dialogue']):\n",
    "#     # 각 대화마다 메시지 포맷 설정\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"다음 글을 요약해주세요:\\n\\n{}\".format(ex)\n",
    "#         }\n",
    "#     ]\n",
    "    \n",
    "#     # 프롬프트 생성\n",
    "#     prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "#     pipe_finetuned = pipeline(\"text-generation\", model=finetune_model, tokenizer=tokenizer, max_new_tokens=512)\n",
    "    \n",
    "#     # 모델을 통해 요약 생성\n",
    "#     outputs = pipe_finetuned(\n",
    "#         prompt,\n",
    "#         do_sample=True,\n",
    "#         temperature=0.2,\n",
    "#         top_k=50,\n",
    "#         top_p=0.92,\n",
    "#         add_special_tokens=True\n",
    "#     )\n",
    "    \n",
    "#     # 요약 텍스트 저장\n",
    "#     summary = outputs[0][\"generated_text\"][len(prompt):]\n",
    "#     summaries.append(summary)\n",
    "    \n",
    "#     # 요약 출력\n",
    "#     print(f\"{summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_0,\"실장님, 더슨 씨, 이제 당신이 내부 메모를 준비하고 있으니, 제가 당신에게 몇 가지 질문을 드리겠습니다.\n",
      "\n",
      " chrysler, 실장님, 이 메모는 어떻게 작성하나요?\n",
      "\n",
      " chrysler, 실장님, 이 메모는 내부와 외부 통신에 적용되며, 모든 직원들이 즉시 메시지를 사용하지 않도록 하기 위해 작성해야 합니다.\n",
      "\n",
      " chrysler, 실장님, 이 메모를 어떻게 작성하나요?\n",
      "\n",
      " chrysler, 실장님, 이 메모는 내부와 외부 통신에 적용되며, 모든 직원들이 즉시 메시지를 사용하지 않도록 하기 위해 작성해야 합니다.\n",
      "\n",
      " chrysler, 실장님, 이 메모를 어떻게 작성하나요?\n",
      "\n",
      " chrysler, 실장님, 이 메모는 내부와 외부 통신에 적용되며, 모든 직원들이 즉시 메시지를 사용하지 않도록 하기 위해 작성해야 합니다.\n",
      "\n",
      " chrysler, 실장님, 이 메모를 어떻게 작성하나요?\n",
      "\n",
      " chrysler, 실장님, 이 메모는 내부와 외부 통신에 적용되며, 모든 직원들이 즉시 메시지를 사용하지 않도록 하기 위해 작성해야 합니다.\n",
      "\n",
      " chrysler, 실장님, 이 메모를 어떻게 작성하나요?\n",
      "\n",
      " chrysler, 실장님, 이 메모는 내부와 외부 통신에 적용되며, 모든 직원들이 즉시 메시지를 사용하지 않도록 하기 위해 작성해야 합니다.\n",
      "\n",
      " chrysler, 실장님, 이 메모를 어떻게 작성하나요?\n",
      "\n",
      " chrysler, 실장님, 이 메모는 내부와 외부 통신에 적용되며, 모든 직원들이 즉시 메시지를 사용하지 않도록 하기 위해 작성해야 합니다.\n",
      "\n",
      " chrysler, 실장님, 이 메모를 어떻게 작성하나요?\n",
      "\n",
      " chrysler, 실장님, 이 메모는 내부와 외부 통신에 적용되며, 모든 직원들이 즉시 메시지를 사용하지 않도록 하기 위해 작성해야 합니다.\n",
      "\n",
      " chrysler, 실장님, 이 메모를 어떻게 작성하나요?\n",
      "\n",
      " chrysler, 실장\"\n",
      "test_1,\"#Person2#는 교통 체증에 걸렸고, #Person1#은 #Person2#에게 대중교통을 이용하라고 제안한다. #Person2#는 #Person1#에게 차가 없어서 운전하는 것을 그만두려고 한다고 말한다.\"\n",
      "test_2,\"#Person1#은 마샤와 히어로가 이혼하려고 한다고 말한다. 마샤는 양육권을 가지게 될 것이고, 다른 세부 사항을 가지고 이의를 제기하는 일도 없을 것이라고 말한다.\"\n",
      "test_3,브라이언은 #Person1#에게 그의 생일 파티를 축하하고 #Person1#의 목걸이가 그의 드레스와 잘 어울린다고 말한다.\n",
      "test_4,\"#Person2#는 #Person1#에게 올림픽 공원의 중심인 올림픽 스타디움을 소개하고, 5000개의 좌석이 있다고 말합니다.\"\n",
      "test_5,\"#Person1#은 #Person2#에게 회사를 창업하기 위해 사업 계획서를 작성하고, 투자자를 모집하고, 일하기로 결정한 것을 설명한다. #Person2#는 #Person1#에게 재무 분석을 설명하고, 투자자들에게 수익을 언제 볼 수 있을지 알려준다.\"\n",
      "test_6,#Person2#가 몸이 좋지 않다. #Person1#은 #Person2#가 수두에 걸린 것 같다고 생각하고 의사를 만나고 싶어한다. #Person2#는 의사에게 수두에 걸린 것 같다고 말한다.\n",
      "test_7,#Person2#는 체크아웃하고 청구서를 받고 싶어합니다. #Person1#는 #Person2#에게 세탁 서비스 비용을 확인하고 수정합니다.\n",
      "test_8,#Person1#은 #Person1#의 아내가 #Person1#와 비서의 불륜을 발견했다고 말한다. #Person1#은 #Person1#의 잘못을 esezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesezesez\n",
      "test_9,#Person2#는 에이브러햄 링컨을 #Person1#에게 설명합니다. #Person2#는 에이브러햄 링컨이 평등한 권리를 위해 싸우는 용기를 존경합니다.\n",
      "test_10,#Person1#과 #Person2#는 모래폭풍이 일어난 후 호흡기 감염자 수가 증가하는 것에 대해 이야기하고 있습니다.\n",
      "test_11,#Person2#는 프란시스의 생일 파티에서 리모컨 자동차를 선물합니다. 프란시스는 #Person2#가 친절하게 선물을 준 것에 감사합니다.\n",
      "test_12,\"토니는 부정행위를 하다가 실패했고, 그가 잘해야 할 일은 열심히 공부하는 것이라고 생각한다.\"\n",
      "test_13,톰은 #Person1#에게 잠시 기차역에 가서 9시 10분에 가야 한다고 말한다.\n",
      "test_14,\"#Person1#은 삶을 조절하고 싶어하고, #Person2#는 #Person1#에게 충분한 수면, 매일 운동, 그리고 미래를 걱정하지 말라고 조언합니다.\"\n",
      "test_15,#Person1#은 루오지아가 결혼했다는 것을 알게 된다. #Person2#는 이를 믿고 루오지아를 위해 파티에 가고 싶어한다. #Person1#은 #Person2#에게 그녀의 행복을 축하하기 위해 와인잔을 가져가라고 제안한다.\n",
      "test_16,\"#Person1#과 #Person2#는 줄기를 당기고 뒷면을 벗깁니다. #Person1#은 #Person2#가 재미있다고 생각하지만, #Person2#는 그것이 잔인하다고 생각합니다.\"\n",
      "test_17,마이크는 #Person1#에게 자신의 누나가 키가 크고 예뻐서 친절하고 사교적이라고 말한다.\n",
      "test_18,#Person1#은 오늘 아침에 어지러웠습니다. #Person2#는 #Person1#의 체온을 측정하고 약간의 열이 있다고 말했습니다.\n",
      "test_19,\"#Person2#는 새로운 휴대폰을 사고 싶어하고, #Person1#은 #Person2#에게 카메라와 MP3 플레이어가 있는 휴대폰을 추천합니다.\"\n",
      "test_20,프랭크는 주디에게 우체국에서 일하게 될 것이라고 말한다. 그는 건강보험을 이용할 수 있기 때문에 우체국에서 일하는 것이 좋다고 생각한다.\n",
      "test_21,\"#Person2#는 #Person1#에게 #Person2#의 자격증, 교육, 그리고 경험에 대해 이야기합니다.\"\n",
      "test_22,\"#Person1#은 미디엄 레어의 스테이크를 주문했지만, 너무 익혀져 있어서 바꿔주고 싶어합니다.\"\n",
      "test_23,#Person1#은 톰에게 그의 소설이 노벨상을 받았다고 말한다. 톰은 놀라워한다.\n",
      "test_24,#Person2#는 #Person1#에게 #Person2#가 자동차 디자인을 하고 과학 석사 학위를 받았다고 말합니다.\n",
      "test_25,\"#Person2#는 #Person1#에게 술을 많이 마시지 않고 밤새도록 술을 마시지 않는다고 말합니다. #Person2#는 맥주를 좋아하지 않지만, 칵테일을 좋아합니다. #Person2#는 #Person1#에게 내일 밤에 맥주 특별 할인을 하러 가자고 제안합니다.\"\n",
      "test_26,\"#Person1#은 메이에게 피크닉 준비를 도와달라고 요청하고, 다니엘에게도 도움을 청한다.\"\n",
      "test_27,제임스와 뮤리엘 더글라스는 휴가 동안 함께 보냈습니다. 제임스는 뮤리엘에게 자신의 휴가를 소개합니다.\n",
      "test_28,\"#Person1#은 바보 같은 여자가 ATM에서 돈을 뽑는 것을 멈추지 않고 돈을 빼려고 하지만, #Person2#는 #Person1#에게 봉쇄된 문을 닫고 현지 당국이 도착할 때까지 문을 잠길 것이라고 말한다.\"\n",
      "test_29,#Person2#는 #Person1#에게 동료들과 소통하는 전략을 알려준다.\n",
      "test_30,\"폴리 씨가 끔찍한 일로 인해 끔찍한 일을 겪고 있고, #Person1#이 폴리 씨에게 끔찍한 일을 벗어나게 해주고 있습니다.\"\n",
      "test_31,모니카는 프란시스에게 보고서 작업을 좀 더 일찍 완성할 수 있도록 요청합니다.\n",
      "test_32,#Person2#가 #Person1#에게 면접 준비를 위한 워크숍을 소개합니다. #Person1#은 면접에서 무엇을 해야 할지에 대한 추가적인 팁을 #Person2#에게 물어봅니다.\n",
      "test_33,\"#Person1#은 마이크에게 캐릭터가 분노와 슬픔을 동시에 느끼고 있다고 말하고, 그들은 시도해 볼 것이다.\"\n",
      "test_34,#Person1#은 제인과 토드 부인을 방문하고 싶어서 #Person2#를 방문한다. #Person2#는 #Person1#에게 가게에서 필요한 게 있으면 전화하라고 요청한다.\n",
      "test_35,\"빌은 피곤해서 퇴근 시간에 집에 가서 좀 쉬고 싶어합니다. 동생은 괜찮지만, 빌은 2주 전에 미국에 출장을 갔습니다.\"\n",
      "test_36,\"클레오는 시위에 참여하지 않고 싶지만, 사이먼은 그녀에게 시위에 가라고 설득한다. 사이먼은 그녀에게 물리학 시험 공부할 계획이지만, 그녀는 그냥 집에 가고 싶어한다.\"\n",
      "test_37,#Person1#은 #Person2#에게 누군가가 문을 두드리고 '전기'나 '가스'라고 말하면 그가 괜찮다고 생각한다고 말합니다.\n",
      "test_38,\"마크는 매기에게 그의 노트를 빌려달라고 요청하고, 매기는 수업 중에 깨어있게 해줄 사람이 필요하므로 수업 중에 깨우는 사람이 필요하다는 것을 알게 된다.\"\n",
      "test_39,\"#Person2#는 버먼 교수님의 강의를 통해 지질학에 관심이 있고, #Person2#는 터너 교수님의 선배들에게서 지질학에 대한 자격을 갖추고 싶어한다.\"\n",
      "test_40,\"#Person1#이 상점에서 펜던트를 구입하고, #Person2#에게 부러진 펜던트를 교체해달라고 요청했습니다.\"\n",
      "test_41,#Person1#과 #Person2#는 경기 중에 옷을 얘기하고 있다. #Person1#은 #Person2#에게 옷을 반품하라고 권하지만 #Person2#는 듣지 않는다.\n",
      "test_42,\"#Person2#는 TV 뉴스 디렉터가 될 수 있고, 컴퓨터와 함께 일하는 것을 좋아한다.\"\n",
      "test_43,\"#Person2#는 연설자들과의 인터뷰를 위해 참석하고 싶지만, #Person1#은 #Person2#에게 몇 가지 방법을 제안한다.\"\n",
      "test_44,\"사라는 #Person1#에게서 50년 동안 사용된 집을 사는 것을 제안하고, 그 집의 위치가 좋다고 말합니다.\"\n",
      "test_45,\"#Person1#은 마크 리치에게 런던의 정보 담당관이 어떻게 정보를 제공하는지에 대해 묻습니다. 마크 리치는 매일 10명의 담당관들이 있는 정보 팀을 가지고 있으며, 방문객들에게 직접적으로 관광 정보를 제공합니다. 마크 리치는 또한 매일 런던의 다른 부분을 보는 것을 제안합니다.\"\n",
      "test_46,\"루시와 린팡은 영어와 수학이 어렵다고 생각하지만, 낸시는 중국어를 좋아하고, 린팡은 과학을 좋아한다.\"\n",
      "test_47,\"제임스는 토마스 부인에게 마당에 낙엽을 다 치고, 제임스가 제가 도와준 것에 대해 감사하며, 제임스는 농구 연습을 위해 자전거를 사려고 합니다.\"\n",
      "test_48,\"#Person1#과 #Person2#는 난방이 꺼져서 춥고, 밤에 바람이 불어넣어야 한다고 생각합니다.\"\n",
      "test_49,\"#Person1#은 #Person2#에게 연기하는 방법을 묻고, #Person2#는 #Person1#에게 그가 연기하는 것을 돕고 싶어한다.\"\n",
      "test_50,#Person1#은 #Person2#의 도움으로 프렌드십 호텔에 택시를 타고 15위안을 지불했습니다.\n",
      "test_51,#Person1#은 배가 고프다. #Person2#는 #Person1#에게 환승을 요청하고 비용이 없다고 말한다.\n",
      "test_52,#Person2#는 #Person1#에게 15분 동안 차로 시내로 가는 길과 7층에 있는 회사의 위치를 알려준다.\n",
      "test_53,#Person1#은 루루와 빅이 헤어졌다고 #Person2#에게 말한다.\n",
      "test_54,\"데이브가 샐리에게 짐을 찾아달라고 전화를 합니다. 샐리는 데이브에게 짐이 돌아오지 않았다고 말하고, 데이브는 샐리에게 전해줄 것입니다.\"\n",
      "test_55,#Person2#가 #Person1#에게 시청으로 가는 방법을 알려준다.\n",
      "test_56,#Person1#은 혹시 여권을 반납한 사람이 있는지 궁금해서 #Person2#에게 도움을 청하고 있습니다. #Person2#는 #Person1#에게 여권이 거기 없다는 것을 알고 신발 매장에 전화해서 여권을 찾았는지 확인하라고 요청합니다.\n",
      "test_57,레아가 나다니엘에게 전화를 걸어 콜린스 선생님과 약속을 잡습니다. 나다니엘은 오늘 오후 2시 15분에 리우 씨와 통화할 수 있을까요?\n",
      "test_58,#Person1#과 #Person2#는 그녀가 딕 이외의 남자와는 결혼하지 않을 것이라고 말했습니다.\n",
      "test_59,\"#Person1#과 #Person2#는 파티에서 섹시한 여자들과 맥주를 마시고 있다. #Person2#는 여자와 말하는 것을 꺼려하고, #Person1#은 #Person2#에게 조금 더 맥주를 마시라고 권한다.\"\n",
      "test_60,\"잭은 #Person1#에게 그의 수업들에 대해 이야기한다. #Person1#은 작년에 비즈니스 커뮤니케이션 수업을 들었는데, 그 수업이 좋았다고 말한다.\"\n",
      "test_61,#Person1#과 #Person2#는 베이징의 날씨에 대해 이야기하고 있습니다. #Person2#는 베이징의 가을이 아름답다고 생각합니다. #Person1#은 베이징의 여름이 춥다고 생각합니다.\n",
      "test_62,\"#Person2#는 #Person1#에게 어젯밤 공포 영화를 봤다고 말합니다. #Person1#은 #Person2#에게 러브 스토리를 보러 가자고 제안하지만, #Person2#는 그것을 좋아하지 않습니다.\"\n",
      "test_63,\"#Person1#과 아담은 학교 구경을 하고 있다. 그들은 운동장, 도서관, 학교의 새 교실 건물, 그리고 부러운 학교 환경에 대해 이야기한다.\"\n",
      "test_64,#Person1#이 #Person2#에게 #Person2#의 아빠가 될 것이라고 말한다.\n",
      "test_65,#Person1#과 #Person2#는 존이 일주일에 7번 만나고 존과 함께하는 여자와의 관계에 대해 이야기하고 있습니다.\n",
      "test_66,\"#Person1#은 #Person2#에게 시스템 업그레이드를 고려하라고 권합니다. #Person1#은 #Person2#에게 더 빠른 프로세서, 더 강력한 하드 디스크, 더 많은 메모리, 더 빠른 모뎀, CD-ROM 드라이브를 추가하라고 제안합니다.\"\n",
      "test_67,\"#Person2#는 중국에서 왔고, #Person1#은 #Person2#에게 몇 가지 질문을 합니다. #Person2#는 #Person1#에게 26살이라고 말하고, #Person1#은 40살이라고 말합니다. #Person2#는 스페인어를 할 수 있고, #Person1#은 스페인어를 할 수 있습니다.\"\n",
      "test_68,\"#Person1#은 체중이 증가하고 있고, #Person2#는 #Person1#에게 운동을 권장합니다.\"\n",
      "test_69,제임스는 8인 룸을 원하고 #Person1#에게 예약을 했습니다.\n",
      "test_70,\"#Person2#는 #Person1#에게 공장이 75,000 제곱미터의 면적을 가지고 있으며 500명의 직원이 있다고 말합니다. #Person1#는 생산 공장에 왔습니다.\"\n",
      "test_71,\"레베카는 #Person1#에게 그녀의 첫 직장에 대해 이야기합니다. 그녀는 2006년에 그들을 떠나기로 결정했고, 그래서 그녀는 런던에 본사를 둔 국가 신문사에서 자리를 찾을 수 있었습니다.\"\n",
      "test_72,\"#Person1#과 #Person2#는 내일 그룹 발표를 위해 문구점에 가기 전에 물건을 사고, 각 선반 위의 표지판을 보고 물건을 찾을 수 있도록 준비하고 있습니다.\"\n",
      "test_73,#Person1#은 메리에게 인터넷에서 일자리를 찾는 방법을 가르친다. 메리는 인터넷에서 일자리를 찾는 것이 안전하다고 생각한다.\n",
      "test_74,#Person2#는 #Person1#에게 자신의 예산 계획을 소개합니다.\n",
      "test_75,제인은 수잔을 볼려고 병원에 가고 싶어한다. 헨리는 그녀를 위해 13번 버스를 타는 것을 제안한다.\n",
      "test_76,\"#Person1#은 #Person2#에게 내일 아침에 바쁘다고 말하고, #Person2#는 #Person1#에게 금요일 오후 2시 30분에 만나는 것을 제안합니다.\"\n",
      "test_77,\"#Person2#는 #Person1#에게 '뉴욕의 친구'라는 서비스를 추천하고, 그 서비스는 설문조사를 통해 정보를 제공합니다.\"\n",
      "test_78,#Person2#는 #Person1#에게 회사의 소개와 직원들의 급여에 대해 이야기합니다. #Person2#는 영업 부서에서 일하고 싶어합니다. #Person1#은 #Person2#에게 회사의 복리후생 혜택을 소개합니다.\n",
      "test_79,\"#Person1#은 계약서를 서명하기 위해 #Person2#에게 초안을 요청하고, #Person2#는 오늘 밤에 계약서를 작성하고 서명하기 위해 복사본을 만들어 준다.\"\n",
      "test_80,\"#Person1#은 #Person2#에게 차량 사고를 경험했고, #Person2#는 구급차와 경찰을 불러주겠다고 말했습니다.\"\n",
      "test_81,#Person2#가 #Person1#에게 학교 클리닉으로 가는 길을 알려준다.\n",
      "test_82,\"#Person2#는 방이 너무 시끄러워서 참을 수 없어서 #Person1#에게 방을 바꾸고 싶어합니다. #Person1#은 내일 방이 없을 수도 있다고 말하고, 내일 방을 제공할 수도 있다고 말합니다.\"\n",
      "test_83,\"#Person1#은 #Person2#에게 비행을 즐기고 편안하게 쉬는 것을 권하고, #Person2#를 연회에 초대합니다. #Person2#는 감사하며, #Person1#에게 관광을 원하라고 합니다.\"\n",
      "test_84,\"#Person2#가 #Person1#에게 올림픽 공원 근처에 있는지 알려주고, #Person1#이 중국 골동품 가게를 찾고 있다고 말합니다.\"\n",
      "test_85,#Person2#의 컴퓨터가 멈추고 시작하지 않습니다. #Person1#과 리 씨는 문제를 해결하기 위해 수리공에게 전화하려고 합니다.\n",
      "test_86,#Person2#는 어머니의 생일 선물로 #Person1#의 추천에 따라 금 시계를 선택했습니다.\n",
      "test_87,\"#Person1#은 로스 씨에게 10월 24일 토요일 오전에 12시 30분에 시작하는 발표를 위해 50명의 초대를 받았다고 말한다. 로스 씨는 100장의 초대장을 보냈고, #Person1#은 30명이 올 것으로 예상한다.\"\n",
      "test_88,\"#Person2#는 #Person1#에게 러시아와 캐나다의 주요 차이점을 설명합니다. #Person2#는 러시아 사람들이 빠르게 움직이며, 사람들이 빠르게 말하고, 차를 너무 빨리 몰고, 좋은 거래도 금방 지나가버리지만, 캐나다에서는 사람들이 좀 더 여유롭다고 말합니다.\"\n",
      "test_89,#Person2#는 올해 휴가를 카리브해로 가려고 계획하고 있다. #Person1#은 작년에 가본 경험을 #Person2#에게 공유한다.\n",
      "test_90,\"#Person1#은 소풍에서 가져갈 과일을 묻고, #Person2#는 #Person1#에게 포도를 가져가라고 말합니다.\"\n",
      "test_91,#Person2#는 #Person1#에게 내일 아침에 소형차를 렌트하는 방법을 알려준다.\n",
      "test_92,#Person1#과 #Person2#는 서로의 의견을 바꾸고 있습니다.\n",
      "test_93,\"#Person1#과 #Person2#는 이전에 팔지 못했던 돈을 벌기 위해 새로운 마케팅 전략을 시도했고, 이는 성공적으로 작동했습니다.\"\n",
      "test_94,#Person2#가 #Person1#에게 내 가방을 찾아달라고 요청한다. #Person1#은 #Person2#에게 돈을 빌려주고 #Person2#가 돈을 갚을 때까지 기다릴 것이다.\n",
      "test_95,\"린과 스티븐은 중국에서의 식사에 대해 이야기하고 있다. 린은 중국에서의 팁에 대해 이야기하고, 스티븐은 택시와 짐꾼의 팁에 대해 이야기한다.\"\n",
      "test_96,빌은 브레인 로커를 알고 있다. 빌은 브레인 로커가 말랐고 키가 커서 훌쩍한 코를 가지고 있다고 #Person1#에게 말한다.\n",
      "test_97,\"#Person2#는 톰 윌슨에게 청구서를 지불하도록 요청하고, 톰 윌슨은 신용카드로 청구서를 지불합니다.\"\n",
      "test_98,#Person1#이 캐롤에게 전화를 걸었지만 그녀가 없습니다. #Person2#는 #Person1#에게 내일 밤 8시 30분에 질의 집에서 디저트를 가져오라고 요청합니다.\n",
      "test_99,#Person2#는 #Person1#에게 #Person2#가 커먼 헬스장에서 일하셨다고 말합니다. #Person1#는 #Person2#를 다른 사람과 혼동해서 #Person2#가 모서리에 있는 홀 빈 카페에서 일하셨다고 생각합니다.\n",
      "test_100,#Person2#는 트럼프가 다시 대통령이 되길 바랍니다. #Person1#는 그가 다시 대통령이 되길 바랍니다.\n",
      "test_101,#Person2#가 #Person1#에게 ATM 사용법을 알려준다.\n",
      "test_102,수잔이 존에게 메모를 주고 싶어합니다.\n",
      "test_103,\"#Person1#은 릴리에게 이번 주말에 강가로 소풍에 참여하도록 초대하고, 그들은 모두 무엇을 가져가야 하는지에 대해 이야기합니다.\"\n",
      "test_104,\"#Person2#는 #Person1#에게 중국에서의 식사 방법에 대해 이야기합니다. #Person2#는 젓가락을 사용해야 하고, 절대로 젓가락을 밥그릇에 꽂아서는 안 된다고 말합니다. #Person2#는 젓가락을 꽂아서 먹는 것을 싫어합니다.\"\n",
      "test_105,메리는 프랭크에게 영화를 보는 것을 좋아한다고 말한다. 그들은 영화의 종류에 대해 이야기한다. 그들은 무비 살롱에서 영화를 빌려서 영화를 보는 것을 좋아한다.\n",
      "test_106,#Person2#는 #Person1#에게 녹색당이 정치 당에 소속되어 있지 않다고 말합니다. #Person2#는 녹색당이 정치 당에 영향을 미칠 수 있다고 생각합니다.\n",
      "test_107,윌슨 씨는 #Person1#에게 상품의 실수에 대해 매우 유감스럽다고 말합니다. #Person1#은 첫 번째 문제를 해결하고 두 번째 문제를 교환하기로 합니다.\n",
      "test_108,#Person2#는 강도를 보았습니다. #Person2#는 경찰서로 오고 싶습니다.\n",
      "test_109,#Person2#는 #Person1#에게 저녁 식사를 위해 피자를 주문하라고 요청한다.\n",
      "test_110,#Person1#은 다이어트를 시작하기로 결정하고 #Person2#에게 새로운 음식을 먹지 말고 빵을 먹지 말고 식사를 하지 말라고 말했다.\n",
      "test_111,\"카렌 후앙은 비교문학 287 수업에 등록하려고 합니다. 그녀는 #Person1#에게 코드를 받고, #Person1#은 그녀에게 수업이 가득 찼는지 확인하는 데 도움을 줍니다.\"\n",
      "test_112,#Person1#은 급해서 우산을 깜빡하고 가든 호텔로 가고 있습니다. #Person2#는 #Person1#에게 가든 호텔로 가는 걸 제안합니다.\n",
      "test_113,잭이 데이지에게 새 차를 태워달라고 요청한다. 데이지는 그 차가 빠르고 코너를 돌아서도 쉽게 가는 것을 발견한다.\n",
      "test_114,#Person2#는 #Person1#에게 그 큰 불이 밤 10시쯤 일어났다고 말합니다.\n",
      "test_115,\"#Person2#는 남편에게 계산을 하라고 요청하고, #Person1#은 그녀에게 영수증을 준다.\"\n",
      "test_116,\"#Person2#는 #Person1#에게 차를 세차하고 싶다고 말합니다. #Person1#은 #Person2#에게 일반 세차 패키지를 추천하고, #Person2#는 그것을 선택합니다.\"\n",
      "test_117,\"해리는 #Person1#에게 올해 휴가를 어디에서 보낼지 물어보고, 더 싸지만 시간이 오래 걸리는 바다를 통해 여행할 수도 있다고 말합니다.\"\n",
      "test_118,\"존슨이 #Person1#에게 웨이트를 하는 방법을 가르쳐준다. #Person1#은 웨이트를 하는 방법을 모른다. 존슨은 #Person1#에게 카드를 사용하여 기계를 사용할 수 있도록 돕고, #Person1#은 힘들 때까지 운동을 계속해야 한다고 말한다.\"\n",
      "test_119,#Person1#과 #Person2# 모두 일자리를 잃은 상황에서 면접을 위해 무엇을 하고 싶은지에 대해 이야기하고 있다.\n",
      "test_120,#Person1#과 #Person2#는 강아지들에게 밥을 주고 목욕을 시켜 줄 것입니다.\n",
      "test_121,#Person1#은 에이든에게 자신의 집주인에게 200달러를 빚지고 있다고 말한다. 에이든은 돈이 부족해서 동의하지 않는다. 그들은 결국 6시에 저녁 식사를 할 예정이다.\n",
      "test_122,#Person2#는 #Person1#에게 자선 단체에서 일한 경험을 이야기합니다. #Person2#는 그것이 #Person2#의 마음에 영향을 미쳤다고 생각합니다.\n",
      "test_123,#Person1#과 #Person2#는 결정을 내려야 하기 때문에 연락을 통해 결정을 내려야 합니다.\n",
      "test_124,\"#Person2#는 #Person1#에게 무탄과 함께 베이징 오리구이를 추천하고, #Person1#은 그것을 시도할 것입니다.\"\n",
      "test_125,\"댄이 안젤라에게 전화를 걸어 그녀에게 메시지를 전달하고, 메건을 초대한다. 안젤라는 댄에게 메건이 결혼식에 참석할 것이라고 말한다.\"\n",
      "test_126,#Person2#는 디저트를 원하고 #Person1#은 먼저 티라미수를 먹고 싶어합니다. #Person2#는 그리스에서 가장 맛있는 티라미수를 먹고 싶어합니다. #Person1#은 #Person2#에게 바나나 튀김 두 개를 가져오고 싶어합니다.\n",
      "test_127,#Person2#는 #Person1#에게 예일 대학교에서 경제학을 공부하고 은행에서 500달러를 받았다고 말했습니다.\n",
      "test_128,#Person1#은 #Person2#의 조언에 따라 디지털 바비를 구매합니다.\n",
      "test_129,\"#Person2#는 조던 스포츠 신발 한 켤레를 원하고, #Person1#은 세일 중인 조던 스포츠 신발을 추천합니다.\"\n",
      "test_130,#Person1#은 과학 박물관에 가고 싶지만 길을 잃어버려서 1.5달러를 지불해야 합니다. #Person2#는 #Person1#에게 기차 시간표를 알려주고 기차를 타는 방법을 알려줍니다.\n",
      "test_131,#Person2#는 #Person1#에게 단계적 은퇴라는 프로그램에 참여하고 있다고 말한다. 이 프로그램은 임시 직원을 고용하는 대신에 접근할 수 있는 웹사이트에 채용 공고를 올린다. #Person2#는 이 프로그램이 어떤 종류의 일이 올라오는지 설명한다.\n",
      "test_132,\"로키는 #Person1#에게 자신의 타입을 설명하고, #Person1#은 그가 결혼할 수 없을까 걱정하고 있다. #Person1#은 로키에게 외향적이고, 사람들의 차이를 비판하지 않는 여자를 좋아한다고 말한다.\"\n",
      "test_133,\"#Person1#과 #Person2#는 폭풍이 왔고, 봄이 오고 있다고 말합니다.\"\n",
      "test_134,\"#Person1#과 #Person2#는 TV를 보는 것에 지루하다고 생각한다. #Person2#는 TV를 보고 싶지 않지만, #Person1#은 TV를 보는 것에 지루하지 않다. 그들은 함께 음악을 할 때 나누어서 TV를 보는 것에 동의한다.\"\n",
      "test_135,\"#Person1#은 벤에게 내일 수업 준비를 하라고 말하고, 벤은 점심시간이 12시라고 말한다.\"\n",
      "test_136,\"애덤은 오늘의 상태가 훨씬 좋아졌다고 말하고, 오늘의 소식과 토요일 경기에 대한 정보를 #Person1#에게 알려준다. #Person1#은 애덤에게 3일 후에 무릎을 쉬게 하고, 애덤은 내일 전체 훈련에 참여할 것이다.\"\n",
      "test_137,#Person1#은 프린터가 고장났습니다. #Person2#는 #Person1#에게 복사본을 출력해달라고 요청합니다.\n",
      "test_138,\"#Person1#은 커튼을 걸고 싶어하고, #Person2#는 #Person1#을 잡아서 걸어가기로 결정했습니다.\"\n",
      "test_139,\"#Person1#은 주말 캠핑 여행을 계획하고 있고, 잭은 가장 적합한 주말을 알고 싶어합니다.\"\n",
      "test_140,#Person2#는 #Person1#에게 #Person2#가 임신했다고 말한다. #Person1#은 놀란다. #Person2#는 #Person1#에게 #Person2#가 훈련 세미나에 참석하러 외출했었다고 말한다. #Person1#은 놀란다.\n",
      "test_141,#Person2#의 딸은 대학을 결정하지 못하고 있어서 걱정하고 있습니다. #Person1#은 #Person2#에게 그녀가 스스로 결정을 내릴 수 있도록 격려하고 있습니다.\n",
      "test_142,#Person1#은 #Person2#에게 #Person1#이 직장을 잃게 된 이유는 #Person1#이 투자 상담사로 일하다 보면 작은 실수 하나로 회사에 수백만의 손해를 끼칠 수 있다고 말한다. #Person2#는 #Person1#에게 평소에 사던 불필요한 것들을 몇 달 동안 사지 않으면 돈을 절약할 수 있다고 말한다.\n",
      "test_143,#Person2#는 #Person1#에게 #Person2#의 친구가 영국에서 방문을 왔다고 말한다. #Person2#는 #Person1#에게 강연 주제를 소개한다.\n",
      "test_144,\"#Person1#은 #Person2#에게 존의 집에 가자고 제안하지만, #Person2#는 아픈 것 같아 집에서 쉬는 것을 선호합니다.\"\n",
      "test_145,\"파버 씨가 3박 동안 더블 룸을 예약하고 싶어합니다. #Person1#은 그에게 예약을 도와주고, 그가 얼마를 내야 하는지 알려줍니다.\"\n",
      "test_146,\"#Person1#은 연립 주택을 원하고, #Person2#는 존 고드프리가 #Person1#에게 전화를 걸어서 웨스트 더비에 있는 다른 사무실을 추천한다.\"\n",
      "test_147,\"#Person1#과 #Person2#는 경찰이 #Person2#가 과속을 했다고 주장하지만, #Person2#는 운전을 멈추지 않았다고 주장한다.\"\n",
      "test_148,\"달린은 댄에게 컴퓨터 주문에 대해 전화를 걸고, 댄은 #Person2#에게 스티브에게 전화해달라고 요청한다.\"\n",
      "test_149,#Person2#는 #Person1#에게 워싱턴 포스트가 1877년 12월에 창립되었고 뉴욕 타임즈가 1851년 9월에 창립되었으며 로스앤젤레스 타임즈가 1881년 12월에 창립되었다고 말한다.\n",
      "test_150,#Person2#의 컴퓨터에 바이러스가 침투했다. #Person1#은 #Person2#에게 이메일을 압축해서 보내라고 조언한다.\n",
      "test_151,#Person2#는 #Person1#의 초대로 주말을 보냈습니다.\n",
      "test_152,\"#Person1#은 중국 음식을 먹고 싶어하고, #Person2#는 판다 익스프레스에서 음식을 사올 예정입니다.\"\n",
      "test_153,메리는 톰에게 판매 직원 자리를 다른 사람에게 제공하기로 결정했다고 말한다. 톰은 그에게 기회를 주기로 결정한다.\n",
      "test_154,#Person1#은 #Person2#에게 경찰에 신고하라고 요청한다.\n",
      "test_155,#Person2#는 음악 선생님이 되고 싶어서 음악에 관심이 있다고 말합니다. #Person1#은 #Person2#의 배경에 대해 묻고 #Person2#가 클래식 음악을 좋아한다고 말합니다. #Person1#은 #Person2#에게 면접을 끝내고 #Person2#에게 클래식 음악을 추천합니다.\n",
      "test_156,\"#Person2#는 #Person1#에게 오늘 저희 집 아래에 사는 여자가 싱글이라고 말하고, #Person2#는 그녀와 함께 저녁 식사에 초대하려고 생각하고 있다고 말합니다.\"\n",
      "test_157,\"#Person1#과 #Person2#는 미렐라가 어제 회사에 어떤 식으로 왔는지에 대해 이야기하고 있다. #Person2#는 그녀가 캐주얼한 스타일을 입고 출근한다고 말한다. #Person2#는 미렐라가 흰 셔츠, 어두운 정장, 그리고 넥타이 차림을 하고 있고, 그녀가 네 벌의 새로운 흰색 원피스를 받았다고 말한다.\"\n",
      "test_158,#Person2#는 자신만의 법률 사무소를 세우려고 하고 있습니다. #Person1#은 #Person2#에게 도움을 청하고 행운을 빕니다.\n",
      "test_159,\"피터는 케이트에게 어젯밤에 온라인에서 게임을 했고, 이제 휴식을 취하고 싶다고 말했다.\"\n",
      "test_160,#Person1#과 #Person2#는 배와 보트가 교통수단으로 중요하지 않다고 생각합니다. #Person2#는 사람들의 생활 속도가 점점 빨라지고 있다고 말합니다.\n",
      "test_161,\"패니는 앤디에게 악몽을 꿨고 미친 밤이었고, 그녀의 엄마가 그녀를 깨웠다고 말한다. 앤디는 미시간 대학교에 들어가는 것에 대해 걱정하고 있다.\"\n",
      "test_162,#Person1#과 #Person2#는 밴드를 시작하고 싶어한다. 그들은 바닐라 아이스가 바람직한 선택인지 궁금해한다.\n",
      "test_163,#Person2#는 뉴올리언스 여행이 좋아서 오늘 밤에 극장에 가고 싶어합니다.\n",
      "test_164,#Person1#은 #Person2#의 도움으로 신용카드로 500위안을 구매했습니다.\n",
      "test_165,#Person1#은 블레이크 씨에게 포스터 씨가 전화를 걸어서 훈련 매뉴얼을 가져오라고 요청한다.\n",
      "test_166,#Person2#는 #Person1#에게 솔트레이크시티로의 4일간의 여행 계획을 말해줍니다. #Person2#는 #Person1#에게 #Person2#의 형과 그의 가족과 함께 그의 40번째 생일을 축하할 예정입니다.\n",
      "test_167,#Person1#은 #Person2#에게 파멜라가 오고 있을 때 #Person1#을 재촉해달라고 요청합니다. #Person2#는 #Person1#에게 파멜라가 다치지 않고 잘 지내길 바랍니다.\n",
      "test_168,#Person2#가 #Person1#에게 77번 거리를 가르치고 12번 버스를 타는 방법을 알려준다.\n",
      "test_169,\"#Person2#는 무슬림들이 무슬림 신앙의 영적 중심지인 메카를 순례하는 것을 소개하고, 그들의 순례가 많은 사람들이 사고로 인해 사망하는 이유를 설명한다. #Person1#은 프랑스에서 치유받으러 가는 곳이 있다고 #Person2#에게 말한다.\"\n",
      "test_170,#Person2#가 #Person1#에게 길을 안내하고 국립은행 위치를 알려준다.\n",
      "test_171,#Person1#과 #Person2#는 며칠 런던에 가고 싶어한다. #Person1#은 기차를 선호하지만 #Person2#는 차를 선호한다. 그들은 결국 차로 가기로 결정한다.\n",
      "test_172,\"톰은 점심에 닭고기가 익지 않은 닭고기가 있는 식당에서 점심을 먹었고, 캐서린은 집에서 싸온 점심을 먹었다.\"\n",
      "test_173,\"#Person2#님이 레모네이드, 바비큐 윙, 베이비 백 립을 주문합니다.\"\n",
      "test_174,\"#Person2#가 #Person1#에게 더블 치즈버거, 감자튀김, 펩시콜라를 주문하고, #Person2#는 5.48달러를 지불합니다.\"\n",
      "test_175,#Person2#는 #Person1#에게 계란을 프라이하는 방법을 가르치고 있다. #Person1#은 계란을 프라이하는 방법을 까다롭게 생각한다.\n",
      "test_176,#Person2#는 #Person1#에게 그랜드 호텔의 주소를 알려준다.\n",
      "test_177,#Person1#은 #Person2#의 사진을 찍고 싶어합니다.\n",
      "test_178,#Person2#는 내일 빈 방을 찾고 싶어합니다. #Person1#은 세 블록 떨어진 선셋 호텔을 추천합니다.\n",
      "test_179,#Person2#는 #Person1#에게 미국 공부를 위해 비자를 신청하고 작성하는 과정에 대해 이야기합니다. #Person2#는 비자를 거절당하는 사람들이 양식을 제대로 작성하지 않거나 필요한 서류를 모두 내지 않기 때문이라고 생각합니다.\n",
      "test_180,앤이 삼-오에게 밤새 웃고 이야기한 후 일찍 잘 거라고 말한다. 삼-오는 앤에게 술집에서 만나자고 제안하지만 앤은 거절한다.\n",
      "test_181,\"#Person1#과 메리는 온라인 쇼핑에 대해 이야기하고, 메리는 #Person1#에게 온라인 은행을 개설하는 방법을 가르친다.\"\n",
      "test_182,#Person2#는 #Person1#에게 미국 회계에 대해 설명합니다.\n",
      "test_183,\"제인은 피터에게 시안에 가는 여름에 함께 가자고 제안한다. 피터는 관심이 있고, 제인은 모니카가 돈이 부족하다고 말한다. 그들은 모니카가 남서부 지방으로 가고 싶어한다고 생각한다.\"\n",
      "test_184,\"#Person2#는 #Person1#의 도움으로 영화를 대여하고, #Person2#의 딸에게 보여줄 영화를 대여합니다. #Person2#는 #Person1#에게 비디오 대여 추적에 편리한 카드를 발급받았습니다.\"\n",
      "test_185,#Person2#가 미스터 리에게 전화를 걸었습니다.\n",
      "test_186,\"#Person2#는 #Person1#에게 내일 아침까지 비행기가 뜨지 못한다고 말합니다. #Person1#는 불을 켜놓지 않으면 잠을 못 자고, 불을 켜놓지 않으면 잠을 못 자는 습관이 있습니다.\"\n",
      "test_187,\"#Person1#은 #Person2#에게 컴퓨터를 도와준 것에 대한 보답을 원하지만, #Person2#는 식당을 찾아야 한다.\"\n",
      "test_188,#Person2#는 스트레스 때문에 불면증을 겪고 있다. #Person1#은 #Person2#에게 스트레스를 해소하는 방법을 알려준다. #Person1#은 #Person2#에게 음악을 듣고 휴식을 취하는 것을 제안한다. #Person2#는 시도해볼 것이다.\n",
      "test_189,#Person1#은 #Person2#에게 주방을 새롭게 만들고 싶다고 말합니다.\n",
      "test_190,월터와 스털링은 새로 오신 선생님에 대해 이야기하고 있다.\n",
      "test_191,#Person2#는 #Person1#에게 추가 검사를 하기 위해 약속을 잡습니다. #Person1#는 검사 결과가 좋지 않다고 생각합니다.\n",
      "test_192,마틴은 램 선생님의 도움으로 문제를 해결하고 램 선생님이 도와준 시험 준비를 하고 있습니다.\n",
      "test_193,#Person1#은 특급으로 소포를 한국에 보내고 싶어합니다. #Person2#는 패키지 우편이 있는데 더 저렴한 방법이 없다고 말합니다.\n",
      "test_194,\"#Person1#은 린다에게 휴대폰을 걸어달라고 요청하고, 린다는 결혼식에 그녀의 여동생을 위해 전화를 걸어달라고 요청한다.\"\n",
      "test_195,#Person1#은 영국인으로서 로스앤젤레스에 왔습니다. #Person2#는 #Person1#에게 음료를 가져다 줄 것입니다.\n",
      "test_196,#Person2#는 #Person1#에게 여름에 경영학 수업을 듣고 삼촌 회사에서 파트타임으로 일할 계획이라고 말한다. #Person1#은 #Person2#에게 좀 쉬라고 요청한다.\n",
      "test_197,\"#Person2#는 우유 배달 중에 울타리를 뛰어넘어 문 쪽으로 갔고, 그 후에 커다란 나무에 부딪혔다. #Person2#는 #Person1#에게 개가 누군가를 쫓아가고 있었다고 말한다.\"\n",
      "test_198,\"#Person2#는 #Person1#에게 회사가 6년 동안 변했다고 말합니다. #Person2#는 일을 더 오래 하고 있고, 매일 7시 넘어 퇴근합니다. #Person2#는 회사가 새로운 기술을 배울 수 있게 도와주고 있습니다.\"\n",
      "test_199,\"#Person1#은 당좌예금 계좌를 개설하고 싶어하고, #Person2#는 #Person1#에게 정보를 요청하고, #Person1#은 이자를 주지 않는 이유와 ATM에서 돈을 인출하는 방법에 대해 묻습니다.\"\n",
      "test_200,\"#Person2#는 #Person1#에게 중국 요리의 종류와 그 요리들을 소개하고, #Person1#은 베이징 요리를 먹고 싶어합니다. #Person2#는 #Person1#에게 전취덕 식당이 어디에 있는지 알려줍니다.\"\n",
      "test_201,\"#Person1#은 #Person2#에게 배송이 느려서 걱정하는 답을 받습니다. #Person2#는 배송이 3개월이라고 말하고, #Person1#은 #Person2#에게 최대 고객인 만큼 그들이 다음 달에 출발하는 첫 배에 주문 물량을 싣는 데 동의했습니다.\"\n",
      "test_202,#Person2#는 #Person1#에게 방의 독서등을 깨뜨린 것을 인정하고 새 방을 가져다 줄 것입니다.\n",
      "test_203,#Person1#과 #Person2#는 마케팅 계획의 첫 부분을 논의하고 있습니다. #Person2#는 #Person1#에게 2006 회계 연도 마케팅 계획을 평가하는 방법을 설명합니다.\n",
      "test_204,\"#Person2#는 휴가 동안 유럽을 여행할 계획이에요. #Person2#는 #Person1#에게 밀라노에서 돌아오고, 영국으로 가는 것을 제안합니다.\"\n",
      "test_205,\"#Person2#는 지미 폭스에게 중형 차량이 없다고 말합니다. 지미 폭스는 차량이 없을 때에도 예약을 받을 수 있다고 말합니다. #Person2#는 지미 폭스에게 컴팩트를 가져가고, 보험료와 대여료에 대해 할인을 드리겠다고 합니다.\"\n",
      "test_206,\"#Person1#은 엄마에게 카드 게임을 하고 싶어하지만, 엄마는 그에게 5분만 허락한다.\"\n",
      "test_207,앤이 존스 씨에게 커피를 하고 회의 일정을 알려준다.\n",
      "test_208,#Person2#는 #Person1#에게 세계 지도를 보여준다. #Person1#는 지리 수업을 준비하고 있다.\n",
      "test_209,#Person2#는 방에서 담배 냄새가 너무 심해서 체크아웃하고 금연 방을 원합니다.\n",
      "test_210,\"빌은 존 샘슨처럼 무심코 행동하지 않았지만, 그가 잊어버린 젖은 페인트 때문에 공지 없이 작업자들이 젖은 시멘트 위를 걸어가고 있다.\"\n",
      "test_211,#Person1#은 엘라에게 전화를 걸고 엘라는 #Person1#에게 #PhoneNumber#을 달라고 요청합니다.\n",
      "test_212,빌과 짐은 딕이 아파서 돌아오지 않았다.\n",
      "test_213,#Person2#는 #Person1#에게 야근에 대한 추가 급여가 없다고 말한다. #Person2#는 생산성이 가장 높은 직원에게는 보너스를 주지 않는다고 말한다. #Person2#는 #Person1#에게 다른 일자리를 찾지 못할까봐 두려워한다.\n",
      "test_214,마크는 리사에게 남편이 그녀를 속이고 있다고 말한다. 마크는 그녀에게 그가 대략 두 달 동안 다른 사람을 만났다고 말한다. 마크는 그녀에게 그가 진실을 말하거나 그와의 관계를 끝내지 않으면 이혼하겠다고 말한다.\n",
      "test_215,#Person2#는 #Person1#에게 적색 육류를 줄이고 식단을 정리하라고 조언한다.\n",
      "test_216,#Person1#은 #Person2#에게 #Person2#가 #Person1#을 떠나고 싶다고 말한다. #Person1#은 #Person2#가 #Person1#을 떠나고 싶다고 말한다.\n",
      "test_217,\"#Person2#는 #Person1#에게 도서관 이용 방법을 알려주고, 책을 어떻게 대출하고 몇 권의 책을 빌 수 있는지 알려준다.\"\n",
      "test_218,#Person1#과 #Person2#는 오늘 날씨가 정말 좋아서 점심을 먹고 밖에서 앉아서 햇빛을 쬐는 것을 좋아합니다.\n",
      "test_219,\"#Person1#은 #Person2#에게 사진을 찍을 수 없다고 말하고, #Person2#는 몰랐습니다. #Person1#은 #Person2#에게 카메라를 돌려주고, 출구 근처의 기념품 가게에서 엽서를 살 수 있다고 말합니다.\"\n",
      "test_220,#Person2#는 #Person1#에게 개인 대출에 대한 정보를 물어본다. #Person2#는 #Person1#에게 연이율이 10%인 만에 48개월 동안 만 달러를 빌린다면 매달 대출원금의 일부와 작은 부분을 갚아야 한다고 말한다.\n",
      "test_221,모니카는 #Person1#에게 그녀의 회의에서의 발표가 성공적이었고 모두가 그녀의 연설에 감동했다고 말한다. 모니카는 #Person1#에게 그녀의 노력이 보상받았다고 말한다.\n",
      "test_222,톰은 아침에 달리는 것을 좋아하지만 #Person1#은 아침에 달리는 것을 좋아하지 않습니다. 그들은 내일 시도해볼 것입니다.\n",
      "test_223,#Person2#는 #Person1#에게 일본 레스토랑의 음식과 영업 시간을 알려준다.\n",
      "test_224,\"#Person1#은 심슨 씨에게 점심 식사를 제안하고, 심슨 씨는 동의합니다.\"\n",
      "test_225,\"#Person1#은 #Person2#에게 그녀를 위해 최고의 레스토랑을 추천하고, 그들은 그들의 호텔을 선택할 수 있는지 물어보고 있다.\"\n",
      "test_226,#Person1#은 #Person2#에게 팁을 주지 않는다.\n",
      "test_227,\"#Person2#는 #Person1#에게 #Person2#의 나라에서의 스포츠에 대해 이야기합니다. #Person2#는 축구, 농구, 탁구, 수영, 익스트림 스포츠, 골프, 럭비를 좋아합니다. #Person1#는 #Person2#의 나라에서의 스포츠에 대해 이야기합니다.\"\n",
      "test_228,#Person2#가 헬싱키로 가는 비행기 예약을 하고 싶어합니다. #Person1#은 #Person2#에게 1070편 비행기와 90편 비행기가 있습니다. #Person2#는 채식주의자 식사를 요청합니다.\n",
      "test_229,#Person1#과 #Person2#는 내일 할머니를 방문할 예정입니다.\n",
      "test_230,#Person2#는 #Person1#에게 대장벽 산호초를 보고 싶다고 말합니다.\n",
      "test_231,\"로라는 헬스장에 가는 중이고, #Person1#은 운동하는 여성들이 남성들보다 많다고 생각한다. #Person1#은 걷는 것을 좋아하고, #Person2#는 야구를 좋아한다. 그들은 헬스장에 가는 것에 대해서도 이야기한다.\"\n",
      "test_232,\"#Person2#는 #Person1#에게 #Person2#가 시카고에서 태어났고, 14살 때 이사왔다고 말합니다. #Person2#는 유럽에서 11년 동안 뮌헨에 살았습니다.\"\n",
      "test_233,\"#Person1#은 #Person2#에게 자신의 새로운 정장을 보여주고, #Person2#는 그것이 좋은 가격이라고 생각하지 않습니다.\"\n",
      "test_234,\"#Person2#는 앤에게 마사 자전거 클럽의 회장인 로빈이 산호수 자전거 투어에 대해 설명합니다. 로빈은 평균적으로 7시간에서 9시간 동안 투어를 즐기고, 올해는 1,200명의 사이클리스트가 참가할 예정입니다.\"\n",
      "test_235,#Person2#는 #Person1#의 추천에 따라 싱거를 시도하고 맛있게 마셔서 즐거운 시간을 보냈습니다.\n",
      "test_236,에릭과 그레고리는 점심을 먹으러 가고 있다. 그들은 서로의 스포츠에 대한 의견을 나누고 있다. 그레고리는 자신이 번지 점프를 하지 않고도 잘 적응했다고 말한다.\n",
      "test_237,\"#Person1#은 제2차 세계 대전에 대한 정보를 찾고 싶어하고, #Person2#는 #Person1#에게 노르망디 상륙작전을 검색해보라고 제안합니다.\"\n",
      "test_238,#Person1#은 #Person2#에게 기계를 사용하는 방법을 가르쳐서 웨이트와 유산소 운동으로 좋은 체형을 갖고 싶다고 말합니다.\n",
      "test_239,\"#Person2#는 #Person1#에게 캠퍼스 내에 주차할 공간이 있다고 말하고, #Person1#은 지금 가득 차 있다고 말합니다.\"\n",
      "test_240,\"수잔은 에밀리에게 급여에 대한 세금과 공제에 대해 설명한다. 에밀리는 그녀의 급여를 설명하고, 그녀가 설명할 수 있는 이유를 이해한다.\"\n",
      "test_241,#Person1#은 간식을 사고 싶어하고 #Person2#는 #Person1#에게 기계를 사용하는 방법을 가르칩니다.\n",
      "test_242,#Person2#는 #Person1#에게 #Person2#가 태국에 여자친구를 만나러 가려고 한다고 말한다. #Person2#는 #Person1#에게 #Person2#가 인터넷을 통해 여자친구를 만났다고 말한다.\n",
      "test_243,제임스는 케이트에게 그녀의 가구를 보여준다. 케이트는 새 가구를 사고 장식을 다시 하려고 돈을 모았다.\n",
      "test_244,#Person2#는 빌에게 하루 운동 스케줄을 말해준다. 빌은 그것이 너무 힘들다고 생각한다.\n",
      "test_245,#Person1#과 마르켓은 과학 과목에 등록해야 한다고 생각합니다. 마르켓은 #Person1#에게 비과학 전공자를 위한 좋은 입문 과목이라고 말합니다.\n",
      "test_246,팀은 #Person1#에게 자신의 프로젝트와 더욱 친환경적인 삶을 어떻게 이끌어갈 것인지에 대해 이야기한다. 팀은 자신의 가족들과 친환경적으로 살려고 노력하고 있다고 말한다. 팀은 자신의 학교에서 실제로 친환경적으로 행동한다고 말한다.\n",
      "test_247,#Person1#은 토니에게 쇼핑 센터의 장난감 부서에서 일하고 있다고 말한다. #Person1#은 최근에 늦게 일어나고 장난감 부서의 사람들이 그를 밀거나 화내며 요구했기 때문에 피곤하다고 말한다.\n",
      "test_248,\"빌은 수가 다이어트 중이기 때문에 케이크를 먹지 못한다. 수는 톰과 쉘린에게 다른 케이크를 가져왔고, 샐러드를 가져다 줄 것이다. 빌은 수에게 식당에 가서 수프를 가져다 줄 것이다.\"\n",
      "test_249,#Person2#는 여름 옷을 찾고 있습니다. #Person1#은 #Person2#에게 20% 할인된 센스를 갖춘 티셔츠와 치마를 추천합니다.\n",
      "test_250,#Person2#는 #Person1#에게 재확인 없이 예약을 취소해야 한다고 말합니다.\n",
      "test_251,#Person2#는 #Person1#에게 부활절에 어린이들이 집에서 기다리는 것을 말한다.\n",
      "test_252,\"#Person1#은 토니가 상처 주는 말을 하는 말을 해서 걱정하고 있습니다. #Person2#는 토니에게 인내심, 애정, 존중을 가지고 대해 줘야 한다고 제안합니다. #Person1#은 토니에게 보상함으로써 토니를 협력하게 하고 싶어합니다.\"\n",
      "test_253,#Person1#은 #Person2#에게 알렉스와 그의 베스트맨이 남자들만의 파티를 준비하고 있다고 말한다. #Person1#은 #Person2#에게 #Person1#이 스트립 클럽에 가지 않을 것이라고 말한다. #Person2#는 결혼한 후 #Person1#이 #Person2#의 파티에 가는 것을 신경 쓰지 않을 것이라고 말한다.\n",
      "test_254,#Person2#는 투표용지를 받고 투표합니다. #Person1#은 #Person2#에게 투표 부스로 가서 투표하라고 합니다.\n",
      "test_255,\"#Person1#과 #Person2#는 회사가 인력을 줄일 것이라는 소식을 들었고, 그들은 상사가 너무 엄격하고 동료들이 어떻게 옷을 입는지도 모르는 상황에서 #Person1#을 숨막히게 해. 그들은 상사가 너무 엄격하고 동료들이 어떻게 옷을 입는지도 모르는 상황에서 #Person1#을 숨막히게 해.\"\n",
      "test_256,#Person1#과 #Person2#는 중국에서의 이혼률과 중국 스타일 이혼에 대해 이야기하고 있다.\n",
      "test_257,\"주디는 #Person1#에게 여행 일정에 따르면 총 금액이 13,000 RMB 이상이라고 말한다. 그녀는 #Person1#에게 돈을 아끼기 위해 유스호스텔을 선택할 수 있다고 말한다.\"\n",
      "test_258,#Person1#과 #Person2#는 메리가 결혼했다는 사실에 놀랐다.\n",
      "test_259,\"#Person2#는 지갑을 잃어버렸고, 도둑맞은지 물어보고 #Person1#에게 도움을 청한다.\"\n",
      "test_260,#Person1#과 #Person2#는 머피 뮤직과 U-튠즈가 합병할 것이라고 합니다. #Person2#는 이것이 의심스럽다고 생각합니다.\n",
      "test_261,조는 톰에게 그녀가 거만한 것 같다고 말한다. 톰은 그녀가 그녀의 상사에게 도움이 되고 있다고 생각한다.\n",
      "test_262,\"사라는 회의에서 밥이 자신의 제안으로 다른 사람들을 싸우고 서로를 탓하는 바람에 화가 나서 회의를 주재한 사람이 밥이었다. 사라는 회의 중에 중요한 아이디어를 공유하고 싶었지만, 밥이 너무 급해지지 않았다.\"\n",
      "test_263,#Person2#는 #Person1#에게 무슬림들이 무슬림 신앙의 영적 중심지인 메카를 향해 하즈를 가는 무슬림들이 무엇을 하고 있는지 설명한다. #Person2#는 또한 영국과 프랑스에서 기독교인들이 치유를 받으러 가는 곳에 대해 이야기한다.\n",
      "test_264,\"테드는 제니에게 반했고, 마이크는 그에게 말해야 할 것이라고 제안한다. 마이크는 테드에게 그녀에게 말해야 할 것이라고 말한다.\"\n",
      "test_265,#Person2#는 화를 일으킨 원인에 대해 생각하고 음악을 듣거나 산책을 하는 것을 좋아합니다.\n",
      "test_266,#Person2#는 #Person1#에게 뉴질랜드의 가족에 대해 이야기한다. #Person2#는 #Person1#에게 빌의 형제인 잭과 그의 딸 사라와 제인이 #Person1#의 사촌이라고 말한다. #Person2#는 또한 빌의 아내가 사라의 엄마가 돌아가셨다고 말한다.\n",
      "test_267,스튜어트 씨가 마라톤에서 우승했습니다. #Person1#과 #Person2#는 그의 성과에 대해 칭찬했습니다.\n",
      "test_268,\"#Person2#는 #Person1#에게 아이들을 위한 선물을 고르는 데 도움을 청하고, #Person1#은 #Person2#에게 5켤레의 운동화를 추천합니다. #Person2#는 #Person1#에게 가격을 물어보고, #Person1#은 #Person2#에게 DENY 브랜드의 향수를 추천합니다. #Person2#는 #Person1#에게 하나를 사겠습니다.\"\n",
      "test_269,#Person1#은 로밍 요금을 감당할 수 없다. #Person2#는 #Person1#에게 SIM 카드를 사용하라고 제안한다.\n",
      "test_270,\"#Person2#는 #Person1#에게 #Person2#의 급여가 월 2,500 위안이라고 말합니다. #Person2#는 #Person1#에게 #Person2#의 급여가 #Person2#의 능력과 경험에 따라 결정될 수 있다고 말합니다.\"\n",
      "test_271,\"#Person2#는 배리와 폴을 좋아하지 않지만, 폴은 엄청 사교적이고 형보다 훨씬 더 정직하다고 생각한다. #Person2#는 자신을 예의바르고, 조심스럽고, 여유로우며, 수줍은 사람으로 생각한다. #Person1#는 낯을 가린다고 생각하지만, 왜냐하면 #Person1#는 새로운 사람들과 이야기하고 있다고 말한다.\"\n",
      "test_272,#Person1#은 #Person2#에게 파사디나나 아카디아에서 중간 크기의 집을 사고 싶다고 말합니다.\n",
      "test_273,\"샘플을 남기고 가는 것을 제안하고, 그랜트 씨는 감사합니다.\"\n",
      "test_274,#Person2#가 #Person1#에게 Sons로 가는 버스를 어떻게 타야 하는지 알려준다.\n",
      "test_275,\"#Person1#은 워드 여사님을 태워주고, 그녀에게 우산을 꺼내라고 요청합니다.\"\n",
      "test_276,\"#Person2#는 #Person1#에게 엑스레이를 찍어야 한다고 말합니다. #Person2#는 아픈 곳이 없지만, 아픈 곳이 있다고 말합니다. #Person1#는 #Person2#에게 하룻밤 머무르는 것을 제안합니다.\"\n",
      "test_277,\"#Person2#는 100% 확실하지는 않지만, 카드를 잃어버린 것 같다고 생각합니다. #Person1#는 #Person2#의 FRCM 일부 세부 사항을 받아서 도와줄 것입니다.\"\n",
      "test_278,#Person1#과 #Person2#는 잠을 자고 싶어한다. #Person2#는 #Person1#에게 깨우지 않도록 하기 위해 전화기 뽑아 놓는 것을 제안한다.\n",
      "test_279,\"#Person2#는 중국 음식을 원하고 #Person1#에게 2인분 식사를 요청합니다. #Person1#은 #Person2#에게 고추장을 주고, #Person2#는 소고기를 맛보고 도움이 필요한 경우에 버튼을 눌러주세요.\"\n",
      "test_280,#Person1#과 #Person2#는 식사를 하고 있다. 그들은 차례가 되면 웨이터에게 원하는 것을 말해야 한다.\n",
      "test_281,\"루시는 노래 부를 사람 없고, #Person1#은 그녀에게 스탠리가 왔다고 말한다.\"\n",
      "test_282,#Person2#는 #Person1#에게 영화관이 단지 영화를 보는 곳이 아니라 사람들을 만나는 곳이라고 말합니다.\n",
      "test_283,#Person1#은 #Person2#에게 봄에 적합한 흰색 드레스를 추천합니다.\n",
      "test_284,\"조슈아는 #Person2#에게 그녀의 용돈을 주고, #Person2#는 돈을 쓸 수 있는 방법을 찾고 있다.\"\n",
      "test_285,#Person2#는 마이크의 생일 파티에 가고 싶어하고 #Person1#에게 차로 갈 수 있도록 요청한다.\n",
      "test_286,\"#Person1#은 #Person2#에게서 우등 우편으로 소포를 보내고, #Person2#는 #Person1#에게 우표를 주문하는 데 도움을 줍니다.\"\n",
      "test_287,#Person2#는 #Person1#에게 태풍이 지진보다 심하지 않다고 말합니다. #Person1#는 중국이 지진으로 인해 많은 사람들이 죽었다고 말합니다. #Person2#는 중국이 지진에 시달린 역사를 #Person1#에게 말합니다. #Person1#는 인간 간의 사랑이 지리에 영향을 받지 않는다고 말합니다.\n",
      "test_288,\"#Person1#과 #Person2#는 비가 내리는 날씨에 대해 이야기하고, #Person2#는 #Person1#에게 봄이 올 것이라고 말합니다.\"\n",
      "test_289,\"#Person1#과 #Person2#는 올림픽 기념품 가게에 가서 둘러보고 있습니다. #Person1#은 올림픽 마스코트를 사고 싶어합니다. #Person2#는 세트를 여러 개 사는 것을 제안하지만, #Person1#은 다른 것들이 있는지 궁금합니다.\"\n",
      "test_290,#Person1#은 양이 많은 음식을 요리하기 때문에 #Person2#가 추천한 알루미늄 팬을 사고 싶어합니다.\n",
      "test_291,#Person1#과 #Person2#는 퇴근 후에 책상 위에 할 일이 산더미처럼 쌓여 있기 때문에 퇴근 후에 타고 가기로 결정했습니다.\n",
      "test_292,#Person1#은 베커 씨에게 사무실을 보여준다. 베커 씨는 #Person1#에게 연방 직원들이 성인이 된 후에 평생 연방 정부에서 일한다고 말한다.\n",
      "test_293,\"피트와 헨리 존슨은 베이징에서 만났습니다. 헨리 존슨은 피트에게 맥주를 좋아한다고 말하고, 그들은 노래방에 가기로 결정합니다.\"\n",
      "test_294,줄리는 존스 선생님에게 식중독으로 아파서 수업을 따라갈 수 없다고 말한다. 그녀는 점심을 먹은 후에 의사에게 식중독이라고 말한다. 그녀는 인터넷으로 생물학 수업을 다운로드 받을 수 있다.\n",
      "test_295,#Person1#은 마이크에게 오늘 저녁에 친구들과 함께 오늘 저녁에 준비할 것이라고 말한다. 마이크는 과일 주스와 콜라를 사러 간다. 마이크는 또한 #Person1#에게 설탕을 사오라고 요청한다.\n",
      "test_296,\"#Person1#과 #Person2#는 각자의 비행 여행에 대해 이야기한다. #Person1#은 버스가 친환경적이지만, #Person2#는 버스가 불편했다고 말한다.\"\n",
      "test_297,\"#Person2#는 #Person1#에게 일에 대해 이야기하고, #Person2#는 주말에도 일해야 한다고 말합니다. #Person2#는 외국어를 할 수 있고, 목소리도 맑습니다.\"\n",
      "test_298,\"지미는 에이미에게 제니와 빌이 아침 9시쯤 차로 제니와 빌을 소풍에 초대해서 집에 엄청 늦게 왔다고 말한다. 그들은 강가에서 점심을 먹고 친구들을 사귀고, 차를 몰고 집에 가기 시작했는데, 그들은 술을 마시고 웃다가 집에 도착했다.\"\n",
      "test_299,피터는 건조해서 물을 좀 주고 차를 마시려고 합니다. #Person1#은 비가 오기 때문에 물을 주지 않아서 피터는 차를 마실 수 있습니다.\n",
      "test_300,#Person1#과 #Person2#는 켄이 돌아오지 않을 경우에도 주문을 잃지 않도록 준비하고 있습니다.\n",
      "test_301,#Person2#가 등기로 편지를 보내기 위해 #Person1#에게 5그램 초과된 우표를 5달러에 구매합니다.\n",
      "test_302,#Person1#은 스미스 씨에게 감염을 진단하고 약을 처방합니다.\n",
      "test_303,\"#Person2#는 중국의 예술과 공예품이 훌륭하다고 생각하고, #Person1#은 #Person2#에게 종이절편, 자수, 바틱을 추천합니다.\"\n",
      "test_304,#Person1#과 #Person2#는 유명한 에펠탑에 대해 이야기하고 있습니다.\n",
      "test_305,\"#Person1#은 브라이언에게 영어를 할 수 있다고 말하고, 브라이언은 캘리포니아와 라스베가스 출장에 대해 이야기한다.\"\n",
      "test_306,#Person2#는 #Person1#에게 #Person2#의 고양이가 여전히 뜨거운 벽돌 위에 있다고 말합니다.\n",
      "test_307,#Person1#은 #Person2#에게 계획을 세우고 계획을 따라가는 것을 권한다.\n",
      "test_308,#Person1#과 #Person2#는 존이 일주일에 일곱 번 그녀와 데이트를 한다고 말합니다.\n",
      "test_309,#Person1#과 #Person2#는 런던의 역사적 장소에 대해 이야기하고 있습니다. 그들은 런던의 역사적 인물들과 그들의 역사적 장소를 방문하고 싶어합니다.\n",
      "test_310,\"다니엘은 과학이 가장 좋아하고, 과학에 관심이 있다고 말합니다. 다니엘은 과학에 관심이 있다는 사실을 #Person2#에게 말하고, #Person2#는 그가 장난꾸러기라고 말합니다.\"\n",
      "test_311,#Person1#과 #Person2#는 #Person1#의 베이비 샤워를 준비하고 있다. #Person1#은 베티에게 선물을 준다. #Person2#는 #Person1#에게 선물을 준다. #Person1#은 #Person2#에게 #Person1#의 양수가 터진 것 같다고 말한다.\n",
      "test_312,\"#Person1#은 중국에 관광하고 싶어하고, #Person2#은 지금 바빠서 동의하지 않습니다.\"\n",
      "test_313,팀과 카렌은 춤을 추다가 멈추고 퇴근한다.\n",
      "test_314,\"#Person2#는 #Person1#에게 오토바이를 샀고, 자전거를 사고 싶다고 말했습니다.\"\n",
      "test_315,#Person2#는 #Person1#에게 영어와 프랑스어를 할 수 있다고 말합니다. #Person2#는 영어를 충분히 잘해서 영어권 국가 사람들과 의사소통이 가능합니다.\n",
      "test_316,타일러가 나오미의 여동생인 낸시에게 메시지를 전달합니다.\n",
      "test_317,\"#Person1#은 뉴욕행 비행기를 취소받았습니다. #Person2#는 #Person1#에게 다른 항공사로 예약하라고 조언하고, #Person1#은 줄을 싫어합니다.\"\n",
      "test_318,\"#Person1#과 #Person2#는 점심에 버거퀸에 가자고 제안하고, 그들은 치즈버거와 밀크쉐이크를 먹기로 결정한다.\"\n",
      "test_319,#Person1#은 #Person2#의 짐이 초과 수하물 요금을 내야 한다고 말합니다.\n",
      "test_320,#Person1#은 목이 말라 죽겠다고 말합니다. #Person2#는 #Person1#에게 무엇을 마시고 싶은지 물어봅니다.\n",
      "test_321,#Person2#는 AB 회사의 왕 묘가 내일 갑작스런 긴급 사항으로 왕 묘가 갑작스러운 긴급 사항으로 갑작스럽게 떠납니다. 그린 씨는 량 씨에게 사과의 말을 전하고 싶어합니다.\n",
      "test_322,\"#Person1#은 굽이 조금 꽉 끼는 신발을 원하고, #Person2#는 사용하면서 약간 늘어나는 신발을 추천합니다.\"\n",
      "test_323,#Person1#은 벤자민에게 프로젝트 보고서를 어떻게 써야 하는지 알려준다. #Person1#은 벤자민에게 올해의 작업에 대한 최종 결론과 내년 계획에 대한 세부 사항을 포함하라고 말한다.\n",
      "test_324,\"#Person2#는 피자를 주문하고, #Person1#은 #Person2#에게 도우를 얇게 만들어 주고, #Person2#의 주소를 물어본다.\"\n",
      "test_325,\"#Person1#은 #Person2#의 도움으로 새우 칵테일, 양파 스테이크, 블랙 커피, 그리고 미네랄 워터를 주문합니다.\"\n",
      "test_326,#Person1#은 #Person2#에게 집에 대한 정보를 알려준다.\n",
      "test_327,\"#Person2#는 보여준 긁힘과 소음이 있는 개인용 스테레오에 대해 #Person1#에게 불만을 표합니다. #Person1#은 #Person2#에게 다른 모델을 제공할 수 있을지 물어보고, 만약 그렇다면 영수증을 보여주는 것을 요청합니다.\"\n",
      "test_328,\"#Person1#은 #Person2#에게 내년 기숙사 보증금을 내는 데 한 시간이나 걸렸다고 말한다. #Person2#는 캠퍼스 밖에서 살 계획이지만, #Person1#은 #Person2#에게 도미토리가 너무 시끄러워서 아무것도 할 수 없다고 말한다. #Person2#는 4월이 2주나 남았다고 말한다.\"\n",
      "test_329,#Person1#과 #Person2#는 중고 서점에서 책을 둘러보고 있다. #Person1#은 #Person2#에게 19세기 연극에 관심이 생긴 소설을 보여준다. #Person2#는 #Person1#에게 75센트짜리 소설을 사는 것을 추천한다.\n",
      "test_330,\"#Person2#는 베이징 맛있는 레스토랑에서 #Person1#에게 중앙에 테이블을 요청하고, #Person1#는 #Person2#에게 음식을 20분 후에 준비하도록 요청합니다.\"\n",
      "test_331,#Person2#는 #Person1#에게 시계를 공인 우편이나 등기 우편으로 보내는 방법을 알려준다.\n",
      "test_332,\"#Person1#이 톰에게 편지를 보냈고, 톰은 #Person1#에게 존에게 전화를 걸어 연락을 주고 싶다고 말한다.\"\n",
      "test_333,#Person1#은 컴퓨터를 구입할 수 있는 날을 기대하고 있습니다.\n",
      "test_334,\"#Person1#과 #Person2#는 맑고 화창한 여름 날에 대해 이야기하고, #Person2#는 #Person1#에게 가을을 좋아한다고 말합니다.\"\n",
      "test_335,#Person1#은 #Person2#에게 컴퓨터에 대한 일반적인 정보를 찾고 있습니다.\n",
      "test_336,\"#Person1#이 프렌치 가든 레스토랑에 #Person2#를 받아주고, #Person2#가 웨이트리스와 함께 몇 가지 음식을 주문합니다.\"\n",
      "test_337,\"#Person2#가 레모네이드, 바비큐 윙, 그리고 베이비 백 립을 주문합니다.\"\n",
      "test_338,#Person2#가 #Person1#에게 커피를 주문하고 나중에 디저트를 주문한다.\n",
      "test_339,#Person2#는 #Person1#에게 로또에 당첨한다면 세계 여행을 위한 티켓 두 장을 사겠다고 말한다. #Person1#의 아빠는 그런 여행을 언급만 해도 놀라서 기절할 거라고 말한다.\n",
      "test_340,#Person1#이 새 강아지 사진을 #Person2#에게 보여준다. #Person2#는 새로 태어난 아기가 귀엽다고 생각한다.\n",
      "test_341,#Person1#은 제인 이모가 톰에게 새 자전거를 사줬다고 #Person2#에게 말한다. #Person2#는 #Person1#에게 톰의 자전거가 #Person1#의 자전거보다 훨씬 좋아한다고 말한다. #Person1#은 #Person2#에게 버스가 아직 안 왔을 때 기다리는 것을 싫어한다고 말한다.\n",
      "test_342,\"#Person2#는 #Person1#의 추천에 따라 치마를 입어보고, 400달러에 치마를 구매합니다.\"\n",
      "test_343,\"#Person2#는 #Person1#에게 #Person2#의 도시가 200년 전만 해도 보잘것없는 작은 마을이었지만, 빠르게 주요 산업 중심지로 성장했다고 말합니다.\"\n",
      "test_344,#Person1#과 #Person2#는 환경 문제에 대해 이야기하고 있습니다. 그들은 환경 문제가 세계의 모든 사람들에게 영향을 미친다고 생각합니다. 그들은 환경 보호에 헌신하는 조직에 가입하는 것을 고려하고 있습니다.\n",
      "test_345,데니스는 16살 소녀로 가장했던 남자친구를 만났습니다. #Person2#는 데니스에게 좋은 의사를 찾으라고 조언합니다.\n",
      "test_346,\"네이선은 #Person1#에게 겨울 방학을 위해 시카고로 떠난다고 말한다. 그는 #Person1#에게 시카고에서의 경험을 쌓는 것에 대해 걱정하지 않고, 타고난 호기심이 많고 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은 매우 사교적이므로, 그들은\"\n",
      "test_347,\"#Person2#는 토요타 캐롤라를 빌리고 싶어하고, #Person1#은 #Person2#에게 차 키를 주고, #Person2#는 신청서를 작성하고 키를 반납하는 방법을 알려준다.\"\n",
      "test_348,\"#Person2#는 #Person1#에게 뉴욕에서의 여행 계획을 돕고 있습니다. #Person2#는 #Person1#에게 UN 건물, 브로드웨이, 대학교, 그리고 지도를 보여줍니다.\"\n",
      "test_349,#Person2#는 #Person1#에게 필름을 현상해 줄 예정입니다.\n",
      "test_350,\"#Person1#은 비행기가 지연되었고, #Person2#는 #Person1#에게 비행기 번호를 물어봅니다. #Person2#는 #Person1#에게 비행기가 폭우 때문이라고 말합니다. #Person1#은 기다려야 합니다.\"\n",
      "test_351,#Person2#는 #Person1#에게 베이징 대학교로 가는 길을 알려준다.\n",
      "test_352,\"#Person2#는 #Person1#에게 신문에 나온 소식을 전한다. #Person2#는 신문에 따르면 오늘은 맑고 따뜻하고, 내일은 흐리고 비가 올 것 같다고 말한다.\"\n",
      "test_353,#Person1#은 컴퓨터 게임이 너무 폭력적이라고 생각한다. #Person2#는 그렇게 생각하지 않는다.\n",
      "test_354,\"#Person1#과 짐은 맥주를 마시고 싶지만, 짐은 건강에 해롭다고 생각한다. 그들은 노래를 부르고 친구들을 만나는 것을 제안한다.\"\n",
      "test_355,#Person1#과 #Person2#는 하우스 레드를 주문하고 싶어한다.\n",
      "test_356,#Person2#는 #Person1#에게 겨울 방학 동안 미국에서 온 외국 관광객들을 안내했다고 말합니다.\n",
      "test_357,#Person1#은 잭의 예약을 받아서 양식을 작성하는 데 도움을 줍니다.\n",
      "test_358,마크는 #Person2#에게 전화를 걸어 빌의 아내가 딸을 낳았다고 말한다. #Person2#는 빌에게 축하를 건다.\n",
      "test_359,\"#Person1#은 #Person2#에게 놀래러 초대하지만, #Person2#는 거절한다.\"\n",
      "test_360,#Person2#는 #Person1#에게 캘리포니아가 홈런으로 리픈의 홈런으로 1루에 선수가 있었기 때문에 5대 4가 됐다고 말합니다.\n",
      "test_361,#Person2#는 #Person1#에게 감사의 말을 전합니다.\n",
      "test_362,#Person2#는 #Person1#에게 #Person2#의 나라에서의 자원 수출과 수입에 대해 이야기합니다. #Person2#는 #Person1#에게 #Person2#의 나라에서의 자원 수출과 수입에 대해 이야기합니다.\n",
      "test_363,#Person1#과 #Person2#는 예술에 대해 이야기하고 있습니다. #Person1#은 #Person2#에게 국립 갤러리에서 열리는 전시회에 가자고 제안합니다. #Person2#는 조각을 좋아합니다. #Person1#은 #Person2#에게 12시 30분에 만나자고 제안합니다.\n",
      "test_364,#Person2#는 비디오를 반납하고 #Person1#에게 비디오를 잘 관리하도록 요청합니다.\n",
      "test_365,#Person1#은 #Person2#에게 볼링이 10개의 프레임에서 두 번의 볼링을 할 수 있다고 말합니다.\n",
      "test_366,#Person2#는 #Person1#에게 은행과 증권사 간의 새로운 서비스에 대해 묻습니다. #Person1#은 #Person2#에게 전화로 상담 서비스를 이용할 수 있다고 말합니다.\n",
      "test_367,#Person1#은 짐을 보관할 공간이 필요합니다. #Person2#는 #Person1#에게 짐을 보관할 수 있는 공간이 있다고 말합니다. #Person1#은 짐을 남기는 것만으로도 충분한 보증이 있다고 말합니다.\n",
      "test_368,\"#Person2#는 파티에 세 명이 오고 싶어합니다. #Person1#은 #Person2#에게 음식, 음료, 그리고 콜라나 주스에 대한 추가 요금에 대해 알려줍니다.\"\n",
      "test_369,\"#Person2#는 해외에서 공부할 예정이고, #Person1#은 #Person2#에게 해외유학 대출을 제공한다. #Person2#는 부모님이 대출을 보증해야 한다고 말하고, #Person1#은 #Person2#의 나이 제한에 대해 설명한다.\"\n",
      "test_370,#Person1#은 #Person2#에게 영어 노래가 있나요?\n",
      "test_371,\"#Person2#는 #Person1#에게 #Person2#가 글쓰기 경험이 있다고 말하고, #Person2#가 소설을 쓰는 중이라고 말합니다.\"\n",
      "test_372,#Person1#과 #Person2#는 면접 결과에 대해 물어보는 것이 최선이라고 생각한다. #Person2#는 #Person1#에게 문의서를 작성하는 것에 대해 조언한다.\n",
      "test_373,#Person1#과 #Person2#는 #Person2#의 집을 둘러보고 있습니다. #Person2#는 #Person1#에게 나무 가지에 매달린 것이 옥수수 이삭이라고 말합니다.\n",
      "test_374,\"#Person1#은 40분 후에 호텔에서 체크아웃할 예정입니다. #Person2#는 #Person1#에게 수하물을 맡길 수 있으며, #Person1#은 수하물이 충분한 보증금을 납부하지 못하면 문제가 있을 수 있다고 말합니다.\"\n",
      "test_375,\"브랜든이 웹사이트를 가입하려고 하고 #Person1#이 그에게 무료라고 말한다. 브랜든은 #Person1#이 신뢰할 수 없다고 생각하지만, #Person1#은 #Person2#가 그들의 목표를 설명하고 있다고 생각한다. #Person1#은 #Person2#가 개인 정보와 신원을 훔치려고 하는 확실한 징조라고 생각한다.\"\n",
      "test_376,#Person1#과 #Person2#는 새로운 실험실 건물이 필요하다고 생각합니다. #Person1#은 교장이 돈을 모을 수 있도록 도움을 청할 것이라고 생각합니다.\n",
      "test_377,\"케이트는 헨리가 신용카드를 가지고 다니기 위해 가게에 들렀고, 카드로 소고기를 사기 위해 가게에 들렀습니다. 헨리는 케이트에게 카드를 사용하는 것을 조심하라고 말합니다.\"\n",
      "test_378,#Person1#은 내일의 계획에 대해 스미스씨에게 알려준다. 스미스씨는 산악 지역의 마을을 선호한다.\n",
      "test_379,\"#Person1#은 벳에게 스트레스와 우울증을 다루는 데 어떤 방법을 사용하는지 물어봤다. 벳은 #Person1#에게 아기를 가지는 것 중에서 가장 좋아하는 부분은 무엇인지, 미래의 계획과 목표는 무엇인지, 그리고 가장 좋아하는 명언은 무엇인지 물어봤다.\"\n",
      "test_380,#Person2#는 우표 수집에 관심을 가지게 된 것은 9년 동안의 사랑 때문입니다. #Person2#는 첫 우표의 내용과 사진을 소개합니다.\n",
      "test_381,\"톰은 #Person1#에게 톰이 부유한 회사를 가지고 있다고 말합니다. 톰은 친환경 제품을 판매하는 회사의 관리자로서 성공한 사업가입니다. 톰은 돈을 모으기 위해 많은 노력을 기울였고, 그들은 재료, 제품 개발, 디자인을 제공합니다.\"\n",
      "test_382,\"#Person2#는 #Person1#에게 술집에서 발견된 남자의 죽음을 살인으로 다루고 있다고 말합니다. #Person2#는 #Person1#에게 남자가 술집을 떠난 시간, 간격, 그리고 다른 사람들에 대해 이야기합니다.\"\n",
      "test_383,셰리는 밥에게 퀘벡의 여름에 관광을 가는 것을 제안하고 밥은 프랑스어를 사용할 수 있을까 궁금해한다. 밥은 퀘벡 시가 세계에서 세 번째로 큰 프랑스어 사용 도시라고 말한다.\n",
      "test_384,톰은 제인에게 수영하러 가자고 제안하지만 제인은 논문을 마무리해야 한다. 그들은 톰과 제인이 함께 시간을 보내고 연구를 할 수 있도록 계획한다.\n",
      "test_385,#Person1#은 등에 통증이 있습니다. #Person2#는 #Person1#에게 약을 복용하라고 제안합니다.\n",
      "test_386,\"#Person1#은 휴대폰이 망가졌고, 프레드는 그녀에게 돈을 빌려달라고 요청한다. 프레드는 그녀가 돈을 갚을 때까지 괜찮다고 말한다.\"\n",
      "test_387,\"#Person1#은 글씨를 개선하고 싶어하고, #Person2#는 #Person1#에게 인내심을 가져야 한다고 말합니다.\"\n",
      "test_388,#Person2#는 #Person1#에게 #Person2#가 속눈썹을 집고 있다고 말한다. #Person1#은 #Person2#가 속눈썹을 집을 때마다 무서워한다고 말한다.\n",
      "test_389,#Person2#는 주말에 집중적으로 운전 수업을 받고 싶어합니다. #Person1#은 #Person2#에게 코치가 훌륭하고 연수 기회가 많다고 말합니다.\n",
      "test_390,피나가 #Person1#에게 티나가 8년 동안 피아노를 배웠다고 말합니다. 피나는 #Person1#에게 선생님을 소개하고 싶어합니다.\n",
      "test_391,#Person2#는 #Person1#에게 #Person2#의 장점과 약점에 대해 말합니다. #Person2#는 #Person1#에게 5년 후에 어디에 있을 것인지 물어봅니다. #Person2#는 #Person1#에게 #Person2#가 잘 맞는지 아닌지를 물어봅니다.\n",
      "test_392,스테파니는 잠을 거의 못 잤고 피곤해. 조지는 그녀에게 금요일 아침에 제출할 수 있게 해달라고 요청한다.\n",
      "test_393,\"#Person1#은 데이비드에게 스케이트를 타고 있는 호수에 얼음이 얹혀 있고, 도시로 돌아가지 않을 것이라고 말한다.\"\n",
      "test_394,밥은 토요일 아침에 TV를 보고 오후에 댄스 파티에 갔다가 댄스를 즐겼다.\n",
      "test_395,#Person1#은 일자리가 없지만 #Person2#에게 50달러를 빌려달라고 요청한다. #Person2#는 결국 #Person1#에게 행운을 가져다 줄 수 있다.\n",
      "test_396,#Person2#는 회장님을 위해 재킷과 넥타이를 빌리고 싶어합니다. #Person1#은 #Person2#에게 9시 30분에 돌아오라고 요청합니다.\n",
      "test_397,\"#Person2#는 #Person1#에게 아이들이 캠프에서 즐거운 시간을 보냈는지에 대해 이야기합니다. #Person2#는 아이들이 홀리로드, 성을 방문하는 것, 그리고 바베큐 파티를 즐겼다고 말합니다.\"\n",
      "test_398,#Person1#은 #Person2#가 대학 입학 요건에 대해 그렇게 많은 질문을 하지 않았다고 생각합니다. #Person2#는 #Person1#에게 사업을 시작하려고 하고 휴가 시간을 이용해서 몇몇 수업을 들어보고 싶다고 말합니다.\n",
      "test_399,칼리나는 차를 나무에 박아서 몇 일 동안 학교를 쉬고 있습니다. 클라크 교수님은 그녀에게 전화로 알려줍니다.\n",
      "test_400,\"#Person2#는 집주인과 수리 비용에 대해 합의를 이루지 못하고 있다. #Person2#는 집주인에게 불평하고 있고, 그에게 2개월 이상 집에 대한 돈을 우편으로 보냈다. 하지만 집주인은 그에게 수리 비용을 지불하지 않겠다고 했다. #Person2#는 집주인에게 돈을 보냈지만, 집주인은 그에게 수리 비용을 지불하지 않겠다고 했다.\"\n",
      "test_401,#Person1#은 #Person2#에게 터너 인테리어에서 수출용 L / C를 수령하는 방법을 알려줍니다.\n",
      "test_402,#Person1#이 찰리에게 지갑을 찾아달라고 요청한다. 찰리는 그녀에게 지갑이 사라지지 않았다고 말한다.\n",
      "test_403,#Person1#과 #Person2#는 설거지를 번갈아서 합니다.\n",
      "test_404,\"#Person1#과 #Person2#는 서로를 사랑하고 있다. #Person2#는 #Person1#에게 미안하지만, #Person1#은 그를 사랑한다.\"\n",
      "test_405,#Person1#과 #Person2#는 실업에 대해 이야기하고 있습니다.\n",
      "test_406,#Person1#은 #Person2#에게 화려한 복장을 구하기 위해 쇼핑을 도와달라고 요청합니다. #Person2#는 #Person1#에게 캐나다 경찰을 찾는 것을 제안합니다.\n",
      "test_407,\"#Person2#는 베이징의 야간 생활을 #Person1#에게 소개하고, #Person1#은 춤추는 것을 좋아한다고 말합니다. #Person2#는 #Person1#에게 디스코를 춰보라고 제안하지만, #Person1#은 춤을 못 춰서 다음 춤은 쉬겠다고 합니다.\"\n",
      "test_408,\"#Person1#은 #Person2#에게 농장을 가지고 싶다고 말하고, #Person2#는 #Person1#에게 농부가 되기 전에 많은 훈련이 필요하다고 말한다. #Person1#은 농부가 되기 전에 농장 동물들과 푸른 들과 함께 시골에서 있는 것을 좋아한다.\"\n",
      "test_409,\"맨이 #Person1#에게 하이네켄, 버드와이저, 애피타이저를 주문하는 것을 도와줍니다.\"\n",
      "test_410,#Person2#는 앤과 큰 싸움을 했다. #Person2#는 #Person1#에게 #Person2#가 너무 화를 내서 앤과의 우정을 방해했다고 말한다. #Person1#은 #Person2#에게 앤과의 우정을 방해하지 않도록 요청한다.\n",
      "test_411,\"#Person2#는 #Person1#에게 1층에 가게, 기념품, 그리고 남자 화장실을 알려준다.\"\n",
      "test_412,#Person2#는 패스트푸드 네이션이 미국에서 가장 인기 있는 음식이라고 생각합니다. #Person1#은 미국이 건강한 음식을 제공하는 패스트푸드 레스토랑에 대해 걱정합니다. #Person2#는 미국인들이 급하게 식사를 할 때 급하게 먹는 것을 싫어합니다.\n",
      "test_413,#Person2#가 버스를 타고 워싱턴 스퀘어 파크에 가고 있다. #Person1#은 #Person2#에게 탑승을 권장하고 #Person2#가 워싱턴 스퀘어 파크에 도착할 때까지 탑승할 수 있도록 도와준다.\n",
      "test_414,#Person1#과 #Person2#는 9-11 테러가 발생한 뉴욕시와 베이징의 아파트에서의 경험을 공유하고 있습니다.\n",
      "test_415,칼과 척은 이웃으로 이사 왔습니다. 그들은 친절한 사람들과 함께 살고 싶어합니다.\n",
      "test_416,\"#Person2#는 #Person1#에게 계란, 진한 토스트, 그리고 오렌지 주스를 주문합니다.\"\n",
      "test_417,스티븐은 피곤해서 침대에 들어가고 있다. #Person1#은 스티븐에게 벌써 10시라고 말한다.\n",
      "test_418,\"#Person1#은 제인에게 다음 주 회의 전에 아이디어를 논의하자고 제안하고, 제인은 동의합니다.\"\n",
      "test_419,\"#Person2#는 그의 여자친구에게 진주 귀걸이 세트를 선물하고 싶어합니다. #Person1#는 그녀에게 반지를 선물하는 것을 제안하고, 결국 #Person2#는 결정합니다.\"\n",
      "test_420,#Person2#는 #Person1#에게 새 차를 추천하고 갤런당 40마일의 연비를 소개합니다. #Person1#은 2.5리터 터보차저 엔진이 있는 포드 핀토를 좋아합니다. #Person2#는 #Person1#에게 0% 다운페이먼트와 첫 해 이자 없이 0% 다운페이먼트를 가져갈 수 있다고 말합니다.\n",
      "test_421,\"발람씨는 애플 코퍼레이션에서 월급이 4,000 위안으로 시작되는 컴퓨터 엔지니어 직위를 제안하고, 발람씨가 월 4,000 위안을 받고 싶다고 요청했습니다.\"\n",
      "test_422,\"#Person1#은 #Person2#에게 혜택 등에 대해 설명하고, 그 후에 네트워크 결제 서비스에 가입하는 것을 제안합니다.\"\n",
      "test_423,#Person1#과 #Person2#는 간식을 만들고 싶어한다. 그들은 샌드위치를 만들기로 결정한다.\n",
      "test_424,#Person1#은 의사가 #Person2#에게 피를 뽑아 백혈구 수치를 확인하라고 요청한다.\n",
      "test_425,셀러 씨가 스티븐에게 전기가 나갔다고 말합니다. 셀러 씨는 스티븐에게 퓨즈를 풀어서 선반에서 찾아보라고 합니다.\n",
      "test_426,\"파울라는 식기세척기가 고장이 나서 수리를 했지만, 코너스 씨가 임대료를 지불하지 않았다고 #Person2#에게 말했습니다. #Person2#는 파울라가 문제를 해결하는 데 도와줄 것입니다.\"\n",
      "test_427,\"#Person1#과 #Person2#는 에어컨이 작동하지 않고, 화장실 변기가 막힌 것, 계단이 막힌 것, 그리고 차의 배수로가 막힌 것 등의 문제를 해결하기 위해 수리공을 불러야 합니다.\"\n",
      "test_428,\"#Person1#과 #Person2#는 일자리를 찾고 있고, #Person2#는 #Person1#에게 전기기사 수습생 프로그램에 대해 묻습니다.\"\n",
      "test_429,#Person2#는 #Person1#에게 사장님의 장점을 #Person1#에게 설명합니다.\n",
      "test_430,\"#Person1#은 탕 씨에게 외국어, 투어 가이드 자격증, 그리고 외국어를 할 수 있는 언어에 대해 물어봅니다.\"\n",
      "test_431,#Person2#는 #Person1#에게 금 시계를 사고 싶어합니다. #Person1#은 가격을 내려주지만 #Person2#는 그 가격이 너무 비싸다고 생각합니다.\n",
      "test_432,톰이 사라씨에게 전화를 걸어 켄을 돌봐달라고 요청한다. 사라씨는 동의한다.\n",
      "test_433,#Person1#은 에이미의 첫 직장에 대해 물어본다. 에이미는 #Person1#에게 그녀의 업무 내용과 급여에 대해 이야기한다.\n",
      "test_434,\"앤드류는 뚱뚱해 보여서 크리스마스 때 앤드류가 400달러를 벌었다고 말한다. 앤드류는 400달러를 벌었고, 그 후에 앤드류는 5갤런의 물을 마셨다. 앤드류는 5갤런의 물을 마시지 않았다.\"\n",
      "test_435,그렉 손더스가 메리씨에게 전화를 걸어 그녀의 성적과 스포츠 경험에 대해 물어본다.\n",
      "test_436,\"#Person2#가 #Person1#을 택시로 데려갑니다. #Person1#은 출퇴근 시간이므로, #Person2#는 조심스럽게 운전해야 합니다.\"\n",
      "test_437,\"#Person1#은 브라이언에게 영어를 할 수 있다고 말하고, 브라이언은 캘리포니아와 라스베가스 출장에 대해 이야기한다.\"\n",
      "test_438,\"#Person2#는 새로운 건강보험에 가입하려고 하고, #Person1#은 #Person2#에게 건강검진을 받는 데 도움을 줍니다.\"\n",
      "test_439,\"#Person1#과 #Person2#는 복권에 당첨되지 않았고, 당첨될 가능성도 없어서, 그들은 현실로 돌아와서 한 잔을 마시기로 결정한다.\"\n",
      "test_440,\"#Person1#은 결혼을 준비하고 있지만, #Person2#는 그녀에게 헛소리하지 말고 결심을 내라고 권한다. #Person1#은 준비가 되어 있다고 말한다.\"\n",
      "test_441,\"#Person1#은 명함을 인쇄하고 싶어합니다. #Person2#는 #Person1#에게 양식을 작성하도록 요청하고, #Person1#은 일주일 후에 #Person2#에게 명함을 주문합니다.\"\n",
      "test_442,브라이언은 비행기에서 체크인을 하고 회의를 준비할 예정입니다.\n",
      "test_443,\"#Person1#은 폴에게 추수감사절 저녁을 초대하고, 폴은 혼자 가는 것을 걱정한다. #Person1#은 폴에게 사람들이 많을수록 좋아한다고 말한다. #Person1#은 폴에게 칠면조, 스터핑, 콩, 고구마, 와인을 가져다 줄 수 있다고 말한다.\"\n",
      "test_444,존은 수잔에게 그녀의 사촌을 돌봐달라고 요청한다. 수잔은 곧 그녀를 돌봐줄 것이다. 그들은 금요일에 그녀를 저녁 식사에 초대한다.\n",
      "test_445,#Person1#과 벤은 벤이 흥미로워 보이는 사진을 보고 있다. 그들은 벤이 이탈리아 와인을 $400에 사는 것을 걱정하고 있다. 그들은 벤이 인도 요리를 시작하는 것을 고려하고 있다.\n",
      "test_446,\"#Person1#과 #Person2#는 피자 체험에 대해 이야기하고, 그들은 내일 저녁에 가기로 결정한다.\"\n",
      "test_447,\"진은 #Person1#에게 운전 면허 시험을 준비하고 있다고 말합니다. 그는 로스앤젤레스에서 일할 예정이고, 그 이유는 차를 사려고 계획하기 때문입니다.\"\n",
      "test_448,\"#Person1#은 #Person2#에게 펜을 빌려달라고 요청하고, #Person2#는 #Person1#에게 현금을 받아주고 카드로 결제하라고 요청합니다.\"\n",
      "test_449,\"#Person2#는 회사의 기금 모금 행사가 성공적으로 진행되었다고 #Person1#에게 말합니다. #Person2#는 마라톤 경기에 참가한 선수들이 10달러의 입장료를 지불하고, 그들은 참가자가 1마일을 달릴 때마다 1달러를 약속하는 후원자를 구하기 위해 돌아다녔습니다.\"\n",
      "test_450,#Person2#가 #Person1#에게 피크 트램으로 가는 방법을 알려준다.\n",
      "test_451,\"#Person1#은 #Person2#에게 샌들우드 부채를 보여주고, #Person2#는 작은 것 두 개와 큰 것 하나를 구입합니다.\"\n",
      "test_452,#Person2#는 #Person1#에게 회사가 외부와 내부 요인으로 영향을 받는다고 말합니다.\n",
      "test_453,\"#Person1#과 #Person2#는 작업을 시작하기로 결정하고, 둘 다 참여할 것입니다.\"\n",
      "test_454,#Person2#는 #Person1#에게 짧은 투어를 추천하고 싶어합니다.\n",
      "test_455,\"#Person2#는 #Person1#의 도움으로 바나나 맛의 소고기 버거, 프렌치 프라이, 그리고 밀크 쉐이크를 주문합니다.\"\n",
      "test_456,\"#Person1#과 #Person2#는 등 축제가 무엇인지에 대해 이야기하고 있다. #Person2#는 #Person1#에게 퍼즐이라는 것을 말하고, #Person1#은 그것들이 중국어 글자를 읽을 수 있다고 말한다.\"\n",
      "test_457,#Person1#은 티나에게 ABC 컴퍼니의 최종 라운드 면접에 성공했다고 말한다.\n",
      "test_458,팀과 카렌은 만나서 반가웠습니다.\n",
      "test_459,\"#Person1#과 #Person2#는 부서 회의에 참석하고 있습니다. #Person1#은 #Person2#에게 사본을 만들어달라고 요청하고, #Person2#는 #Person1#에게 부서 회의에서 무엇을 논의하나요?에 대해 묻습니다.\"\n",
      "test_460,#Person2#가 #Person1#에게 목욕기 사용법을 가르쳐준다.\n",
      "test_461,\"#Person2#는 #Person1#에게 베스트셀러 진열대에 있는 책을 보여주고, 소설을 좋아한다고 말합니다.\"\n",
      "test_462,#Person1#은 #Person2#에게 자신의 친구가 영어를 말할 줄 알고 영어를 읽지 못한다고 말합니다. #Person2#는 #Person1#이 #Person2#를 자원봉사로 제안하는 것을 듣고 기뻐합니다.\n",
      "test_463,#Person2#가 #Person1#에게 티켓 예약을 도와준다.\n",
      "test_464,\"#Person1#과 #Person2#는 오늘 저녁 식사를 같이 만들고 싶어합니다. #Person1#은 #Person2#에게 매운 치킨 커리 레시피를 가르쳐줍니다. #Person2#는 채소를 씻고 볶음을 하고, #Person1#은 볶음과 밥을 준비합니다.\"\n",
      "test_465,#Person2#가 #Person1#에게 IC 전화기와 동전 전화기의 사용법을 가르칩니다.\n",
      "test_466,\"#Person1#은 톰에게 중고 컴퓨터에 대해 물어보고, 톰은 중고 컴퓨터가 새 것만큼 좋다고 생각한다.\"\n",
      "test_467,#Person1#은 모건에게 중국 사람들이 식당에서 남은 음식을 집으로 가져가는지 물어봤다. 모건은 중국 사람들이 그렇게 하기 때문에 괜찮다고 생각한다.\n",
      "test_468,\"해리는 중국 동료들이 가방을 너무 비싸게 샀다고 말하고, 그 이유는 흥정을 통해 삼십에 살 수 있다고 말합니다. 해리는 흥정이 자유 거래의 규칙이라고 생각합니다.\"\n",
      "test_469,#Person2#는 영어를 쓰는 것과 말하는 것 모두에 능숙하다고 말합니다. #Person2#는 중학교 때부터 영어를 공부하고 대학에서 영어를 전공했습니다.\n",
      "test_470,\"#Person2#는 새로운 계좌를 개설하고 싶어하고, #Person1#은 #Person2#에게 당좌 계좌와 저축 계좌를 개설하는 데 도움을 주고 있습니다.\"\n",
      "test_471,\"#Person2#는 #Person1#에게 특별 할인 쿠폰을 받는 방법을 알려주고, #Person1#은 설탕 9봉지를 사고 쿠폰 3장을 받게 됩니다.\"\n",
      "test_472,쟈니가 청소를 하고 쓰레기를 밖에 버렸다. 로빈슨 부인은 스티브에게 감사한다.\n",
      "test_473,스미스씨가 기차 표를 잃어버렸습니다. #Person1#은 스미스씨가 표를 사기 위해 넘어갈 것이라고 말합니다.\n",
      "test_474,\"앤은 #Person1#에게 뉴욕에서의 비행 중에 웰빙 프로그램을 했고, 그 효과가 있다고 말한다. 앤은 #Person1#에게 샴페인과 시차 적응을 선택할 수 있다고 말한다.\"\n",
      "test_475,메리는 일자리를 찾았고 톰은 일자리를 찾고 싶어한다. 톰은 #Person2#에게 맥도날드에서 일해고 건설 회사에서 일할 사람들을 필요로 한다고 말한다.\n",
      "test_476,해리는 차에 치였지만 경찰에게 처벌을 받지 않았다. 해리는 신문을 읽으면서 길을 건너지 않을 것이라고 말했다.\n",
      "test_477,\"#Person1#은 '비즈니스 월드'에서 자전거 상점을 인수하고 설립한 스티븐 케인씨를 인터뷰합니다. 케인씨는 사업을 결심하고 사업 대출을 받았습니다. 그는 정규 근무 시간을 지키지 않는다며, 새로운 사업에서의 성공을 기원합니다.\"\n",
      "test_478,#Person1#과 #Person2#는 중년이 아닌 중년 부부에 대해 이야기하고 있습니다.\n",
      "test_479,\"#Person2#는 #Person1#의 제안에 따라 디저트를 두 명에게 나눠 먹고 싶어합니다. #Person2#는 초콜릿 무스 케이크, 신선한 딸기 쇼트케이크, 그리고 럼주 사과 크리스프를 원합니다. #Person1#은 #Person2#에게 디저트 포크 네 개를 가져다 줄 것입니다.\"\n",
      "test_480,\"#Person2#와 #Person1#은 새로운 침대와 책상을 선택하고, #Person2#는 #Person1#에게 상자를 풀어보라고 요청한다.\"\n",
      "test_481,#Person2#는 #Person1#에게 도서관 카드를 발급하고 규정과 벌금에 대해 알려준다.\n",
      "test_482,\"#Person2#는 #Person1#에게 #Person2#의 근무 시간, 휴식 시간, 그리고 서류 작업에 대해 이야기합니다. #Person2#는 칸막이 사무실에서 일하지만, #Person1#는 그렇지 않습니다.\"\n",
      "test_483,\"닉은 옷을 빨래하는 방법을 묻고, 앨리스는 닉에게 기계를 어떻게 사용하는지 가르쳐준다.\"\n",
      "test_484,#Person1#과 #Person2#는 각자의 일정에 대해 이야기하고 있다.\n",
      "test_485,케이티는 #Person1#에게 평가서를 읽고 있다고 말한다. #Person1#은 케이티가 일할 준비가 되어 있다고 생각한다.\n",
      "test_486,\"#Person1#과 #Person2#는 할아버지의 생일 파티에 대해 이야기하고 있다. 그들은 음식, 선물, 그리고 파티 날짜에 대해 논의하고 있다.\"\n",
      "test_487,지나와 지안 루카는 회의에서 서로 자기 소개를 합니다. 지나는 인터넷 서비스 제공업체에서 일하고 지안 루카는 스페인 출신의 프리랜서입니다.\n",
      "test_488,#Person1#과 #Person2#는 새들의 울음소리 때문에 시골이 시끄러워서 걱정하고 있다. #Person2#는 #Person1#에게 이런 일이 17년에 한 번씩 나타나는 새들의 울음소리에 대해 이야기한다.\n",
      "test_489,#Person2#님이 #Person1#에게 칠면조 샌드위치와 소고기 스프와 다이어트 콜라를 주문합니다.\n",
      "test_490,\"제임스는 짐을 싸고 있고, 짐을 싸는 데는 2시간밖에 남지 않았다. 그는 #Person1#에게 짐을 가져가고, 데이비드가 20분 전에 여기 오는 것을 알고 있다.\"\n",
      "test_491,\"테드는 아내의 부모님과 함께 부모님 집에 머무를 예정이지만, #Person1#은 중국에서 몇 주를 보낼 예정이다.\"\n",
      "test_492,\"#Person1#은 오후에 영화관에 가고 싶어하고, #Person2#는 헬렌 아주머니가 항상 #Person1#을 데려가는 것에 놀랍니다.\"\n",
      "test_493,#Person2#가 #Person1#에게 도둑들에 대한 경찰에 전화했다고 말합니다.\n",
      "test_494,\"잭은 찰리에게 비디오 게임을 제안하고, 찰리는 잭에게 자신의 캐릭터를 만드는 게임을 제안한다.\"\n",
      "test_495,#Person2#는 #Person1#에게 컨트리 음악에 관심을 가지게 된 경험을 #Person2#의 아내와 함께 이야기합니다. #Person2#는 CBC에 가서 컨트리 음악의 라디오 프로그램을 시작했습니다.\n",
      "test_496,앨리스는 #Person1#에게 세탁기와 건조기를 사용하는 방법을 가르칩니다. #Person1#은 옷을 깨끗하게 하고 싶어합니다. 앨리스는 #Person1#에게 옷을 빨아야 한다고 말합니다.\n",
      "test_497,\"스티브는 매튜에게 자신의 계약을 갱신하고 싶지 않다고 말한다. 매튜는 여전히 분류된 광고를 찾고 있고, 다우 부인이 그의 이웃이 될 수 있다면 좋겠다고 말한다.\"\n",
      "test_498,프랭크는 벳시에게 친구들과 함께 파티를 열려고 한다고 말한다. 벳시는 그 파티가 기다려지고 있다고 말한다.\n",
      "\n",
      "요약문이 /kkh/summaries2.csv 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# 요약을 저장할 리스트\n",
    "summaries = []\n",
    "\n",
    "# 데이터셋 전체 요약 처리\n",
    "for i, ex in enumerate(DATASET_TEST['test']['dialogue']):\n",
    "    # 각 대화마다 메시지 포맷 설정\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"다음 글을 요약해주세요:\\n\\n{}\".format(ex)\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 프롬프트 생성\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # 요약 파이프라인\n",
    "    pipe_finetuned = pipeline(\"text-generation\", model=finetune_model, tokenizer=tokenizer, max_new_tokens=512)\n",
    "    \n",
    "    # 모델을 통해 요약 생성\n",
    "    outputs = pipe_finetuned(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_k=50,\n",
    "        top_p=0.92,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # 요약 텍스트 저장\n",
    "    summary = outputs[0][\"generated_text\"][len(prompt):].strip()\n",
    "    \n",
    "    # 요약에 콤마가 있으면 쌍따옴표로 감싸기\n",
    "    if ',' in summary:\n",
    "        summary = f'\"{summary}\"'\n",
    "    \n",
    "    # summaries 리스트에 저장\n",
    "    summaries.append([f\"test_{i}\", summary])\n",
    "    \n",
    "    # 요약 출력\n",
    "    print(f\"test_{i},{summary}\")\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_file = \"/kkh/gemma-01-03-bestloss.csv\"\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # 헤더 작성\n",
    "    writer.writerow([\"fname\", \"summary\"])\n",
    "    \n",
    "    # 내용 작성\n",
    "    writer.writerows(summaries)\n",
    "\n",
    "print(f\"\\n요약문이 {csv_file} 파일로 저장되었습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

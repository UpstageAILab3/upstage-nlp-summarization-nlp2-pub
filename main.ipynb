{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from model.model import Model\n",
    "from model.trainer import Trainer\n",
    "from model.test import Test\n",
    "from model.model_analyze import ModelAnalyze\n",
    "from data_pre.dataset import TrainValidDataset, TestDataset\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model & Tokenizer & Dataset ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myvenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open('/root/upstage-nlp-summarization-nlp2/config.yaml', \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Model & Tokenizer\n",
    "model_instance = Model(config, device)\n",
    "model = model_instance.getModel()\n",
    "tokenizer = model_instance.getTokenizer()\n",
    "\n",
    "# dataset\n",
    "train_dataset = TrainValidDataset(config, tokenizer, is_train=True)\n",
    "val_dataset = TrainValidDataset(config, tokenizer, is_train=False)\n",
    "test_dataset = TestDataset(config, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myvenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/myvenv/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4290' max='7800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4290/7800 17:51 < 14:36, 4.00 it/s, Epoch 11/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.614725</td>\n",
       "      <td>0.143275</td>\n",
       "      <td>0.027206</td>\n",
       "      <td>0.139412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.245200</td>\n",
       "      <td>0.608342</td>\n",
       "      <td>0.361342</td>\n",
       "      <td>0.123474</td>\n",
       "      <td>0.346808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.389600</td>\n",
       "      <td>0.580516</td>\n",
       "      <td>0.364083</td>\n",
       "      <td>0.128525</td>\n",
       "      <td>0.350191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.325400</td>\n",
       "      <td>0.565599</td>\n",
       "      <td>0.370407</td>\n",
       "      <td>0.132611</td>\n",
       "      <td>0.354429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.325400</td>\n",
       "      <td>0.560738</td>\n",
       "      <td>0.376468</td>\n",
       "      <td>0.139522</td>\n",
       "      <td>0.360432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.295900</td>\n",
       "      <td>0.558430</td>\n",
       "      <td>0.379644</td>\n",
       "      <td>0.139568</td>\n",
       "      <td>0.363962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.276600</td>\n",
       "      <td>0.560502</td>\n",
       "      <td>0.381270</td>\n",
       "      <td>0.144705</td>\n",
       "      <td>0.365656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.259700</td>\n",
       "      <td>0.562498</td>\n",
       "      <td>0.374397</td>\n",
       "      <td>0.134737</td>\n",
       "      <td>0.359304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.247200</td>\n",
       "      <td>0.565898</td>\n",
       "      <td>0.374683</td>\n",
       "      <td>0.138324</td>\n",
       "      <td>0.360761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.247200</td>\n",
       "      <td>0.569009</td>\n",
       "      <td>0.379212</td>\n",
       "      <td>0.141993</td>\n",
       "      <td>0.363684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.233500</td>\n",
       "      <td>0.570126</td>\n",
       "      <td>0.379283</td>\n",
       "      <td>0.142584</td>\n",
       "      <td>0.362239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4290, training_loss=0.746473709853379, metrics={'train_runtime': 1071.7407, 'train_samples_per_second': 232.463, 'train_steps_per_second': 7.278, 'total_flos': 4.177517883162624e+16, 'train_loss': 0.746473709853379, 'epoch': 11.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(config, model, train_dataset, val_dataset, tokenizer).get_trainer()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:40<00:00,  2.53s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>#Person1#ì€ ë”ìŠ¨ ì”¨ì—ê²Œ ì‚¬ë¬´ì‹¤ í†µì‹ ì´ ì´ë©”ì¼ í†µì‹ ê³¼ ê³µì‹ ë©”ëª¨ë¡œ ì œí•œëœë‹¤ê³ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>#Person2#ëŠ” êµí†µ ì²´ì¦ì— ê±¸ë ¸ë‹¤. #Person1#ëŠ” #Person2#ê°€ ëŒ€...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>ì¼€ì´íŠ¸ëŠ” ë§ˆìƒ¤ì™€ íˆì–´ë¡œê°€ ì´í˜¼í•˜ë ¤ê³  í•œë‹¤ê³  #Person1#ì—ê²Œ ë§í•œë‹¤. #Pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>#Person1#ì€ ë¸Œë¼ì´ì–¸ì˜ ìƒì¼ì„ ì¶•í•˜í•˜ê¸° ìœ„í•´ íŒŒí‹°ì— ì™”ë‹¤. #Person2#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>#Person2#ëŠ” #Person1#ì—ê²Œ ì˜¬ë¦¼í”½ ìŠ¤íƒ€ë””ì›€ì— 5000ê°œì˜ ì¢Œì„ì´ ìˆë‹¤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>test_495</td>\n",
       "      <td>ì°°ë¦¬ëŠ” ì•„ë¹ ë¥¼ ë°ë¦¬ëŸ¬ ê°€ì•¼í•˜ê¸° ë•Œë¬¸ì— #Person1# ê³¼ í•¨ê»˜ ë¹„ë””ì˜¤ ê²Œì„ì„ í•˜ê³ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>test_496</td>\n",
       "      <td>#Person2#ëŠ” ì»¨íŠ¸ë¦¬ ìŒì•…ì— ê´€ì‹¬ì„ ê°€ì§€ê²Œ ëœ ê³„ê¸°ì™€ ë¼ë””ì˜¤ ë°©ì†¡êµ­ì—ì„œ ì¼í•˜ê¸°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>test_497</td>\n",
       "      <td>ì•¨ë¦¬ìŠ¤ëŠ” #Person1#ì—ê²Œ ê¸°ê³„ë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ì•Œë ¤ì£¼ê³ , ê¸°ê³„ê°€ ë„ˆë¬´ ë§ì€...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>test_498</td>\n",
       "      <td>ìŠ¤í‹°ë¸Œê°€ ë§¤íŠœì—ê²Œ ì „í™”ë¥¼ ê±¸ì–´ ê³„ì•½ì´ ë‹¤ìŒ ë‹¬ì— ëë‚˜ê¸° ë•Œë¬¸ì— ì§‘ì„ ì°¾ê³  ìˆë‹¤ê³  ë§...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>test_499</td>\n",
       "      <td>í”„ë­í¬ëŠ” ìŠ¹ì§„í•˜ê³  ì¹œêµ¬ë“¤ ëª¨ë‘ë¥¼ ìœ„í•œ í° íŒŒí‹°ë¥¼ ì—´ë ¤ê³  í•œë‹¤. ë²³ì‹œëŠ” íŒŒí‹°ì— ê°€ê³  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fname                                            summary\n",
       "0      test_0  #Person1#ì€ ë”ìŠ¨ ì”¨ì—ê²Œ ì‚¬ë¬´ì‹¤ í†µì‹ ì´ ì´ë©”ì¼ í†µì‹ ê³¼ ê³µì‹ ë©”ëª¨ë¡œ ì œí•œëœë‹¤ê³ ...\n",
       "1      test_1  #Person2#ëŠ” êµí†µ ì²´ì¦ì— ê±¸ë ¸ë‹¤. #Person1#ëŠ” #Person2#ê°€ ëŒ€...\n",
       "2      test_2  ì¼€ì´íŠ¸ëŠ” ë§ˆìƒ¤ì™€ íˆì–´ë¡œê°€ ì´í˜¼í•˜ë ¤ê³  í•œë‹¤ê³  #Person1#ì—ê²Œ ë§í•œë‹¤. #Pers...\n",
       "3      test_3  #Person1#ì€ ë¸Œë¼ì´ì–¸ì˜ ìƒì¼ì„ ì¶•í•˜í•˜ê¸° ìœ„í•´ íŒŒí‹°ì— ì™”ë‹¤. #Person2#...\n",
       "4      test_4  #Person2#ëŠ” #Person1#ì—ê²Œ ì˜¬ë¦¼í”½ ìŠ¤íƒ€ë””ì›€ì— 5000ê°œì˜ ì¢Œì„ì´ ìˆë‹¤...\n",
       "..        ...                                                ...\n",
       "494  test_495  ì°°ë¦¬ëŠ” ì•„ë¹ ë¥¼ ë°ë¦¬ëŸ¬ ê°€ì•¼í•˜ê¸° ë•Œë¬¸ì— #Person1# ê³¼ í•¨ê»˜ ë¹„ë””ì˜¤ ê²Œì„ì„ í•˜ê³ ...\n",
       "495  test_496  #Person2#ëŠ” ì»¨íŠ¸ë¦¬ ìŒì•…ì— ê´€ì‹¬ì„ ê°€ì§€ê²Œ ëœ ê³„ê¸°ì™€ ë¼ë””ì˜¤ ë°©ì†¡êµ­ì—ì„œ ì¼í•˜ê¸°...\n",
       "496  test_497  ì•¨ë¦¬ìŠ¤ëŠ” #Person1#ì—ê²Œ ê¸°ê³„ë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ì•Œë ¤ì£¼ê³ , ê¸°ê³„ê°€ ë„ˆë¬´ ë§ì€...\n",
       "497  test_498  ìŠ¤í‹°ë¸Œê°€ ë§¤íŠœì—ê²Œ ì „í™”ë¥¼ ê±¸ì–´ ê³„ì•½ì´ ë‹¤ìŒ ë‹¬ì— ëë‚˜ê¸° ë•Œë¬¸ì— ì§‘ì„ ì°¾ê³  ìˆë‹¤ê³  ë§...\n",
       "498  test_499  í”„ë­í¬ëŠ” ìŠ¹ì§„í•˜ê³  ì¹œêµ¬ë“¤ ëª¨ë‘ë¥¼ ìœ„í•œ í° íŒŒí‹°ë¥¼ ì—´ë ¤ê³  í•œë‹¤. ë²³ì‹œëŠ” íŒŒí‹°ì— ê°€ê³  ...\n",
       "\n",
       "[499 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '/root/upstage-nlp-summarization-nlp2/results/checkpoint-2340'\n",
    "test_inst = Test(config, test_dataset, tokenizer, model_path, device)\n",
    "result_df = test_inst()\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model ì„±ëŠ¥ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datasetì˜ 0.2í¼ ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "sample_size = int(len(train_dataset) * 0.2)\n",
    "indices = random.sample(range(len(train_dataset)), sample_size)\n",
    "\n",
    "valid_dataset = Subset(train_dataset, indices)\n",
    "model_path = '/root/upstage-nlp-summarization-nlp2/results/checkpoint-2340'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "  0%|          | 0/78 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_analyze \u001b[38;5;241m=\u001b[39m ModelAnalyze(config, tokenizer, device)\n\u001b[0;32m----> 2\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_analyze\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m result_df\n",
      "File \u001b[0;32m/root/upstage-nlp-summarization-nlp2/model/model_analyze.py:34\u001b[0m, in \u001b[0;36mModelAnalyze.get_result\u001b[0;34m(self, dataset, model_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader):\n\u001b[0;32m---> 34\u001b[0m         item_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid2text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m         generated_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     36\u001b[0m                                        no_repeat_ngram_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_repeat_ngram_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     37\u001b[0m                                        early_stopping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_stopping\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     38\u001b[0m                                        max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerate_max_length\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     39\u001b[0m                                        num_beams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     40\u001b[0m         item_generated_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2text(generated_ids)\n",
      "File \u001b[0;32m/root/upstage-nlp-summarization-nlp2/model/model_analyze.py:62\u001b[0m, in \u001b[0;36mModelAnalyze.id2text\u001b[0;34m(self, id_data)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mid2text\u001b[39m(\u001b[38;5;28mself\u001b[39m, id_data):\n\u001b[0;32m---> 62\u001b[0m     summary_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     remove_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     65\u001b[0m     preprocessed_text \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m summary_text\u001b[38;5;241m.\u001b[39msplit() \n\u001b[1;32m     66\u001b[0m                          \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m remove_tokens]\n",
      "File \u001b[0;32m/opt/conda/envs/myvenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4016\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   4013\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   4014\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 4016\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4020\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4021\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/myvenv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:651\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    650\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 651\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    654\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    657\u001b[0m )\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "model_analyze = ModelAnalyze(config, tokenizer, device)\n",
    "result_df = model_analyze.get_result(valid_dataset, model_path)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

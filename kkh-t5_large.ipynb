{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n-Ps2nkVsBb"
   },
   "source": [
    "# **ğŸ’ğŸ»ğŸ—¨ï¸ğŸ’ğŸ»â€â™‚ï¸ëŒ€í™” ìš”ì•½ Baseline code**\n",
    "> **Dialogue Summarization** ê²½ì§„ëŒ€íšŒì— ì˜¤ì‹  ì—¬ëŸ¬ë¶„ í™˜ì˜í•©ë‹ˆë‹¤! ğŸ‰    \n",
    "> ë³¸ ëŒ€íšŒì—ì„œëŠ” ìµœì†Œ 2ëª…ì—ì„œ ìµœëŒ€ 7ëª…ì´ ë“±ì¥í•˜ì—¬ ë‚˜ëˆ„ëŠ” ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” BART ê¸°ë°˜ ëª¨ë¸ì˜ baseline codeë¥¼ ì œê³µí•©ë‹ˆë‹¤.     \n",
    "> ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì¼ìƒ ëŒ€í™”ì— ëŒ€í•œ ìš”ì•½ì„ íš¨ê³¼ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNq_LylZa1ug"
   },
   "source": [
    "## âš™ï¸ ë°ì´í„° ë° í™˜ê²½ì„¤ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjCiuI_V4glr"
   },
   "source": [
    "### 1) í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYqDF_-r2ToB"
   },
   "source": [
    "- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•œ í›„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zbZ7SU9P2TYN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import platform\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from rouge import Rouge # ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "import wandb # ëª¨ë¸ í•™ìŠµ ê³¼ì •ì„ ì†ì‰½ê²Œ Trackingí•˜ê³ , ì‹œê°í™”í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ê´€ë ¨\n",
    "os_name = platform.system()\n",
    "if os_name == 'Windows':\n",
    "    PRE_PATH = ''\n",
    "elif os_name == 'Linux':\n",
    "    PRE_PATH = '/kkh/'\n",
    "elif os_name == 'Darwin': # ë§¥\n",
    "    PRE_PATH = '/kkh/'\n",
    "DATA_PATH = PRE_PATH + \"data/\" # ëŒ€íšŒì—ì„œ ì œê³µí•œ ë°ì´í„°\n",
    "OUTPUT_PATH = PRE_PATH + \"output/\" # ëª¨ë¸ì˜ ì¶œë ¥ ê°’\n",
    "CHECKPOINT_PATH = PRE_PATH + \"checkpoint/\" # ëª¨ë¸ì˜ ìµœì¢… ì¶œë ¥ ê°’\n",
    "PREDICTION_PATH = PRE_PATH + \"prediction/\" # ìµœì¢… ì˜ˆì¸¡ ê°’\n",
    "LOG_PATH = PRE_PATH + \"log/\"\n",
    "TRAIN_PATH = DATA_PATH + \"train_kkh_new.csv\"\n",
    "VALID_PATH = DATA_PATH + \"dev_kkh_new.csv\"\n",
    "TEST_PATH = DATA_PATH + \"test.csv\"\n",
    "\n",
    "# í—ˆê¹…í˜ì´ìŠ¤ ê´€ë ¨\n",
    "# HUGGING_MODEL = \"digit82/kobart-summarization\"\n",
    "# HUGGING_MODEL = \"EbanLee/kobart-summary-v3\"\n",
    "# HUGGING_MODEL = \"gogamza/kobart-base-v2\"\n",
    "# HUGGING_MODEL = \"gogamza/kobart-summarization\"\n",
    "# HUGGING_MODEL = \"alaggung/bart-r3f\"\n",
    "# HUGGING_MODEL = \"NLPBada/kobart-chat-persona-extraction-v2\"\n",
    "# HUGGING_MODEL = \"lcw99/t5-large-korean-text-summary\"\n",
    "HUGGING_MODEL = \"eenzeenee/t5-base-korean-summarization\"\n",
    "\n",
    "# ì™„ë””ë¹„ ê´€ë ¨\n",
    "WANDB_ENTITY = 'helpot'\n",
    "WANDB_PROJECT_NAME = 'bootcamp-upstage-nlp'\n",
    "WANDB_RUN_NAME = \"t5-base\"\n",
    "WANDB_LOG = \"end\" # end or checkpoint\n",
    "WANDB_WATCH = \"false\" # true or false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists and is empty. Using the existing folder: /kkh/output\n",
      "Folder exists and is empty. Using the existing folder: /kkh/checkpoint\n",
      "Folder exists and is empty. Using the existing folder: /kkh/prediction\n"
     ]
    }
   ],
   "source": [
    "def manage_output_folder(output_path):\n",
    "    if isinstance(output_path, tuple):\n",
    "        output_path = output_path[0]\n",
    "    if not isinstance(output_path, str):\n",
    "        raise ValueError(\"output_path should be a string\")\n",
    "    if output_path.endswith('/') or output_path.endswith('\\\\'):\n",
    "        output_path = output_path[:-1]\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        # í´ë”ê°€ ì¡´ì¬í•  ë•Œ í´ë”ê°€ ë¹„ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
    "        if not os.listdir(output_path):\n",
    "            # í´ë”ê°€ ë¹„ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "            print(f\"Folder exists and is empty. Using the existing folder: {output_path}\")\n",
    "        else:\n",
    "            # í´ë”ê°€ ë¹„ì–´ ìˆì§€ ì•Šìœ¼ë©´ \"_backup\"ì„ ë¶™ì—¬ ì´ë¦„ì„ ë³€ê²½\n",
    "            backup_path = output_path + \"_backup\"\n",
    "            print(f\"Folder already exists and is not empty. Renaming to: {backup_path}\")\n",
    "            shutil.move(output_path, backup_path)\n",
    "            # ìƒˆë¡œìš´ í´ë” ìƒì„±\n",
    "            os.makedirs(output_path)\n",
    "            print(f\"New folder created: {output_path}\")\n",
    "    else:\n",
    "        # í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œ ìƒì„±\n",
    "        os.makedirs(output_path)\n",
    "        print(f\"New folder created: {output_path}\")\n",
    "\n",
    "# í•¨ìˆ˜ í˜¸ì¶œ\n",
    "manage_output_folder(OUTPUT_PATH)\n",
    "manage_output_folder(CHECKPOINT_PATH)\n",
    "manage_output_folder(PREDICTION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6234b40da824419a9553f583001d15a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d95d18116f4cd88907d55646b8bb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.92M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecad63711c7246e39bf1b5809f03faaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HUGGING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5vsACJI7CVb8"
   },
   "outputs": [],
   "source": [
    "loaded_config = {\n",
    "    \"general\": {\n",
    "        \"data_path\": DATA_PATH,\n",
    "        \"output_dir\": OUTPUT_PATH,\n",
    "        \"model_name\": HUGGING_MODEL,\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 1024,\n",
    "        \"decoder_max_len\": 204,\n",
    "        # \"special_tokens\": ['#Address#', '#CarNumber#', '#CardNumber#', '#DateOfBirth#', '#Email#', '#PassportNumber#', '#Person#', '#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#', '#Person6#', '#Person7#', '#PhoneNumber#', '#SSN#']\n",
    "        \"special_tokens\": ['#Address#', '#CarNumber#', '#CardNumber#', '#DateOfBirth#', '#Email#', '#PassportNumber#', '#PhoneNumber#', '#SSN#'],\n",
    "        \"person_tokens\": ['#Person#', '#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#', '#Person6#', '#Person7#']\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 50,  # Seq2SeqTrainingArguments ê¸°ë°˜ ì„¤ì •\n",
    "        \"learning_rate\": 1e-5,  # Seq2SeqTrainingArguments ê¸°ë°˜ ì„¤ì •\n",
    "        \"per_device_train_batch_size\": 4,  # Seq2SeqTrainingArguments ê¸°ë°˜ ì„¤ì •\n",
    "        \"per_device_eval_batch_size\": 4,  # Seq2SeqTrainingArguments ê¸°ë°˜ ì„¤ì •\n",
    "        \"auto_find_batch_size\": False,  # Seq2SeqTrainingArguments ê¸°ë°˜ ì„¤ì •\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,  # Seq2SeqTrainingArguments ê¸°ë°˜ ì„¤ì •\n",
    "        \"lr_scheduler_type\": 'linear',  # Seq2SeqTrainingArguments ê¸°ë°˜ ì„¤ì •\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"evaluation_strategy\": 'epoch',\n",
    "        \"save_strategy\": 'epoch',\n",
    "        \"save_total_limit\": 20,\n",
    "        \"fp16\": True,  # Seq2SeqTrainingArguments ê¸°ë°˜ ì„¤ì •\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"seed\": 42,\n",
    "        \"logging_dir\": LOG_PATH,\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"predict_with_generate\": True,\n",
    "        \"generation_max_length\": 200,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_threshold\": 0.001,\n",
    "        \"report_to\": \"wandb\"\n",
    "    },\n",
    "    \"wandb\": {\n",
    "        \"entity\": WANDB_ENTITY,\n",
    "        \"project\": WANDB_PROJECT_NAME,\n",
    "        \"name\": WANDB_RUN_NAME\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"ckt_path\": CHECKPOINT_PATH,\n",
    "        \"result_path\": PREDICTION_PATH,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": 100,\n",
    "        \"num_beams\": 4,\n",
    "        \"batch_size\": 32,\n",
    "        \"remove_tokens\": [f\"{tokenizer.pad_token}\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "QFHIE2G04y-K",
    "outputId": "19312d21-f5bf-495f-c626-cc17b82024a4"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(TRAIN_PATH))\n",
    "val_df = pd.read_csv(os.path.join(VALID_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IIaIrpH4kWo"
   },
   "source": [
    "## 1. ë°ì´í„° ê°€ê³µ ë° ë°ì´í„°ì…‹ í´ë˜ìŠ¤ êµ¬ì¶•\n",
    "- csv file ì„ ë¶ˆëŸ¬ì™€ì„œ encoder ì™€ decoderì˜ ì…ë ¥í˜•íƒœë¡œ ê°€ê³µí•´ì¤ë‹ˆë‹¤.\n",
    "- ê°€ê³µëœ ë°ì´í„°ë¥¼ torch dataset class ë¡œ êµ¬ì¶•í•˜ì—¬ ëª¨ë¸ì— ì…ë ¥ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oWPawUUflwHa"
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤: ë°ì´í„°ì…‹ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì…ë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "class Preprocess:\n",
    "    def __init__(self, prefix: str) -> None:\n",
    "        self.prefix = prefix  # T5 ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  íƒœìŠ¤í¬ í”„ë¦¬í”½ìŠ¤ (ì˜ˆ: \"summarize: \")\n",
    "\n",
    "    @staticmethod\n",
    "    # ì‹¤í—˜ì— í•„ìš”í•œ ì»¬ëŸ¼ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "    def make_set_as_df(file_path, is_train=True):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if is_train:\n",
    "            train_df = df[['fname', 'dialogue', 'summary']]\n",
    "            return train_df\n",
    "        else:\n",
    "            test_df = df[['fname', 'dialogue']]\n",
    "            return test_df\n",
    "\n",
    "    # T5 ëª¨ë¸ì˜ ì…ë ¥ í˜•íƒœì— ë§ê²Œ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "    def make_input(self, dataset, is_test=False):\n",
    "        if is_test:\n",
    "            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì˜ ê²½ìš°, ì…ë ¥ í…ìŠ¤íŠ¸ì— íƒœìŠ¤í¬ í”„ë¦¬í”½ìŠ¤ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "            encoder_input = dataset['dialogue'].apply(lambda x: self.prefix + str(x))\n",
    "            decoder_input = [\"\"] * len(dataset['dialogue'])  # T5 ëª¨ë¸ì—ì„œëŠ” í…ŒìŠ¤íŠ¸ ì‹œ ë””ì½”ë” ì…ë ¥ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "            return encoder_input.tolist(), decoder_input\n",
    "        else:\n",
    "            # í•™ìŠµ ë°ì´í„°ì…‹ì˜ ê²½ìš°, ì…ë ¥ í…ìŠ¤íŠ¸ì— íƒœìŠ¤í¬ í”„ë¦¬í”½ìŠ¤ë¥¼ ì¶”ê°€í•˜ê³ , ì¶œë ¥ í…ìŠ¤íŠ¸ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "            encoder_input = dataset['dialogue'].apply(lambda x: self.prefix + str(x))\n",
    "            decoder_input = dataset['summary'].apply(lambda x: str(x))  # T5 ëª¨ë¸ì—ì„œëŠ” ë””ì½”ë” ì…ë ¥ì„ ë³„ë„ë¡œ ì¤€ë¹„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "            decoder_output = dataset['summary'].apply(lambda x: str(x))  # ëª©í‘œ ìš”ì•½ ë¬¸ì¥ì„ ë””ì½”ë” ì¶œë ¥ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6GDvodoF8sED"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Trainì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "class DatasetForTrain(Dataset):\n",
    "    def __init__(self, encoder_input, labels, length):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.labels = labels\n",
    "        self.length = length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ì¸ì½”ë” ì…ë ¥: input_idsì™€ attention_mask\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        \n",
    "        # ë””ì½”ë”ì˜ labelsëŠ” ìš”ì•½ë¬¸ìœ¼ë¡œ ì„¤ì •\n",
    "        item['labels'] = self.labels['input_ids'][idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "# Validationì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "class DatasetForVal(Dataset):\n",
    "    def __init__(self, encoder_input, labels, length):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.labels = labels\n",
    "        self.length = length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ì¸ì½”ë” ì…ë ¥: input_idsì™€ attention_mask\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        \n",
    "        # ë””ì½”ë”ì˜ labelsëŠ” ìš”ì•½ë¬¸ìœ¼ë¡œ ì„¤ì •\n",
    "        item['labels'] = self.labels['input_ids'][idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "# Testì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "class DatasetForInference(Dataset):\n",
    "    def __init__(self, encoder_input, test_id, length):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.test_id = test_id\n",
    "        self.length = length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ì¸ì½”ë” ì…ë ¥: input_idsì™€ attention_mask\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        item['ID'] = self.test_id[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hT9z4vvS2CCb"
   },
   "outputs": [],
   "source": [
    "def prepare_train_dataset(config, preprocessor, data_path, tokenizer):\n",
    "    # ì¸ì½”ë” ì…ë ¥ ë° ì¶œë ¥ ë°ì´í„°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "    encoder_input_train, _, decoder_output_train = preprocessor.make_input(train_df)\n",
    "    encoder_input_val, _, decoder_output_val = preprocessor.make_input(val_df)\n",
    "    print('-' * 10, 'Load data complete', '-' * 10)\n",
    "\n",
    "    # ì¸ì½”ë” ì…ë ¥ì„ í† í¬ë‚˜ì´ì €ë¡œ í† í°í™”í•©ë‹ˆë‹¤.\n",
    "    tokenized_encoder_inputs = tokenizer(encoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "                                         add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    # ë””ì½”ë” ì¶œë ¥ì„ í† í¬ë‚˜ì´ì €ë¡œ í† í°í™”í•©ë‹ˆë‹¤ (T5 ëª¨ë¸ì—ì„œ labelsë¡œ ì‚¬ìš©).\n",
    "    tokenized_labels = tokenizer(decoder_output_train, return_tensors=\"pt\", padding=True,\n",
    "                                 add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    # í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    train_inputs_dataset = DatasetForTrain(tokenized_encoder_inputs, tokenized_labels, len(encoder_input_train))\n",
    "\n",
    "    # ê²€ì¦ ë°ì´í„°ì˜ ì¸ì½”ë” ì…ë ¥ì„ í† í¬ë‚˜ì´ì €ë¡œ í† í°í™”í•©ë‹ˆë‹¤.\n",
    "    val_tokenized_encoder_inputs = tokenizer(encoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "                                             add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    # ê²€ì¦ ë°ì´í„°ì˜ ë””ì½”ë” ì¶œë ¥ì„ í† í¬ë‚˜ì´ì €ë¡œ í† í°í™”í•©ë‹ˆë‹¤ (T5 ëª¨ë¸ì—ì„œ labelsë¡œ ì‚¬ìš©).\n",
    "    val_tokenized_labels = tokenizer(decoder_output_val, return_tensors=\"pt\", padding=True,\n",
    "                                     add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    # ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    val_inputs_dataset = DatasetForVal(val_tokenized_encoder_inputs, val_tokenized_labels, len(encoder_input_val))\n",
    "\n",
    "    print('-' * 10, 'Make dataset complete', '-' * 10)\n",
    "    return train_inputs_dataset, val_inputs_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5sKIJ5K5Pz1"
   },
   "source": [
    "## 2. Trainer ë° Trainingargs êµ¬ì¶•í•˜ê¸°\n",
    "- Huggingface ì˜ Trainer ì™€ Training argumentsë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì„ ì¼ê´„ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì£¼ëŠ” í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aQk8ILcEeGNz"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì„±ëŠ¥ì— ëŒ€í•œ í‰ê°€ ì§€í‘œë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ë³¸ ëŒ€íšŒì—ì„œëŠ” ROUGE ì ìˆ˜ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "def compute_metrics(config,tokenizer,pred):\n",
    "    rouge = Rouge()\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n",
    "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•´ ë¯¸ë¦¬ ì •ì˜ëœ ë¶ˆí•„ìš”í•œ ìƒì„±í† í°ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    replaced_predictions = decoded_preds.copy()\n",
    "    replaced_labels = labels.copy()\n",
    "    remove_tokens = config['inference']['remove_tokens']\n",
    "    for token in remove_tokens:\n",
    "        replaced_predictions = [sentence.replace(token,\" \") for sentence in replaced_predictions]\n",
    "        replaced_labels = [sentence.replace(token,\" \") for sentence in replaced_labels]\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[0]}\")\n",
    "    print(f\"GOLD: {replaced_labels[0]}\")\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[1]}\")\n",
    "    print(f\"GOLD: {replaced_labels[1]}\")\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[2]}\")\n",
    "    print(f\"GOLD: {replaced_labels[2]}\")\n",
    "\n",
    "    # ìµœì¢…ì ì¸ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    results = rouge.get_scores(replaced_predictions, replaced_labels,avg=True)\n",
    "\n",
    "    # ROUGE ì ìˆ˜ ì¤‘ F-1 scoreë¥¼ í†µí•´ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "    result = {key: value[\"f\"] for key, value in results.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RInkG8g-HjBi"
   },
   "outputs": [],
   "source": [
    "# í•™ìŠµì„ ìœ„í•œ trainer í´ë˜ìŠ¤ì™€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "def load_trainer_for_train(config,generate_model,tokenizer,train_inputs_dataset,val_inputs_dataset):\n",
    "    print('-'*10, 'Make training arguments', '-'*10,)\n",
    "    # set training args\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=config['general']['output_dir'], # model output directory\n",
    "                overwrite_output_dir=config['training']['overwrite_output_dir'],\n",
    "                num_train_epochs=config['training']['num_train_epochs'],  # total number of training epochs\n",
    "                learning_rate=config['training']['learning_rate'], # learning_rate\n",
    "                per_device_train_batch_size=config['training']['per_device_train_batch_size'], # batch size per device during training\n",
    "                per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],# batch size for evaluation\n",
    "                warmup_ratio=config['training']['warmup_ratio'],  # number of warmup steps for learning rate scheduler\n",
    "                weight_decay=config['training']['weight_decay'],  # strength of weight decay\n",
    "                lr_scheduler_type=config['training']['lr_scheduler_type'],\n",
    "                optim =config['training']['optim'],\n",
    "                gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "                evaluation_strategy=config['training']['evaluation_strategy'], # evaluation strategy to adopt during training\n",
    "                save_strategy =config['training']['save_strategy'],\n",
    "                save_total_limit=config['training']['save_total_limit'], # number of total save model.\n",
    "                fp16=config['training']['fp16'],\n",
    "                load_best_model_at_end=config['training']['load_best_model_at_end'], # ìµœì¢…ì ìœ¼ë¡œ ê°€ì¥ ë†’ì€ ì ìˆ˜ ì €ì¥\n",
    "                seed=config['training']['seed'],\n",
    "                logging_dir=config['training']['logging_dir'], # directory for storing logs\n",
    "                logging_strategy=config['training']['logging_strategy'],\n",
    "                predict_with_generate=config['training']['predict_with_generate'], #To use BLEU or ROUGE score\n",
    "                generation_max_length=config['training']['generation_max_length'],\n",
    "                do_train=config['training']['do_train'],\n",
    "                do_eval=config['training']['do_eval'],\n",
    "                report_to=config['training']['report_to'] # (ì„ íƒ) wandbë¥¼ ì‚¬ìš©í•  ë•Œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "            )\n",
    "\n",
    "    wandb.init(\n",
    "        entity=config['wandb']['entity'],\n",
    "        project=config['wandb']['project'],\n",
    "        name=config['wandb']['name'],\n",
    "    )\n",
    "    os.environ[\"WANDB_LOG_MODEL\"] = WANDB_LOG\n",
    "    os.environ[\"WANDB_WATCH\"] = WANDB_WATCH\n",
    "\n",
    "    # Validation lossê°€ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ í•™ìŠµì„ ì¤‘ë‹¨ì‹œí‚¤ëŠ” EarlyStopping ê¸°ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    MyCallback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=config['training']['early_stopping_patience'],\n",
    "        early_stopping_threshold=config['training']['early_stopping_threshold']\n",
    "    )\n",
    "    print('-'*10, 'Make training arguments complete', '-'*10,)\n",
    "    print('-'*10, 'Make trainer', '-'*10,)\n",
    "\n",
    "    # Trainer í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=generate_model, # ì‚¬ìš©ìê°€ ì‚¬ì „ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ëª¨ë¸ì„ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "        args=training_args,\n",
    "        train_dataset=train_inputs_dataset,\n",
    "        eval_dataset=val_inputs_dataset,\n",
    "        compute_metrics = lambda pred: compute_metrics(config,tokenizer, pred),\n",
    "        callbacks = [MyCallback]\n",
    "    )\n",
    "    print('-'*10, 'Make trainer complete', '-'*10,)\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KKWHe8dE5fSx"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, T5ForConditionalGeneration\n",
    "\n",
    "# í•™ìŠµì„ ìœ„í•œ tokenizerì™€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "def load_tokenizer_and_model_for_train(config, device):\n",
    "    print('-'*10, 'Load tokenizer & model', '-'*10,)\n",
    "    print('-'*10, f'Model Name : {config[\"general\"][\"model_name\"]}', '-'*10,)\n",
    "\n",
    "    # ëª¨ë¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°\n",
    "    model_name = config['general']['model_name']\n",
    "\n",
    "    # T5 ëª¨ë¸ ì„¤ì • ë¡œë“œ\n",
    "    t5_config = AutoConfig.from_pretrained(model_name)\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # T5 ëª¨ë¸ ë¡œë“œ\n",
    "    generate_model = T5ForConditionalGeneration.from_pretrained(model_name, config=t5_config)\n",
    "\n",
    "    # ìŠ¤í˜ì…œ í† í° ì¶”ê°€\n",
    "    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    tokenizer.add_tokens(config['tokenizer']['person_tokens'])\n",
    "\n",
    "    # ì¶”ê°€ëœ í† í° ìˆ˜ì— ë§ì¶° ëª¨ë¸ì˜ í† í° ì„ë² ë”© í¬ê¸° ì¬ì¡°ì •\n",
    "    generate_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # ëª¨ë¸ì„ ì§€ì •ëœ ì¥ì¹˜ë¡œ ì´ë™\n",
    "    generate_model.to(device)\n",
    "    print(generate_model.config)\n",
    "\n",
    "    print('-'*10, 'Load tokenizer & model complete', '-'*10,)\n",
    "    return generate_model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvutzKQYvQgl"
   },
   "source": [
    "## 3. ëª¨ë¸ í•™ìŠµí•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImZUb-BC42J-"
   },
   "source": [
    "- ì•ì—ì„œ êµ¬ì¶•í•œ í´ë˜ìŠ¤ ë° í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qnA96wmR44is"
   },
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    # ì‚¬ìš©í•  deviceë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print('-' * 10, f'device : {device}', '-' * 10)\n",
    "    print(torch.__version__)\n",
    "\n",
    "    # ì‚¬ìš©í•  ëª¨ë¸ê³¼ tokenizerë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "    generate_model, tokenizer = load_tokenizer_and_model_for_train(config, device)\n",
    "    print('-' * 10, \"tokenizer special tokens : \", tokenizer.special_tokens_map, '-' * 10)\n",
    "\n",
    "    # í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "    # T5 ëª¨ë¸ì€ bos_tokenê³¼ eos_tokenì„ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, prefixë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    preprocessor = Preprocess(prefix=config['tokenizer'].get('prefix', 'summarize: ')) \n",
    "    data_path = config['general']['data_path']\n",
    "    train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config, preprocessor, data_path, tokenizer)\n",
    "\n",
    "    # Trainer í´ë˜ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "    trainer = load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset)\n",
    "    trainer.train()   # ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "    # (ì„ íƒ) ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œëœ í›„ wandbë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1DMS60wL-Dhv",
    "outputId": "cbb6aba7-18ff-4d12-b9e7-2a2ef31d94d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- device : cuda:0 ----------\n",
      "2.3.1+cu121\n",
      "---------- Load tokenizer & model ----------\n",
      "---------- Model Name : eenzeenee/t5-base-korean-summarization ----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d862ffcefe45acb1a1a8f24942972e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/782 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6f61879ded41c38ea007ce1a6c7fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"_name_or_path\": \"eenzeenee/t5-base-korean-summarization\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_length\": 128,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50374\n",
      "}\n",
      "\n",
      "---------- Load tokenizer & model complete ----------\n",
      "---------- tokenizer special tokens :  {'eos_token': '</s>', 'unk_token': '<pad>', 'pad_token': '<pad>', 'additional_special_tokens': ['#Address#', '#CarNumber#', '#CardNumber#', '#DateOfBirth#', '#Email#', '#PassportNumber#', '#PhoneNumber#', '#SSN#']} ----------\n",
      "---------- Load data complete ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Make dataset complete ----------\n",
      "---------- Make training arguments ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhelpotcreator\u001b[0m (\u001b[33mhelpot\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20240908_000013-0qhfbhpt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/helpot/bootcamp-upstage-nlp/runs/0qhfbhpt' target=\"_blank\">t5-base</a></strong> to <a href='https://wandb.ai/helpot/bootcamp-upstage-nlp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/helpot/bootcamp-upstage-nlp' target=\"_blank\">https://wandb.ai/helpot/bootcamp-upstage-nlp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/helpot/bootcamp-upstage-nlp/runs/0qhfbhpt' target=\"_blank\">https://wandb.ai/helpot/bootcamp-upstage-nlp/runs/0qhfbhpt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Make training arguments complete ----------\n",
      "---------- Make trainer ----------\n",
      "---------- Make trainer complete ----------\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 430.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Trainer í´ë˜ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# (ì„ íƒ) ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œëœ í›„ wandbë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3324\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3363\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3362\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3363\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/utils/operations.py:825\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/utils/operations.py:813\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_fp32\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/utils/operations.py:792\u001b[0m, in \u001b[0;36mconvert_to_fp32\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_fp16_bf16_tensor\u001b[39m(tensor):\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (is_torch_tensor(tensor) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m    788\u001b[0m         torch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m    789\u001b[0m         torch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m    790\u001b[0m     )\n\u001b[0;32m--> 792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecursively_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_convert_to_fp32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_is_fp16_bf16_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/utils/operations.py:118\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[0;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m honor_type(\n\u001b[1;32m    108\u001b[0m         data,\n\u001b[1;32m    109\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m         ),\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[0;32m--> 118\u001b[0m         {\n\u001b[1;32m    119\u001b[0m             k: recursively_apply(\n\u001b[1;32m    120\u001b[0m                 func, v, \u001b[38;5;241m*\u001b[39margs, test_type\u001b[38;5;241m=\u001b[39mtest_type, error_on_other_type\u001b[38;5;241m=\u001b[39merror_on_other_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    121\u001b[0m             )\n\u001b[1;32m    122\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    123\u001b[0m         }\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m test_type(data):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(data, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/utils/operations.py:119\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m honor_type(\n\u001b[1;32m    108\u001b[0m         data,\n\u001b[1;32m    109\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m         ),\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[1;32m    118\u001b[0m         {\n\u001b[0;32m--> 119\u001b[0m             k: \u001b[43mrecursively_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_other_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_on_other_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    123\u001b[0m         }\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m test_type(data):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(data, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/utils/operations.py:126\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[0;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[1;32m    118\u001b[0m         {\n\u001b[1;32m    119\u001b[0m             k: recursively_apply(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         }\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m test_type(data):\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_on_other_type:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported types (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) passed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. Only nested list/tuple/dicts of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects that are valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` should be passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/accelerate/utils/operations.py:784\u001b[0m, in \u001b[0;36mconvert_to_fp32.<locals>._convert_to_fp32\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_to_fp32\u001b[39m(tensor):\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 430.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(loaded_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFtWqowCGzEc"
   },
   "source": [
    "## 4. ëª¨ë¸ ì¶”ë¡ í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ê³³ì— ë‚´ê°€ ì‚¬ìš©í•  wandb config ì„¤ì •\n",
    "loaded_config['inference']['ckt_path'] = \"/root/output/checkpoint-4674/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFGul3-rSscf"
   },
   "source": [
    "- test dataë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "lV1Do7nlTylG"
   },
   "outputs": [],
   "source": [
    "# tokenization ê³¼ì •ê¹Œì§€ ì§„í–‰ëœ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì— ì…ë ¥ë  ë°ì´í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "def prepare_test_dataset(config, preprocessor, tokenizer):\n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "    test_file_path = os.path.join(config['general']['data_path'], 'test.csv')\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "    test_data = preprocessor.make_set_as_df(test_file_path, is_train=False)\n",
    "    test_id = test_data['fname']\n",
    "\n",
    "    print('-' * 150)\n",
    "    print(f'test_data:\\n{test_data[\"dialogue\"][0]}')\n",
    "    print('-' * 150)\n",
    "\n",
    "    # ì¸ì½”ë” ì…ë ¥ ì¤€ë¹„ (T5 ëª¨ë¸ì—ì„œëŠ” ë””ì½”ë” ì…ë ¥ì„ ë”°ë¡œ ì¤€ë¹„í•˜ì§€ ì•ŠìŒ)\n",
    "    encoder_input_test, _ = preprocessor.make_input(test_data, is_test=True)\n",
    "    print('-' * 10, 'Load data complete', '-' * 10,)\n",
    "\n",
    "    # ì¸ì½”ë” ì…ë ¥ì„ í† í¬ë‚˜ì´ì €ë¡œ í† í°í™”í•©ë‹ˆë‹¤.\n",
    "    test_tokenized_encoder_inputs = tokenizer(\n",
    "        encoder_input_test,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=config['tokenizer']['encoder_max_len'],\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    test_encoder_inputs_dataset = DatasetForInference(test_tokenized_encoder_inputs, test_id, len(encoder_input_test))\n",
    "    print('-' * 10, 'Make dataset complete', '-' * 10,)\n",
    "\n",
    "    return test_data, test_encoder_inputs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "eb49bLULT3aS"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# ì¶”ë¡ ì„ ìœ„í•œ tokenizerì™€ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "def load_tokenizer_and_model_for_test(config, device):\n",
    "    print('-'*10, 'Load tokenizer & model', '-'*10,)\n",
    "\n",
    "    # ëª¨ë¸ ì´ë¦„ê³¼ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì„¤ì •\n",
    "    model_name = config['general']['model_name']\n",
    "    ckt_path = config['inference']['ckt_path']\n",
    "    print('-'*10, f'Model Name : {model_name}', '-'*10,)\n",
    "\n",
    "    # í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # ìŠ¤í˜ì…œ í† í° ì¶”ê°€\n",
    "    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    tokenizer.add_tokens(config['tokenizer']['person_tokens'])\n",
    "\n",
    "    # T5 ëª¨ë¸ ë¡œë“œ\n",
    "    generate_model = T5ForConditionalGeneration.from_pretrained(ckt_path)\n",
    "    generate_model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # ëª¨ë¸ì„ ì§€ì •í•œ ì¥ì¹˜ë¡œ ì´ë™\n",
    "    generate_model.to(device)\n",
    "    print('-'*10, 'Load tokenizer & model complete', '-'*10,)\n",
    "\n",
    "    return generate_model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Axzu9rsoGLgJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# í•™ìŠµëœ ëª¨ë¸ì´ ìƒì„±í•œ ìš”ì•½ë¬¸ì˜ ì¶œë ¥ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "def inference(config):\n",
    "    # ì‚¬ìš©í•  ì¥ì¹˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print('-'*10, f'device : {device}', '-'*10,)\n",
    "    print(torch.__version__)\n",
    "\n",
    "    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    generate_model, tokenizer = load_tokenizer_and_model_for_test(config, device)\n",
    "\n",
    "    # ë°ì´í„° ê²½ë¡œì™€ ì „ì²˜ë¦¬ ê°ì²´ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "    data_path = config['general']['data_path']\n",
    "    preprocessor = Preprocess(prefix=config['tokenizer'].get('prefix', 'summarize: '))\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "    test_data, test_encoder_inputs_dataset = prepare_test_dataset(config, preprocessor, tokenizer)\n",
    "    dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=config['inference']['batch_size'])\n",
    "\n",
    "    summary = []\n",
    "    text_ids = []\n",
    "\n",
    "    # ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(dataloader):\n",
    "            text_ids.extend(item['ID'])\n",
    "            input_ids = item['input_ids'].to(device)\n",
    "            \n",
    "            generated_ids = generate_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],\n",
    "                early_stopping=config['inference']['early_stopping'],\n",
    "                max_length=config['inference']['generate_max_length'],\n",
    "                num_beams=config['inference']['num_beams'],\n",
    "            )\n",
    "            \n",
    "            # ìš”ì•½ë¬¸ ë””ì½”ë”© ë° ì €ì¥\n",
    "            for ids in generated_ids:\n",
    "                # ìŠ¤í˜ì…œ í† í°ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ skip_special_tokens=Falseë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "                result = tokenizer.decode(ids, skip_special_tokens=False, clean_up_tokenization_spaces=True)\n",
    "\n",
    "                # ë¶ˆí•„ìš”í•œ ìŠ¤í˜ì…œ í† í° ì œê±°\n",
    "                result = result.replace('<pad>', '').replace('</s>', '').strip()\n",
    "                summary.append(result)\n",
    "\n",
    "    # ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ê³  íŒŒì¼ë¡œ ì €ì¥\n",
    "    output = pd.DataFrame({\"fname\": test_data['fname'], \"summary\": summary})\n",
    "    result_path = config['inference']['result_path']\n",
    "    \n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    \n",
    "    output.to_csv(os.path.join(result_path, \"output_t5_remove_pad_more_specialtoken_epoch100.csv\"), index=False)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "-pJ1ZXf-5V50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- device : cuda:0 ----------\n",
      "2.4.0+cu121\n",
      "---------- Load tokenizer & model ----------\n",
      "---------- Model Name : eenzeenee/t5-base-korean-summarization ----------\n",
      "---------- Load tokenizer & model complete ----------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "test_data:\n",
      "#Person1#: ë”ìŠ¨ ì”¨, ë°›ì•„ì“°ê¸° ì¢€ í•´ì£¼ì„¸ìš”. \n",
      "#Person2#: ë„¤, ì‹¤ì¥ë‹˜...\n",
      "#Person1#: ì´ê²ƒì€ ì˜¤ëŠ˜ ì˜¤í›„ê¹Œì§€ ëª¨ë“  ì§ì›ì—ê²Œ ë‚´ë¶€ ë©”ëª¨ë¡œ ì „ë‹¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì¤€ë¹„ë˜ì…¨ë‚˜ìš”?\n",
      "#Person2#: ë„¤, ì‹¤ì¥ë‹˜. ì‹œì‘í•˜ì…”ë„ ë©ë‹ˆë‹¤.\n",
      "#Person1#: ëª¨ë“  ì§ì›ë“¤ì—ê²Œ ì£¼ì˜í•˜ë¼... ì¦‰ì‹œ íš¨ë ¥ì„ ë°œíœ˜í•˜ì—¬, ëª¨ë“  ì‚¬ë¬´ì‹¤ í†µì‹ ì€ ì´ë©”ì¼ í†µì‹ ê³¼ ê³µì‹ ë©”ëª¨ë¡œ ì œí•œë©ë‹ˆë‹¤. ê·¼ë¬´ ì‹œê°„ ë™ì•ˆ ì§ì›ë“¤ì´ ì¦‰ì‹œ ë©”ì‹œì§€ í”„ë¡œê·¸ë¨ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ì—„ê²©íˆ ê¸ˆì§€ë©ë‹ˆë‹¤.\n",
      "#Person2#: ì‹¤ì¥ë‹˜, ì´ê²ƒì€ ë‚´ë¶€ í†µì‹ ì—ë§Œ ì ìš©ë˜ëŠ” ê±´ê°€ìš”? ì•„ë‹ˆë©´ ì™¸ë¶€ í†µì‹ ì—ë„ ì œí•œì´ ë˜ëŠ” ê±´ê°€ìš”?\n",
      "#Person1#: ì´ê²ƒì€ ëª¨ë“  í†µì‹ ì— ì ìš©ë˜ì–´ì•¼ í•©ë‹ˆë‹¤, ì´ ì‚¬ë¬´ì‹¤ ë‚´ì˜ ì§ì›ë“¤ ì‚¬ì´ë¿ë§Œ ì•„ë‹ˆë¼ ì™¸ë¶€ í†µì‹ ì—ë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤.\n",
      "#Person2#: í•˜ì§€ë§Œ ì‹¤ì¥ë‹˜, ë§ì€ ì§ì›ë“¤ì´ ê³ ê°ê³¼ ì†Œí†µí•˜ê¸° ìœ„í•´ ì¦‰ì‹œ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "#Person1#: ê·¸ë“¤ì€ ê·¸ë“¤ì˜ ì˜ì‚¬ì†Œí†µ ë°©ë²•ì„ ë°”ê¾¸ì–´ì•¼ë§Œ í•©ë‹ˆë‹¤. ì´ ì‚¬ë¬´ì‹¤ì—ì„œ ëˆ„êµ¬ë„ ì¦‰ì‹œ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê¸°ë¥¼ ì›í•©ë‹ˆë‹¤. ë„ˆë¬´ ë§ì€ ì‹œê°„ì„ ë‚­ë¹„í•˜ê²Œ ë©ë‹ˆë‹¤! ì´ì œ, ë©”ëª¨ë¥¼ ê³„ì†í•´ì£¼ì„¸ìš”. ìš°ë¦¬ê°€ ì–´ë””ê¹Œì§€ í–ˆë‚˜ìš”?\n",
      "#Person2#: ì´ê²ƒì€ ë‚´ë¶€ì™€ ì™¸ë¶€ í†µì‹ ì— ì ìš©ë©ë‹ˆë‹¤.\n",
      "#Person1#: ê·¸ë ‡ìŠµë‹ˆë‹¤. ì¦‰ì‹œ ë©”ì‹œì§€ë¥¼ ê³„ì† ì‚¬ìš©í•˜ëŠ” ì–´ë–¤ ì§ì›ì´ë¼ë„ ë¨¼ì € ê²½ê³ ë¥¼ ë°›ê³  ì§ë¬´ ì •ì§€ì— ì²˜í•´ì§ˆ ê²ƒì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ìœ„ë°˜ ì‹œì—ëŠ” ì§ì›ì€ í•´ê³ ì— ì²˜í•´ì§ˆ ê²ƒì…ë‹ˆë‹¤. ì´ ìƒˆë¡œìš´ ì •ì±…ì— ëŒ€í•œ ì–´ë–¤ ì§ˆë¬¸ì´ë¼ë„ ë¶€ì„œì¥ì—ê²Œ ì§ì ‘ ë¬¸ì˜í•˜ë©´ ë©ë‹ˆë‹¤.\n",
      "#Person2#: ê·¸ê²Œ ë‹¤ì‹ ê°€ìš”?\n",
      "#Person1#: ë„¤. ì´ ë©”ëª¨ë¥¼ ì˜¤í›„ 4ì‹œ ì „ì— ëª¨ë“  ì§ì›ì—ê²Œ íƒ€ì´í•‘í•˜ì—¬ ë°°í¬í•´ ì£¼ì„¸ìš”.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------- Load data complete ----------\n",
      "---------- Make dataset complete ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:03<00:00,  4.00s/it]\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµëœ ëª¨ë¸ì˜ testë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "if __name__ == \"__main__\":\n",
    "    output = inference(loaded_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "OsPmLfhbzZqS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>ì€ ì—ê²Œ ëª¨ë“  ì§ì›ì—ê²Œì˜ ë‚´ë¶€ ë©”ëª¨ë¥¼ ë°›ì•„ì“°ê¸° í•˜ë„ë¡ ìš”ì²­í•˜ê³ , ì¦‰ì‹œ íš¨ë ¥ì„ ë°œíœ˜í•˜...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>ëŠ” êµí†µ ì²´ì¦ì— ê±¸ë ¸ë‹¤. ì€ ì—ê²Œ ëŒ€ì¤‘êµí†µì„ ì´ìš©í•˜ë¼ê³  ì œì•ˆí•˜ê³ , ë‚ ì”¨ê°€ ì¢‹ì„ ë•Œ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>ì¼€ì´íŠ¸ëŠ” ë§ˆìƒ¤ì™€ íˆì–´ë¡œê°€ 2ê°œì›” ë™ì•ˆ ë³„ê±° ì¤‘ì´ë‹¤ê°€ ì´í˜¼ì„ ì‹ ì²­í–ˆë‹¤ê³  ì—ê²Œ ë§í•œë‹¤....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>ë¸Œë¼ì´ì–¸ì€ ì˜ ìƒì¼ì„ ì¶•í•˜í•˜ê¸° ìœ„í•´ íŒŒí‹°ë¥¼ ì¦ê¸°ê³  ìˆë‹¤. ëŠ” ë¸Œë¼ì´ì–¸ì—ê²Œ í•¨ê»˜ ìˆ  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>ëŠ” ì—ê²Œ ì˜¬ë¦¼í”½ ìŠ¤íƒ€ë””ì›€ì´ 6ì›”ì— ì™„ê³µë  ì˜ˆì •ì´ë©°, ì´ 5000ê°œì˜ ì¢Œì„ì„ ê°€ì§€ê³  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>test_495</td>\n",
       "      <td>ì°°ë¦¬ëŠ” ì­ì—ê²Œ í•™êµ ëë‚˜ê³  ì§‘ì— ì™€ì„œ ë¹„ë””ì˜¤ ê²Œì„ì„ í•˜ìê³  ì œì•ˆí•˜ê³ , ê·¸ëŠ” ê·¸ê²ƒì´ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>test_496</td>\n",
       "      <td>ì€ ì—ê²Œ ì»¨íŠ¸ë¦¬ ìŒì•…ì— ê´€ì‹¬ì„ ê°€ì§€ê²Œ ëœ ê³„ê¸°ì™€ ë¼ë””ì˜¤ ë°©ì†¡êµ­ì—ì„œ ì¼í•˜ê¸° ì‹œì‘í•œ ì´...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>test_497</td>\n",
       "      <td>ì•¨ë¦¬ìŠ¤ëŠ” ê¸°ê³„ë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ì—ê²Œ ì•Œë ¤ì¤ë‹ˆë‹¤. ì€ ì•¨ë¦¬ìŠ¤ê°€ ì˜·ì„ ê¹¨ë—í•˜ê²Œ í•˜ê³ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>test_498</td>\n",
       "      <td>ìŠ¤í‹°ë¸ŒëŠ” ë§¤íŠœì—ê²Œ ìµœê·¼ì— ì‚´ ê³³ì„ ì°¾ê³  ìˆìœ¼ë©°, ê·¸ì˜ ì´ì›ƒì¸ ë‹¤ìš° ë¶€ì¸ì´ ê·¸ë…€ì™€ í•¨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>test_499</td>\n",
       "      <td>í”„ë­í¬ê°€ ë²³ì‹œì—ê²Œ ìŠ¹ì§„ì„ ì¶•í•˜í•˜ê¸° ìœ„í•´ í° íŒŒí‹°ë¥¼ ì—´ë ¤ê³  í•œë‹¤ê³  ë§í•œë‹¤. í”„ë­í¬ëŠ” ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fname                                            summary\n",
       "0      test_0  ì€ ì—ê²Œ ëª¨ë“  ì§ì›ì—ê²Œì˜ ë‚´ë¶€ ë©”ëª¨ë¥¼ ë°›ì•„ì“°ê¸° í•˜ë„ë¡ ìš”ì²­í•˜ê³ , ì¦‰ì‹œ íš¨ë ¥ì„ ë°œíœ˜í•˜...\n",
       "1      test_1  ëŠ” êµí†µ ì²´ì¦ì— ê±¸ë ¸ë‹¤. ì€ ì—ê²Œ ëŒ€ì¤‘êµí†µì„ ì´ìš©í•˜ë¼ê³  ì œì•ˆí•˜ê³ , ë‚ ì”¨ê°€ ì¢‹ì„ ë•Œ ...\n",
       "2      test_2  ì¼€ì´íŠ¸ëŠ” ë§ˆìƒ¤ì™€ íˆì–´ë¡œê°€ 2ê°œì›” ë™ì•ˆ ë³„ê±° ì¤‘ì´ë‹¤ê°€ ì´í˜¼ì„ ì‹ ì²­í–ˆë‹¤ê³  ì—ê²Œ ë§í•œë‹¤....\n",
       "3      test_3  ë¸Œë¼ì´ì–¸ì€ ì˜ ìƒì¼ì„ ì¶•í•˜í•˜ê¸° ìœ„í•´ íŒŒí‹°ë¥¼ ì¦ê¸°ê³  ìˆë‹¤. ëŠ” ë¸Œë¼ì´ì–¸ì—ê²Œ í•¨ê»˜ ìˆ  ...\n",
       "4      test_4  ëŠ” ì—ê²Œ ì˜¬ë¦¼í”½ ìŠ¤íƒ€ë””ì›€ì´ 6ì›”ì— ì™„ê³µë  ì˜ˆì •ì´ë©°, ì´ 5000ê°œì˜ ì¢Œì„ì„ ê°€ì§€ê³  ...\n",
       "..        ...                                                ...\n",
       "494  test_495  ì°°ë¦¬ëŠ” ì­ì—ê²Œ í•™êµ ëë‚˜ê³  ì§‘ì— ì™€ì„œ ë¹„ë””ì˜¤ ê²Œì„ì„ í•˜ìê³  ì œì•ˆí•˜ê³ , ê·¸ëŠ” ê·¸ê²ƒì´ ...\n",
       "495  test_496  ì€ ì—ê²Œ ì»¨íŠ¸ë¦¬ ìŒì•…ì— ê´€ì‹¬ì„ ê°€ì§€ê²Œ ëœ ê³„ê¸°ì™€ ë¼ë””ì˜¤ ë°©ì†¡êµ­ì—ì„œ ì¼í•˜ê¸° ì‹œì‘í•œ ì´...\n",
       "496  test_497  ì•¨ë¦¬ìŠ¤ëŠ” ê¸°ê³„ë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ì—ê²Œ ì•Œë ¤ì¤ë‹ˆë‹¤. ì€ ì•¨ë¦¬ìŠ¤ê°€ ì˜·ì„ ê¹¨ë—í•˜ê²Œ í•˜ê³ ...\n",
       "497  test_498  ìŠ¤í‹°ë¸ŒëŠ” ë§¤íŠœì—ê²Œ ìµœê·¼ì— ì‚´ ê³³ì„ ì°¾ê³  ìˆìœ¼ë©°, ê·¸ì˜ ì´ì›ƒì¸ ë‹¤ìš° ë¶€ì¸ì´ ê·¸ë…€ì™€ í•¨...\n",
       "498  test_499  í”„ë­í¬ê°€ ë²³ì‹œì—ê²Œ ìŠ¹ì§„ì„ ì¶•í•˜í•˜ê¸° ìœ„í•´ í° íŒŒí‹°ë¥¼ ì—´ë ¤ê³  í•œë‹¤ê³  ë§í•œë‹¤. í”„ë­í¬ëŠ” ...\n",
       "\n",
       "[499 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output  # ê° ëŒ€í™”ë¬¸ì— ëŒ€í•œ ìš”ì•½ë¬¸ì´ ì¶œë ¥ë¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "083ea69907bb48d4a8fff919bac51aad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08d05bc20a96432badd459e1ffaf868e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "13651c09564a4337b8274c1cb436faa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14f6c91d6c634379b498586c51e606e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21d2e54b5a0a4f79973a512105da43eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2307c6dcbe0141acb5e61baae19cade7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "285007b45236478ca147c6df752c8da4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a190bda0b72407e9a953cd2104dd3b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2fd3d7bbcd6948d8904d33001f95ea03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3645438ace1f4596a8dbc157b48c1521": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14f6c91d6c634379b498586c51e606e0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_08d05bc20a96432badd459e1ffaf868e",
      "value": " 295/295 [00:00&lt;00:00, 21.3kB/s]"
     }
    },
    "3a04e871b74b45d7bf02fd33bb103577": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3bcd6b6b956347b29e1efa20a1d00542": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c284a826f6843f6aa47eacad478ac30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_affff1d8a89e4c14955d1b2aa39ff1ab",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_13651c09564a4337b8274c1cb436faa5",
      "value": "tokenizer.json: 100%"
     }
    },
    "45187decb58b4ad39ad532259c6277e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4747b668e2fa4ab58a449446f80030f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "52095cc7087243ac916055e569fd22f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c18f0e3bc35e44d9915c3f84cd282a26",
      "max": 109,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3a04e871b74b45d7bf02fd33bb103577",
      "value": 109
     }
    },
    "58001a60eacc44d5b38a68648adccde4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58c794fb7ce543a39fdf66d757f6eeab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f5fde5b0ac840a18bd5cc380e564ff6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_45187decb58b4ad39ad532259c6277e5",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "5dfcf310ca9e4e2794076098a5d69cea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3c284a826f6843f6aa47eacad478ac30",
       "IPY_MODEL_6caedd60c6b747469c82930be1f95d6d",
       "IPY_MODEL_64f2218f899d446393cfea44f206f0a6"
      ],
      "layout": "IPY_MODEL_d068f541df3f438dbd5138863e64b2f2"
     }
    },
    "64f2218f899d446393cfea44f206f0a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d22fbc2c5dbf422399e496c9b500025a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_775d8bbeceac4e2da4f21ab6235c89ed",
      "value": " 682k/682k [00:00&lt;00:00, 5.40MB/s]"
     }
    },
    "6caedd60c6b747469c82930be1f95d6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3bcd6b6b956347b29e1efa20a1d00542",
      "max": 682133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2fd3d7bbcd6948d8904d33001f95ea03",
      "value": 682133
     }
    },
    "6f5fde5b0ac840a18bd5cc380e564ff6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "775d8bbeceac4e2da4f21ab6235c89ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a6464a355f7464c989033965d418a8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2307c6dcbe0141acb5e61baae19cade7",
      "max": 295,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4747b668e2fa4ab58a449446f80030f5",
      "value": 295
     }
    },
    "a15af9e8158f4903b9189f3d322a5ef3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac00d6c2cf974b33a628acb3f1471316",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_285007b45236478ca147c6df752c8da4",
      "value": " 109/109 [00:00&lt;00:00, 9.44kB/s]"
     }
    },
    "ac00d6c2cf974b33a628acb3f1471316": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "affff1d8a89e4c14955d1b2aa39ff1ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c18f0e3bc35e44d9915c3f84cd282a26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d068f541df3f438dbd5138863e64b2f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d22fbc2c5dbf422399e496c9b500025a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de1a3f7701c243839fe03b930a9b9e30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ebc22683058a4f229c5588e52fc93536",
       "IPY_MODEL_52095cc7087243ac916055e569fd22f3",
       "IPY_MODEL_a15af9e8158f4903b9189f3d322a5ef3"
      ],
      "layout": "IPY_MODEL_21d2e54b5a0a4f79973a512105da43eb"
     }
    },
    "e920dbc173c045d1a32143349f1dff8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_58c794fb7ce543a39fdf66d757f6eeab",
       "IPY_MODEL_8a6464a355f7464c989033965d418a8a",
       "IPY_MODEL_3645438ace1f4596a8dbc157b48c1521"
      ],
      "layout": "IPY_MODEL_58001a60eacc44d5b38a68648adccde4"
     }
    },
    "ebc22683058a4f229c5588e52fc93536": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_083ea69907bb48d4a8fff919bac51aad",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2a190bda0b72407e9a953cd2104dd3b2",
      "value": "special_tokens_map.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

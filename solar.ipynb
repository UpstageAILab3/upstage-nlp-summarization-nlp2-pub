{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eac3251398442c78e7fa8dc817722ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 모델과 토크나이저 불러오기\n",
    "model_name = \"yanolja/EEVE-Korean-10.8B-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 모델 로드 (device_map=\"auto\"로 자동 배분)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    load_in_8bit=True,  # 8-bit quantization 사용\n",
    "    device_map=\"auto\"   # 자동으로 GPU/CPU에 배분\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 대화를 한국어로 20퍼센트 정도로 요약해 주세요: Person1#: 안녕하세요, 오늘 하루 어떠셨어요? \n",
      "Person2#: 요즘 숨쉬기가 좀 힘들어요.\n",
      "Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n",
      "Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n",
      "Person1#: 알고 있는 알레르기가 있나요?\n",
      "Person2#: 아니요, 알고 있는 알레르기는 없어요.\n",
      "Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n",
      "Person2#: 운동을 할 때 많이 나타나요.\n",
      "Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n",
      "Person2#: 도와주셔서 감사합니다, 의사 선생님.\n",
      "\n",
      "Person1: Hello, how was your day?\n",
      "Person2: I'm having trouble breathing lately.\n",
      "Person1: Have you had any recent illnesses like a cold?\n",
      "Person2: No, it's not a cold. It just feels like my chest is heavy when I breathe.\n",
      "Person1: Do you have any known allergies?\n",
      "Person2: No, I don't have any known allergies.\n",
      "Person1: Does this symptom\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 프롬프트\n",
    "prompt = \"\"\"다음 대화를 한국어로 20퍼센트 정도로 요약해 주세요: Person1#: 안녕하세요, 오늘 하루 어떠셨어요? \n",
    "Person2#: 요즘 숨쉬기가 좀 힘들어요.\n",
    "Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n",
    "Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n",
    "Person1#: 알고 있는 알레르기가 있나요?\n",
    "Person2#: 아니요, 알고 있는 알레르기는 없어요.\n",
    "Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n",
    "Person2#: 운동을 할 때 많이 나타나요.\n",
    "Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n",
    "Person2#: 도와주셔서 감사합니다, 의사 선생님.\"\"\"\n",
    "\n",
    "\n",
    "# 입력 프롬프트를 토큰화하고, max_length 설정\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "# 모델을 사용해 응답 생성 (pad_token_id를 eos_token_id로 설정)\n",
    "output_tokens = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# 출력 결과 디코딩\n",
    "output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# 결과 출력\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from g2pk import G2p\n",
    "\n",
    "# G2P 객체 생성\n",
    "g2p = G2p()\n",
    "\n",
    "# 특수 토큰과 대체 단어 설정\n",
    "special_tokens = {\n",
    "    '#Person1#': '임시토큰11',\n",
    "    '#Person2#': '임시토큰12',\n",
    "    '#Person3#': '임시토큰13',\n",
    "    '#PhoneNumber#': '임시토큰14',\n",
    "    '#Address#': '임시토큰15',\n",
    "    '#PassportNumber#': '임시토큰16'\n",
    "}\n",
    "\n",
    "# 특수 토큰을 임시로 대체하는 함수\n",
    "def replace_special_tokens(text):\n",
    "    for token, replacement in special_tokens.items():\n",
    "        text = text.replace(token, replacement)\n",
    "    return text\n",
    "\n",
    "# 특수 토큰을 원래 상태로 복원하는 함수\n",
    "def restore_special_tokens(text):\n",
    "    for replacement, token in special_tokens.items():\n",
    "        text = text.replace(token, replacement)\n",
    "    return text\n",
    "\n",
    "# summary와 dialogue에서 고유한 영어 단어 추출 함수\n",
    "def extract_unique_english_words(text):\n",
    "    words = text.split()\n",
    "    english_words = set()\n",
    "    \n",
    "    for word in words:\n",
    "        cleaned_word = re.sub(r'[^a-zA-Z]', '', word)\n",
    "        if cleaned_word and cleaned_word not in special_tokens.values():\n",
    "            english_words.add(cleaned_word)\n",
    "    \n",
    "    return list(english_words)\n",
    "\n",
    "# 영어 이름을 한국어 발음으로 변환하는 함수\n",
    "def english_to_korean_pronunciation(name):\n",
    "    return g2p(name)\n",
    "\n",
    "# Summary와 dialogue에서 영어 이름을 한국어 발음으로 교체하는 함수\n",
    "def replace_english_names(row):\n",
    "    dialogue = row['dialogue']\n",
    "    summary = row['summary']\n",
    "    \n",
    "    # 특수 토큰을 임시로 대체\n",
    "    dialogue = replace_special_tokens(dialogue)\n",
    "    summary = replace_special_tokens(summary)\n",
    "    \n",
    "    # 1. summary와 dialogue에서 고유한 영어 단어 추출\n",
    "    english_words_summary = extract_unique_english_words(summary)\n",
    "    english_words_dialogue = extract_unique_english_words(dialogue)\n",
    "    \n",
    "    # 2. summary에만 있는 영어 단어 처리\n",
    "    unique_to_summary = list(set(english_words_summary) - set(english_words_dialogue))\n",
    "    \n",
    "    # 3. summary와 dialogue에 모두 등장하는 영어 단어 처리\n",
    "    common_english_words = list(set(english_words_summary) & set(english_words_dialogue))\n",
    "    \n",
    "    # 4. summary에만 있는 영어 단어 번역 및 교체\n",
    "    if unique_to_summary:\n",
    "        for e_word in unique_to_summary:\n",
    "            k_word = english_to_korean_pronunciation(e_word)\n",
    "            summary = summary.replace(e_word, k_word)\n",
    "            print(f'Replacing {e_word} with {k_word} in summary only.')\n",
    "    \n",
    "    # 5. summary와 dialogue에 모두 있는 영어 단어 번역 및 교체\n",
    "    if common_english_words:\n",
    "        for e_word in common_english_words:\n",
    "            k_word = english_to_korean_pronunciation(e_word)\n",
    "            summary = summary.replace(e_word, k_word)\n",
    "            dialogue = dialogue.replace(e_word, k_word)\n",
    "            print(f'Replacing {e_word} with {k_word} in both summary and dialogue.')\n",
    "    \n",
    "    # 업데이트된 값을 반환\n",
    "    row['summary'] = summary\n",
    "    row['dialogue'] = dialogue\n",
    "    \n",
    "    return row\n",
    "\n",
    "# 데이터프레임 생성 예시\n",
    "# new_ordered_df = pd.DataFrame({\n",
    "#     'dialogue': ordered_df[ordered_df['fname'] == 'train_3104']['dialogue'],\n",
    "#     'summary': ordered_df[ordered_df['fname'] == 'train_3104']['summary']\n",
    "# })\n",
    "\n",
    "# Summary와 dialogue에서 영어 이름을 한국어 발음으로 교체\n",
    "new_ordered_df = new_ordered_df.apply(replace_english_names, axis=1)\n",
    "\n",
    "# 특수 토큰을 원래 상태로 복원\n",
    "def restore_tokens(row):\n",
    "    row['dialogue'] = restore_special_tokens(row['dialogue'])\n",
    "    row['summary'] = restore_special_tokens(row['summary'])\n",
    "    return row\n",
    "\n",
    "# 복원 작업 적용\n",
    "new_ordered_df = new_ordered_df.apply(restore_tokens, axis=1)\n",
    "\n",
    "# # 결과 확인\n",
    "# print(new_ordered_df['summary'])\n",
    "# print(new_ordered_df['dialogue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from g2pk import G2p\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "\n",
    "# G2P 객체 생성\n",
    "g2p = G2p()\n",
    "translator =Translator()\n",
    "\n",
    "# NER 모델 로드\n",
    "tokenizer_en = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "model_en = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "ner_en = pipeline(\"ner\", model=model_en, tokenizer=tokenizer_en)\n",
    "\n",
    "\n",
    "\n",
    "special_tokens = {\n",
    "    '#Address#': '#임시주소153#',\n",
    "    '#CarNumber#': '#임시차량번호154#',\n",
    "    '#CardNumber#': '#임시카드번호155#',\n",
    "    '#DateOfBirth#': '#임시생년월일156#',\n",
    "    '#Email#': '#임시이메일157#',\n",
    "    '#PassportNumber#': '#임시여권번호158#',\n",
    "    '#Person#': '#임시인물159#',\n",
    "    '#Person1#': '#임시인물160#',\n",
    "    '#Person2#': '#임시인물161#',\n",
    "    '#Person3#': '#임시인물162#',\n",
    "    '#Person4#': '#임시인물163#',\n",
    "    '#Person5#': '#임시인물164#',\n",
    "    '#Person6#': '#임시인물165#',\n",
    "    '#Person7#': '#임시인물166#',\n",
    "    '#PhoneNumber#': '#임시전화번호167#',\n",
    "    '#SSN#': '#임시주민번호168#'\n",
    "}\n",
    "# 특수 토큰을 임시로 대체하는 함수\n",
    "def replace_special_tokens(text):\n",
    "    for token, replacement in special_tokens.items():\n",
    "        text = text.replace(token, replacement)\n",
    "    return text\n",
    "\n",
    "# 특수 토큰을 원래 상태로 복원하는 함수\n",
    "def restore_special_tokens(text):\n",
    "    for replacement, token in special_tokens.items():\n",
    "        text = text.replace(token, replacement)\n",
    "    return text\n",
    "\n",
    "def detect_language(word):\n",
    "    is_korean = all('가' <= char <= '힣' for char in word)\n",
    "\n",
    "    if is_korean:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# 한글 자모 분리를 위한 유니코드 조작 함수\n",
    "def decompose_hangul(syllable):\n",
    "    base_code = ord(syllable) - 0xAC00\n",
    "    choseong = base_code // 588\n",
    "    jungseong = (base_code - (choseong * 588)) // 28\n",
    "    jongseong = base_code % 28\n",
    "    choseong_base = 0x1100\n",
    "    jungseong_base = 0x1161\n",
    "    jongseong_base = 0x11A7\n",
    "\n",
    "    if jongseong != 0:\n",
    "        return chr(choseong_base + choseong), chr(jungseong_base + jungseong), chr(jongseong_base + jongseong)\n",
    "    else:\n",
    "        return chr(choseong_base + choseong), chr(jungseong_base + jungseong), None  # 종성이 없으면 None\n",
    "\n",
    "def split_syllables(text):\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if '가' <= char <= '힣':  # 한글 음절 여부 확인\n",
    "            result.append(decompose_hangul(char))\n",
    "        else:\n",
    "            result.append((char,))\n",
    "    return result\n",
    "\n",
    "def normalize_consonant(consonant):\n",
    "    if consonant is None:\n",
    "        return None\n",
    "    consonant_code = ord(consonant)\n",
    "    \n",
    "    # 초성을 종성으로 변환\n",
    "    if 0x1100 <= consonant_code <= 0x1112:  # 초성 범위\n",
    "        return chr(consonant_code + 169)  # 초성을 종성으로 변환\n",
    "    \n",
    "    # 종성을 초성으로 변환\n",
    "    elif 0x11A8 <= consonant_code <= 0x11C2:  # 종성 범위\n",
    "        return chr(consonant_code - 169)  # 종성을 초성으로 변환\n",
    "    \n",
    "    return consonant\n",
    "\n",
    "# 음절 단위로 유사도 계산 및 초성/종성 변환 후 비교\n",
    "def calculate_similarity(word1, word2):\n",
    "    word1_split = split_syllables(word1)\n",
    "    word2_split = split_syllables(word2)\n",
    "\n",
    "    match_count = 0\n",
    "    max_count = max(len(word1_split), len(word2_split))\n",
    "\n",
    "    i = 0\n",
    "    while i < min(len(word1_split), len(word2_split)):\n",
    "        syllable1 = word1_split[i]\n",
    "        syllable2 = word2_split[i]\n",
    "\n",
    "        print(f\"Comparing syllable '{syllable1}' with '{syllable2}'\")\n",
    "\n",
    "        # 초성, 중성 비교\n",
    "       \n",
    "        if syllable1[0] == syllable2[0]:\n",
    "            match_count += 4/11  # 각 성분이 일치할 때마다 1/3점 부여\n",
    "            print(f\"Partial match for '{syllable1[0]}' and '{syllable2[0]}'. Match count: {match_count}\")\n",
    "        else:\n",
    "            print(f\"No match for '{syllable1[0]}' and '{syllable2[0]}'\")\n",
    "               \n",
    "        if syllable1[1] == syllable2[1]:\n",
    "            match_count += 1/3  # 각 성분이 일치할 때마다 1/3점 부여\n",
    "            print(f\"Partial match for '{syllable1[1]}' and '{syllable2[1]}'. Match count: {match_count}\")\n",
    "        else:\n",
    "            print(f\"No match for '{syllable1[1]}' and '{syllable2[1]}'\")\n",
    "\n",
    "        # 종성 비교\n",
    "        if syllable1[2] == syllable2[2]:  # 종성이 일치할 때\n",
    "            match_count += 1/3\n",
    "            print(f\"Match for 종성 '{syllable1[2]}' and '{syllable2[2]}'. Match count: {match_count}\")\n",
    "        elif syllable1[2] is None and syllable2[2] is not None:  # 첫 단어에 종성이 없고, 두 번째 단어에 종성이 있는 경우\n",
    "            if i + 1 < len(word1_split):\n",
    "                next_choseong = normalize_consonant(word1_split[i + 1][0])\n",
    "                if next_choseong == syllable2[2]:\n",
    "                    match_count += 1/3\n",
    "                    print(f\"Match for 종성 '{syllable2[2]}' with next 초성 '{next_choseong}'. Match count: {match_count}\")\n",
    "                else:\n",
    "                    print(f\"No match for 종성 '{syllable1[2]}' and '{syllable2[2]}'\")\n",
    "        elif syllable2[2] is None and syllable1[2] is not None:  # 첫 단어에 종성이 있고, 두 번째 단어에 종성이 없는 경우\n",
    "            if i + 1 < len(word2_split):\n",
    "                next_choseong = normalize_consonant(word2_split[i + 1][0])\n",
    "                if next_choseong == syllable1[2]:\n",
    "                    match_count += 1/3\n",
    "                    print(f\"Match for 종성 '{syllable1[2]}' with next 초성 '{next_choseong}'. Match count: {match_count}\")\n",
    "                else:\n",
    "                    print(f\"No match for 종성 '{syllable1[2]}' and '{syllable2[2]}'\")\n",
    "        else:\n",
    "            print(f\"No match for 종성 '{syllable1[2]}' and '{syllable2[2]}'\")\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # 유사도 비율 계산\n",
    "    similarity = match_count / max_count\n",
    "    print(f\"Final similarity: {similarity:.2f}\\n\")\n",
    "\n",
    "    return similarity\n",
    "\n",
    "# 영어 이름을 한국어 발음으로 변환하고, 동일한 이름이 양쪽에 있으면 그대로 변환\n",
    "def convert_word(word, entity_type, context_words, is_common):\n",
    "    if entity_type in ['B-PER', 'I-PER']:  # 인물 이름인 경우 처리\n",
    "        korean_pronunciation = g2p(word)\n",
    "        google_translation = translator.translate(word, src='en', dest='ko').text\n",
    "        print(f\"Korean Pronunciation of '{word}': {korean_pronunciation}\")\n",
    "        print(is_common)\n",
    "        print(f\"Korean Pronunciation of '{word}': {google_translation} from google\")\n",
    "        \n",
    "        \n",
    "        if is_common:  # 동일한 이름이 양쪽에 있을 경우\n",
    "            if detect_language(google_translation): # 구글 번역이 한글일 경우\n",
    "                return google_translation\n",
    "            elif detect_language(korean_pronunciation): # G2P 가 한글일 경우\n",
    "                return korean_pronunciation\n",
    "            else:\n",
    "                return google_translation\n",
    "        elif context_words:  # Context words가 존재하는지 확인\n",
    "            if detect_language(google_translation) and detect_language(korean_pronunciation):\n",
    "                \n",
    "                best_match = max(context_words, key=lambda w: calculate_similarity(korean_pronunciation, w), default=korean_pronunciation)\n",
    "                best_match_google = max(context_words, key=lambda w: calculate_similarity(google_translation, w), default=google_translation)\n",
    "                similarity_score = calculate_similarity(korean_pronunciation, best_match)\n",
    "                similarity_score_google = calculate_similarity(google_translation, best_match_google)\n",
    "                print(f\"Best match for '{korean_pronunciation}' in context: '{best_match}' with similarity {similarity_score:.2f}\")\n",
    "                print(f\"Best match for '{google_translation}' in context: '{best_match_google}' with similarity {similarity_score_google:.2f}\")\n",
    "                \n",
    "                if similarity_score <= similarity_score_google:\n",
    "                    best_match = best_match_google\n",
    "                    similarity_score = similarity_score_google\n",
    "                \n",
    "                if similarity_score > 0.6:  # 유사도 임계값을 낮춰서 매칭 범위를 확장\n",
    "                    return best_match\n",
    "                else:\n",
    "                    return google_translation\n",
    "            elif detect_language(google_translation):   \n",
    "                                \n",
    "                best_match_google = max(context_words, key=lambda w: calculate_similarity(google_translation, w), default=google_translation)\n",
    "                similarity_score_google = calculate_similarity(google_translation, best_match_google)\n",
    "                print(f\"Best match for '{google_translation}' in context: '{best_match_google}' with similarity {similarity_score_google:.2f}\")\n",
    "                \n",
    "\n",
    "                best_match = best_match_google\n",
    "                similarity_score = similarity_score_google\n",
    "                if similarity_score > 0.6:  # 유사도 임계값을 낮춰서 매칭 범위를 확장\n",
    "                    return best_match\n",
    "                else:\n",
    "                    return google_translation\n",
    "                \n",
    "            elif detect_language(korean_pronunciation):\n",
    "                \n",
    "                best_match = max(context_words, key=lambda w: calculate_similarity(korean_pronunciation, w), default=korean_pronunciation)\n",
    "                similarity_score = calculate_similarity(korean_pronunciation, best_match)\n",
    "                print(f\"Best match for '{korean_pronunciation}' in context: '{best_match}' with similarity {similarity_score:.2f}\")\n",
    "                \n",
    "                if similarity_score > 0.6:  # 유사도 임계값을 낮춰서 매칭 범위를 확장\n",
    "                    return best_match\n",
    "                else:\n",
    "                    return korean_pronunciation\n",
    "            else:\n",
    "                return word\n",
    "                \n",
    "        elif detect_language(google_translation):\n",
    "            return google_translation\n",
    "        elif detect_language(korean_pronunciation):\n",
    "            return korean_pronunciation\n",
    "        else:\n",
    "            return word\n",
    "    else:\n",
    "        return word  # 기타 개체는 그대로 유지\n",
    "\n",
    "# 엔티티 인식 후 변환하는 메인 함수\n",
    "def process_text(row):\n",
    "    dialogue = row['dialogue']\n",
    "    summary = row['summary']\n",
    "    \n",
    "    english_words_dialogue = extract_english_words(dialogue)\n",
    "    print(english_words_dialogue)\n",
    "    english_words_summary = extract_english_words(summary)\n",
    "    print(english_words_summary)\n",
    "    english_words = list(set(english_words_dialogue + english_words_summary))\n",
    "    entities = classify_entities(english_words, ner_en)  # 영어 NER 사용\n",
    "    \n",
    "    context_words = re.findall(r'\\b[가-힣]+\\b', dialogue + \" \" + summary)  # 한국어 단어 추출\n",
    "    print(context_words)\n",
    "\n",
    "    for word, entity_type in entities.items():\n",
    "        is_common = word in english_words_dialogue and word in english_words_summary\n",
    "        converted_word = convert_word(word, entity_type, context_words, is_common)\n",
    "        dialogue = dialogue.replace(word, converted_word)\n",
    "        summary = summary.replace(word, converted_word)\n",
    "    \n",
    "    row['new_dialogue'] = dialogue\n",
    "    row['new_summary'] = summary\n",
    "    return row\n",
    "\n",
    "# 영어 단어 추출 함수\n",
    "def extract_english_words(text):\n",
    "    # 정규 표현식을 사용하여 영어 단어만 추출\n",
    "    english_words = re.findall(r'\\b[A-Za-z]+\\b', text)\n",
    "    return english_words\n",
    "\n",
    "\n",
    "# 엔티티 분류 함수\n",
    "def classify_entities(words, ner_pipeline):\n",
    "    entities = {}\n",
    "    for word in words:\n",
    "        ner_results = ner_pipeline(word)\n",
    "        if ner_results:\n",
    "            entities[word] = ner_results[0]['entity']\n",
    "    print(f\"Extracted Entities: {entities}\")  # NER 모델 출력 확인\n",
    "    return entities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_filtered['dialogue'] = df_filtered['dialogue'].apply(replace_special_tokens)\n",
    "df_filtered['summary'] = df_filtered['summary'].apply(replace_special_tokens)\n",
    "\n",
    "df_filtered = df_filtered.apply(process_text, axis=1)\n",
    "\n",
    "# 특수 토큰 복원\n",
    "df_filtered['dialogue'] = df_filtered['dialogue'].apply(restore_special_tokens)\n",
    "df_filtered['summary'] = df_filtered['summary'].apply(restore_special_tokens)\n",
    "\n",
    "df_filtered['new_dialogue'] = df_filtered['new_dialogue'].apply(restore_special_tokens)\n",
    "df_filtered['new_summary'] = df_filtered['new_summary'].apply(restore_special_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "\n",
    "# 번역기 객체 생성\n",
    "translator = Translator()\n",
    "\n",
    "# 예제 DataFrame (실제 df로 대체하세요)\n",
    "\n",
    "\n",
    "# 1. english_words_count가 0이 아닌 행 필터링\n",
    "df_filtered = new_ordered_df[new_ordered_df['english_words_count'] > 0].copy()\n",
    "\n",
    "# 2. dialogue와 summary를 영어로 번역한 뒤 다시 한국어로 재번역\n",
    "df_filtered['dialogue_translated'] = df_filtered['dialogue'].apply(lambda x: translator.translate(translator.translate(x, src='ko', dest='en').text, src='en', dest='ko').text)\n",
    "df_filtered['summary_translated'] = df_filtered['summary'].apply(lambda x: translator.translate(translator.translate(x, src='ko', dest='en').text, src='en', dest='ko').text)\n",
    "\n",
    "# 3. 원래 df에 재번역된 값 채워넣기\n",
    "new_ordered_df.loc[new_ordered_df['english_words_count'] > 0, 'dialogue'] = df_filtered['dialogue_translated']\n",
    "new_ordered_df.loc[new_ordered_df['english_words_count'] > 0, 'summary'] = df_filtered['summary_translated']\n",
    "\n",
    "# 결과 확인\n",
    "print(new_ordered_df[['dialogue', 'summary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ordered_df = pd.read_csv('/dj/new_filtered.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'dialogue': [\n",
    "        \"#Person1#: 안녕, Peter Johnson! 어떻게 지냈어? 우리가 마지막으로 만난 지 꽤 됐네. \"\n",
    "        \"#Person2#: 안녕, 죤 스미트! 난 잘 지냈어. 어제 뉴욕에서 돌아왔어. \"\n",
    "        \"#Person1#: 뉴욕에서 좋은 시간 보냈어? 거기서 뭘 했어? \"\n",
    "        \"#Person2#: 네, 정말 좋은 시간이었어. 주로 친구들을 만나고 몇 가지 비즈니스 미팅도 했어. \"\n",
    "        \"#Person1#: 좋았겠네! 나도 뉴욕에 가본 지 오래됐어. 다음에 같이 가자. \"\n",
    "        \"#Person2#: 좋은 생각이야. 그때는 우리 둘 다 여유를 좀 가질 수 있었으면 좋겠어. \"\n",
    "    ],\n",
    "    'summary': [\n",
    "        \"#Person7# Peter Johnson and John Smith catch up after not seeing each other for a while. \"\n",
    "        \"John mentions that he recently returned from New York, where he spent time meeting friends and attending business meetings. \"\n",
    "        \"They both agree to plan a trip to New York together in the future.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# 특수 토큰 설정 및 대체 단어 정의\n",
    "special_tokens = {\n",
    "    '#Address#': '#임시주소153#',\n",
    "    '#CarNumber#': '#임시차량번호154#',\n",
    "    '#CardNumber#': '#임시카드번호155#',\n",
    "    '#DateOfBirth#': '#임시생년월일156#',\n",
    "    '#Email#': '#임시이메일157#',\n",
    "    '#PassportNumber#': '#임시여권번호158#',\n",
    "    '#Person#': '#임시인물159#',\n",
    "    '#Person1#': '#임시인물160#',\n",
    "    '#Person2#': '#임시인물161#',\n",
    "    '#Person3#': '#임시인물162#',\n",
    "    '#Person4#': '#임시인물163#',\n",
    "    '#Person5#': '#임시인물164#',\n",
    "    '#Person6#': '#임시인물165#',\n",
    "    '#Person7#': '#임시인물166#',\n",
    "    '#PhoneNumber#': '#임시전화번호167#',\n",
    "    '#SSN#': '#임시주민번호168#'\n",
    "}\n",
    "# 특수 토큰을 임시로 대체하는 함수\n",
    "def replace_special_tokens(text):\n",
    "    for token, replacement in special_tokens.items():\n",
    "        text = text.replace(token, replacement)\n",
    "    return text\n",
    "\n",
    "# 특수 토큰을 원래 상태로 복원하는 함수\n",
    "def restore_special_tokens(text):\n",
    "    for replacement, token in special_tokens.items():\n",
    "        text = text.replace(token, replacement)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "df['dialogue'] = df['dialogue'].apply(replace_special_tokens)\n",
    "df['summary'] = df['summary'].apply(replace_special_tokens)\n",
    "\n",
    "\n",
    "# 특수 토큰 복원\n",
    "df['dialogue'] = df['dialogue'].apply(restore_special_tokens)\n",
    "df['summary'] = df['summary'].apply(restore_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "JVMNotFoundException",
     "evalue": "No JVM shared library file (libjvm.so) found. Try setting up the JAVA_HOME environment variable properly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJVMNotFoundException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 133\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# 예제 실행\u001b[39;00m\n\u001b[1;32m    132\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m안녕하세요, 오늘 날씨가 참 좋네요. 어떻게 지내세요?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 133\u001b[0m \u001b[43mcalculate_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 102\u001b[0m, in \u001b[0;36mcalculate_importance\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     98\u001b[0m total_importance \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# 문장 전처리 및 임베딩\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     embeddings, tokens, pos_tags \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_and_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Self-Attention 객체 생성 및 중요도 계산\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     attention \u001b[38;5;241m=\u001b[39m SelfAttention(d_model, num_heads, seed)\n",
      "Cell \u001b[0;32mIn[1], line 57\u001b[0m, in \u001b[0;36mpreprocess_and_embed\u001b[0;34m(text, d_model)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_and_embed\u001b[39m(text, d_model):\n\u001b[0;32m---> 57\u001b[0m     kkma \u001b[38;5;241m=\u001b[39m \u001b[43mKkma\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# 특수문자 제거\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/konlpy/tag/_kkma.py:44\u001b[0m, in \u001b[0;36mKkma.__init__\u001b[0;34m(self, jvmpath, max_heap_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, jvmpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_heap_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jpype\u001b[38;5;241m.\u001b[39misJVMStarted():\n\u001b[0;32m---> 44\u001b[0m         \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvmpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_heap_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     kkmaJavaPackage \u001b[38;5;241m=\u001b[39m jpype\u001b[38;5;241m.\u001b[39mJPackage(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkr.lucypark.kkma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     47\u001b[0m     KkmaInterfaceJavaClass \u001b[38;5;241m=\u001b[39m kkmaJavaPackage\u001b[38;5;241m.\u001b[39mKkmaInterface\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/konlpy/jvm.py:55\u001b[0m, in \u001b[0;36minit_jvm\u001b[0;34m(jvmpath, max_heap_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m args \u001b[38;5;241m=\u001b[39m [javadir, os\u001b[38;5;241m.\u001b[39msep]\n\u001b[1;32m     53\u001b[0m classpath \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m folder_suffix]\n\u001b[0;32m---> 55\u001b[0m jvmpath \u001b[38;5;241m=\u001b[39m jvmpath \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDefaultJVMPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# NOTE: Temporary patch for Issue #76. Erase when possible.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m'\u001b[39m\\\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m jvmpath\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.8.0\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\\\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m jvmpath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibjvm.dylib\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/jpype/_jvmfinder.py:74\u001b[0m, in \u001b[0;36mgetDefaultJVMPath\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     finder \u001b[38;5;241m=\u001b[39m LinuxJVMFinder()\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_jvm_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/jpype/_jvmfinder.py:212\u001b[0m, in \u001b[0;36mJVMFinder.get_jvm_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jvm_notsupport_ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jvm_notsupport_ext\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m JVMNotFoundException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo JVM shared library file (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    213\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound. Try setting up the JAVA_HOME \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment variable properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m                            \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libfile))\n",
      "\u001b[0;31mJVMNotFoundException\u001b[0m: No JVM shared library file (libjvm.so) found. Try setting up the JAVA_HOME environment variable properly."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from konlpy.tag import Kkma\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "\n",
    "class SelfAttention:\n",
    "    def __init__(self, d_model, num_heads, seed):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        assert d_model % num_heads == 0, \"d_model / num_heads 는 나머지가 0이어야 합니다.\"\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        # 무작위 시드 고정\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Query, Key, Value를 위한 가중치 초기화\n",
    "        self.wq = np.random.randn(d_model, d_model)  # Q의 가중치\n",
    "        self.wk = np.random.randn(d_model, d_model)  # K의 가중치\n",
    "        self.wv = np.random.randn(d_model, d_model)  # V의 가중치\n",
    "        self.wo = np.random.randn(d_model, d_model)  # Output의 가중치\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.transpose(0, 2, 1, 3)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v):\n",
    "        matmul_qk = np.matmul(q, k.transpose(0, 1, 3, 2))\n",
    "        dk = k.shape[-1]\n",
    "        scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
    "        attention_weights = self.softmax(scaled_attention_logits)\n",
    "        output = np.matmul(attention_weights, v)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        q = np.matmul(x, self.wq)\n",
    "        k = np.matmul(x, self.wk)\n",
    "        v = np.matmul(x, self.wv)\n",
    "\n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "        attention_output = attention_output.transpose(0, 2, 1, 3).reshape(batch_size, -1, self.d_model)\n",
    "        output = np.matmul(attention_output, self.wo)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "def preprocess_and_embed(text, d_model):\n",
    "    kkma = Kkma()\n",
    "\n",
    "    # 특수문자 제거\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # 전체 문장을 대상으로 품사 태깅\n",
    "    pos_tags = kkma.pos(text)\n",
    "    print(f'@ pos_tags: {pos_tags}')\n",
    "    # @ pos_tags: [('안녕', 'NNG'), ('하세', 'VV'), ('요', 'EFN'), ('오늘', 'NNG'), ...]\n",
    "\n",
    "    tokens = [word for word, pos in pos_tags]\n",
    "    unique_tokens = list(set(tokens))\n",
    "    print(f'@ tokens: {tokens}')\n",
    "    # @ tokens: ['안녕', '하세', '요', '오늘', '날씨', '가', '참', '좋', '네요', '어떻', '게', '지내', '세요']\n",
    "\n",
    "    print(f'@ unique_tokens: {unique_tokens}')\n",
    "    # @ unique_tokens: ['날씨', '요', '안녕', '참', '가', '하세', '좋', '오늘', '네요', '세요', '어떻', '게', '지내']\n",
    "\n",
    "    # 단어를 정수 인덱스로 변환\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(unique_tokens)\n",
    "    indexed_tokens = encoder.transform(tokens)\n",
    "    print(f'@ indexed_tokens: {indexed_tokens}')\n",
    "    # @ indexed_tokens: [ 8  5  1  7  0  4  3  6  9 10 11 12  2]\n",
    "\n",
    "    # One-hot 임베딩 (간단한 예시용, 실제로는 더 복잡한 임베딩 사용)\n",
    "    embeddings = np.eye(len(unique_tokens))[indexed_tokens]\n",
    "    print(f'@ embeddings: {embeddings}')\n",
    "    # @ embeddings: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], ...]\n",
    "\n",
    "    # 임베딩 벡터의 차원을 d_model로 맞춤\n",
    "    embeddings = np.pad(embeddings, ((0, 0), (0, d_model - embeddings.shape[1])), 'constant')\n",
    "    print(f'@ embeddings2: {embeddings}')\n",
    "    # @ embeddings2: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], ...]\n",
    "\n",
    "    return embeddings, tokens, pos_tags\n",
    "\n",
    "# 중요도 계산 및 출력\n",
    "def calculate_importance(text):\n",
    "    d_model = 32  # 임베딩 차원\n",
    "    num_heads = 4  # Self-Attention 헤드 수\n",
    "    total_importance = {}\n",
    "\n",
    "    for seed in range(1, 11):\n",
    "        # 문장 전처리 및 임베딩\n",
    "        embeddings, tokens, pos_tags = preprocess_and_embed(text, d_model)\n",
    "\n",
    "        # Self-Attention 객체 생성 및 중요도 계산\n",
    "        attention = SelfAttention(d_model, num_heads, seed)\n",
    "        _, attention_weights = attention.call(embeddings[np.newaxis, ...])\n",
    "\n",
    "        # 각 단어에 대한 중요도를 계산 (Attention 가중치 평균)\n",
    "        token_importance = attention_weights.mean(axis=1).flatten()\n",
    "\n",
    "        for token, importance in zip(tokens, token_importance):\n",
    "            if token in total_importance:\n",
    "                total_importance[token].append(importance)\n",
    "            else:\n",
    "                total_importance[token] = [importance]\n",
    "\n",
    "    # 평균 중요도 계산\n",
    "    avg_importance = {token: np.mean(importances) for token, importances in total_importance.items()}\n",
    "\n",
    "    # 명사(NNG)와 동사(VV)만 남기기 위해 POS 태그 사용\n",
    "    filtered_avg_importance = {token: avg_importance[token] for token, pos in pos_tags if pos in ['NNG', 'VV']}\n",
    "\n",
    "    # 중요도 순으로 단어 정렬\n",
    "    sorted_word_importance = sorted(filtered_avg_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(\"단어 중요도 순위:\")\n",
    "    for word, score in sorted_word_importance:\n",
    "        print(f\"단어: {word}, 중요도 점수: {score:.4f}\")\n",
    "\n",
    "# 예제 실행\n",
    "text = \"안녕하세요, 오늘 날씨가 참 좋네요. 어떻게 지내세요?\"\n",
    "calculate_importance(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

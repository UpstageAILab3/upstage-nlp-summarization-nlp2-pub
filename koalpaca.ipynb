{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3f25e6f58b4aa6979c393919857032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# bitsandbytes 설치 필요\n",
    "# pip install bitsandbytes\n",
    "\n",
    "# KoAlpaca-Polyglot-12.8B 모델 불러오기\n",
    "model_name = \"beomi/KoAlpaca-Polyglot-12.8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 8-bit 양자화로 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    load_in_8bit=True  # 8-bit quantization 적용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약 결과: \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 요약할 텍스트\n",
    "prompt = \"\"\"### 명령어: 다음 대화를 간결하게 요약해 주세요:\n",
    "Person1#: 안녕하세요, 오늘 하루 어떠셨어요?\n",
    "Person2#: 요즘 숨쉬기가 좀 힘들어요.\n",
    "Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n",
    "Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n",
    "Person1#: 알고 있는 알레르기가 있나요?\n",
    "Person2#: 아니요, 알고 있는 알레르기는 없어요.\n",
    "Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n",
    "Person2#: 운동을 할 때 많이 나타나요.\n",
    "Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n",
    "Person2#: 도와주셔서 감사합니다, 의사 선생님.\"\"\"\n",
    "\n",
    "# 입력 텍스트를 토큰화\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "# 모델을 사용해 요약 생성\n",
    "output_tokens = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=100,  # 적절한 길이 설정\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,  # 종료 토큰 사용\n",
    ")\n",
    "\n",
    "# 출력 결과를 디코딩\n",
    "output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# 불필요한 명령어 구문을 제거 (후처리)\n",
    "output_cleaned = output.split(\"### 명령어:\")[0]\n",
    "\n",
    "# 결과 출력\n",
    "print(\"요약 결과:\", output_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 결과: ### 명령어: 다음 대화를 간결하게 요약해 주세요:\n",
      "Person1#: 안녕하세요, 오늘 하루 어떠셨어요?\n",
      "Person2#: 요즘 숨쉬기가 좀 힘들어요.\n",
      "Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n",
      "Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n",
      "Person1#: 알고 있는 알레르기가 있나요?\n",
      "Person2#: 아니요, 알고 있는 알레르기는 없어요.\n",
      "Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n",
      "Person2#: 운동을 할 때 많이 나타나요.\n",
      "Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n",
      "Person2#: 도와주셔서 감사합니다, 의사 선생님.\n",
      "\n",
      "### 응답: \"Person1의 증상은 천식의 증상과 유사합니다. Person2는 활동 시에 숨이 차는 것이 주 증상입니다. Person1은 의사와 함께 검사를 받아야 합니다.\"\n",
      "요약 결과: 다음 대화를 간결하게 요약해 주세요:\n",
      "Person1#: 안녕하세요, 오늘 하루 어떠셨어요?\n",
      "Person2#: 요즘 숨쉬기가 좀 힘들어요.\n",
      "Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n",
      "Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n",
      "Person1#: 알고 있는 알레르기가 있나요?\n",
      "Person2#: 아니요, 알고 있는 알레르기는 없어요.\n",
      "Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n",
      "Person2#: 운동을 할 때 많이 나타나요.\n",
      "Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n",
      "Person2#: 도와주셔서 감사합니다, 의사 선생님.\n",
      "\n",
      "### 응답: \"Person1의 증상은 천식의 증상과 유사합니다. Person2는 활동 시에 숨이 차는 것이 주 증상입니다. Person1은 의사와 함께 검사를 받아야 합니다.\"\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 요약할 텍스트\n",
    "prompt = \"\"\"### 명령어: 다음 대화를 간결하게 요약해 주세요:\n",
    "Person1#: 안녕하세요, 오늘 하루 어떠셨어요?\n",
    "Person2#: 요즘 숨쉬기가 좀 힘들어요.\n",
    "Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n",
    "Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n",
    "Person1#: 알고 있는 알레르기가 있나요?\n",
    "Person2#: 아니요, 알고 있는 알레르기는 없어요.\n",
    "Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n",
    "Person2#: 운동을 할 때 많이 나타나요.\n",
    "Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n",
    "Person2#: 도와주셔서 감사합니다, 의사 선생님.\"\"\"\n",
    "\n",
    "# 입력 텍스트를 토큰화\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "# 모델을 사용해 요약 생성\n",
    "output_tokens = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=100,  # 적절한 길이 설정\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,  # 종료 토큰 사용\n",
    ")\n",
    "\n",
    "# 출력 결과를 디코딩\n",
    "output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# 디버깅을 위해 전체 결과 출력\n",
    "print(\"전체 결과:\", output)\n",
    "\n",
    "# 불필요한 명령어 구문을 제거하는 후처리\n",
    "output_cleaned = output.replace(\"### 명령어:\", \"\").strip()\n",
    "\n",
    "# 최종 요약 결과 출력\n",
    "print(\"요약 결과:\", output_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 결과: ### 명령어: 다음 대화를 간결하게 요약해 주세요:\n",
      "Person1#: 안녕하세요, 오늘 하루 어떠셨어요?\n",
      "Person2#: 요즘 숨쉬기가 좀 힘들어요.\n",
      "Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n",
      "Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n",
      "Person1#: 알고 있는 알레르기가 있나요?\n",
      "Person2#: 아니요, 알고 있는 알레르기는 없어요.\n",
      "Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n",
      "Person2#: 운동을 할 때 많이 나타나요.\n",
      "Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n",
      "Person2#: 도와주셔서 감사합니다, 의사 선생님.\n",
      "\n",
      "### 응답: \"Person1의 증상은 천식의 증상과 유사합니다. Person2는 활동 시에 숨이 차는 것이 주 증상입니다. Person1은 의사와 함께 검사를 받아야 합니다.\"\n",
      "요약 결과: \"Person1의 증상은 천식의 증상과 유사합니다. Person2는 활동 시에 숨이 차는 것이 주 증상입니다. Person1은 의사와 함께 검사를 받아야 합니다.\"\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 요약할 텍스트\n",
    "prompt = \"\"\"### 명령어: 다음 대화를 간결하게 요약해 주세요:\n",
    "Person1#: 안녕하세요, 오늘 하루 어떠셨어요?\n",
    "Person2#: 요즘 숨쉬기가 좀 힘들어요.\n",
    "Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n",
    "Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n",
    "Person1#: 알고 있는 알레르기가 있나요?\n",
    "Person2#: 아니요, 알고 있는 알레르기는 없어요.\n",
    "Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n",
    "Person2#: 운동을 할 때 많이 나타나요.\n",
    "Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n",
    "Person2#: 도와주셔서 감사합니다, 의사 선생님.\"\"\"\n",
    "\n",
    "# 입력 텍스트를 토큰화\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "# 모델을 사용해 요약 생성\n",
    "output_tokens = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=100,  # 적절한 길이 설정\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,  # 종료 토큰 사용\n",
    ")\n",
    "\n",
    "# 출력 결과를 디코딩\n",
    "output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# 디버깅을 위해 전체 결과 출력\n",
    "print(\"전체 결과:\", output)\n",
    "\n",
    "# \"### 응답\" 뒤의 결과만 추출하는 후처리\n",
    "if \"### 응답:\" in output:\n",
    "    output_cleaned = output.split(\"### 응답:\")[1].strip()\n",
    "else:\n",
    "    output_cleaned = output\n",
    "\n",
    "# 최종 요약 결과 출력\n",
    "print(\"요약 결과:\", output_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 결과: ### 명령어: 다음 대화를 간결하게 요약해 주세요:\n",
      "#Person1#: UFO를 믿으세요?\n",
      "#Person2#: 물론이죠, 그들은 저기 어딘가에 있어요.\n",
      "#Person1#: 하지만 저는 그들을 본 적이 없어요.\n",
      "#Person2#: 당신은 바본가요? 그들이 UFO라고 불리우는 이유가 모두가 볼 수 있는 것이 아니기 때문이에요.\n",
      "#Person1#: 그러니까 당신은 그들을 볼 수 있다는 거죠.\n",
      "#Person2#: 맞아요. 저는 꿈에서 그들을 볼 수 있어요.\n",
      "#Person1#: 그들이 지구에 오나요?\n",
      "#Person2#: 아니요. 그들의 임무는 외계인들을 우주에서 여기로 보내는 것이에요.\n",
      "#Person1#: 우주에선 온 외계인들이라고요? 그들과 대화하나요? 그들은 어떻게 생겼나요?\n",
      "#Person2#: 좋아요, 좋아요, 하나씩이요! 그들은 로봇처럼 생겼지만 말할 수 있어요. 그들의 임무는 인간과 친구가 되는 것이에요.\n",
      "#Person1#: 그러니까 당신이 그들과 대화한다는 거죠? 어떤 언어로요?\n",
      "#Person2#: 당연히 영어로요, 그들도 화성에서 영어를 배우거든요.\n",
      "#Person1#: 와. 정말 멋져 보이네요!\n",
      "\n",
      "#Person2#: 그리고 당신은 그들과 소통할 수 있어요.\n",
      "\n",
      "#Person1#: 어떻게요?\n",
      "\n",
      "#Person2#: 간단해요. 그들은 우리와 같은 감정을 가지고 있어요. 그들은 당신을 친구로 생각해요.\n",
      "\n",
      "#Person1#: 그러니까, 그들이 저를 친구로 생각한다는 건가요?\n",
      "\n",
      "#Person2#: 네\n",
      "요약 결과: ### 명령어: 다음 대화를 간결하게 요약해 주세요:\n",
      "#Person1#: UFO를 믿으세요?\n",
      "#Person2#: 물론이죠, 그들은 저기 어딘가에 있어요.\n",
      "#Person1#: 하지만 저는 그들을 본 적이 없어요.\n",
      "#Person2#: 당신은 바본가요? 그들이 UFO라고 불리우는 이유가 모두가 볼 수 있는 것이 아니기 때문이에요.\n",
      "#Person1#: 그러니까 당신은 그들을 볼 수 있다는 거죠.\n",
      "#Person2#: 맞아요. 저는 꿈에서 그들을 볼 수 있어요.\n",
      "#Person1#: 그들이 지구에 오나요?\n",
      "#Person2#: 아니요. 그들의 임무는 외계인들을 우주에서 여기로 보내는 것이에요.\n",
      "#Person1#: 우주에선 온 외계인들이라고요? 그들과 대화하나요? 그들은 어떻게 생겼나요?\n",
      "#Person2#: 좋아요, 좋아요, 하나씩이요! 그들은 로봇처럼 생겼지만 말할 수 있어요. 그들의 임무는 인간과 친구가 되는 것이에요.\n",
      "#Person1#: 그러니까 당신이 그들과 대화한다는 거죠? 어떤 언어로요?\n",
      "#Person2#: 당연히 영어로요, 그들도 화성에서 영어를 배우거든요.\n",
      "#Person1#: 와. 정말 멋져 보이네요!\n",
      "\n",
      "#Person2#: 그리고 당신은 그들과 소통할 수 있어요.\n",
      "\n",
      "#Person1#: 어떻게요?\n",
      "\n",
      "#Person2#: 간단해요. 그들은 우리와 같은 감정을 가지고 있어요. 그들은 당신을 친구로 생각해요.\n",
      "\n",
      "#Person1#: 그러니까, 그들이 저를 친구로 생각한다는 건가요?\n",
      "\n",
      "#Person2#: 네\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 요약할 텍스트\n",
    "prompt = \"\"\"### 명령어: 다음 대화를 간결하게 요약해 주세요:\n",
    "#Person1#: UFO를 믿으세요?\n",
    "#Person2#: 물론이죠, 그들은 저기 어딘가에 있어요.\n",
    "#Person1#: 하지만 저는 그들을 본 적이 없어요.\n",
    "#Person2#: 당신은 바본가요? 그들이 UFO라고 불리우는 이유가 모두가 볼 수 있는 것이 아니기 때문이에요.\n",
    "#Person1#: 그러니까 당신은 그들을 볼 수 있다는 거죠.\n",
    "#Person2#: 맞아요. 저는 꿈에서 그들을 볼 수 있어요.\n",
    "#Person1#: 그들이 지구에 오나요?\n",
    "#Person2#: 아니요. 그들의 임무는 외계인들을 우주에서 여기로 보내는 것이에요.\n",
    "#Person1#: 우주에선 온 외계인들이라고요? 그들과 대화하나요? 그들은 어떻게 생겼나요?\n",
    "#Person2#: 좋아요, 좋아요, 하나씩이요! 그들은 로봇처럼 생겼지만 말할 수 있어요. 그들의 임무는 인간과 친구가 되는 것이에요.\n",
    "#Person1#: 그러니까 당신이 그들과 대화한다는 거죠? 어떤 언어로요?\n",
    "#Person2#: 당연히 영어로요, 그들도 화성에서 영어를 배우거든요.\n",
    "#Person1#: 와. 정말 멋져 보이네요!\"\"\"\n",
    "\n",
    "# 입력 텍스트를 토큰화\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "# 모델을 사용해 요약 생성\n",
    "output_tokens = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=100,  # 적절한 길이 설정\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,  # 종료 토큰 사용\n",
    ")\n",
    "\n",
    "# 출력 결과를 디코딩\n",
    "output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# 디버깅을 위해 전체 결과 출력\n",
    "print(\"전체 결과:\", output)\n",
    "\n",
    "# \"### 응답\" 뒤의 결과만 추출하는 후처리\n",
    "if \"### 응답:\" in output:\n",
    "    output_cleaned = output.split(\"### 응답:\")[1].strip()\n",
    "else:\n",
    "    output_cleaned = output\n",
    "\n",
    "# 최종 요약 결과 출력\n",
    "print(\"요약 결과:\", output_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"beomi/KoAlpaca-Polyglot-12.8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db888609ed347fe82ee520ddc833546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(30003, 5120)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear8bitLt(in_features=5120, out_features=15360, bias=True)\n",
       "          (dense): Linear8bitLt(in_features=5120, out_features=5120, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear8bitLt(in_features=5120, out_features=20480, bias=True)\n",
       "          (dense_4h_to_h): Linear8bitLt(in_features=20480, out_features=5120, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=5120, out_features=30003, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 모델 설정\n",
    "MODEL = \"beomi/KoAlpaca-Polyglot-12.8B\"\n",
    "\n",
    "# 모델 로드 (8-bit 양자화만 사용, 오프로드 설정 제거)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    load_in_8bit=True,  # 8-bit 양자화 사용\n",
    "    device_map=\"auto\"   # 자동으로 CPU/GPU에 나누어 로드\n",
    ")\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please discard the `device` argument when creating your pipeline object.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 요약 함수\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize\u001b[39m(dialogue):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/__init__.py:1070\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1068\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m-> 1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:70\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[1;32m     72\u001b[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m     73\u001b[0m     )\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:790\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m hf_device_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_device_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 790\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    792\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscard the `device` argument when creating your pipeline object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    793\u001b[0m     )\n\u001b[1;32m    795\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: The model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please discard the `device` argument when creating your pipeline object."
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    'text-generation', \n",
    "    model=model,\n",
    "    tokenizer=MODEL,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "# 요약 함수\n",
    "def summarize(dialogue):\n",
    "    prompt = f\"### 질문: 다음 대화를 간결하게 요약해 주세요:\\n\\n{dialogue}\\n\\n### 답변:\"\n",
    "    ans = pipe(\n",
    "        prompt,\n",
    "        do_sample=True, \n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        return_full_text=False,\n",
    "        eos_token_id=2,\n",
    "    )\n",
    "    print(ans[0]['generated_text'])\n",
    "\n",
    "# 예시 대화 요약 요청\n",
    "dialogue = \"\"\"#Person1#: UFO를 믿으세요?\n",
    "#Person2#: 물론이죠, 그들은 저기 어딘가에 있어요.\n",
    "#Person1#: 하지만 저는 그들을 본 적이 없어요.\n",
    "#Person2#: 당신은 바본가요? 그들이 UFO라고 불리우는 이유가 모두가 볼 수 있는 것이 아니기 때문이에요.\n",
    "#Person1#: 그러니까 당신은 그들을 볼 수 있다는 거죠.\n",
    "#Person2#: 맞아요. 저는 꿈에서 그들을 볼 수 있어요.\n",
    "#Person1#: 그들이 지구에 오나요?\n",
    "#Person2#: 아니요. 그들의 임무는 외계인들을 우주에서 여기로 보내는 것이에요.\n",
    "#Person1#: 우주에서 온 외계인들이라고요? 그들과 대화하나요? 그들은 어떻게 생겼나요?\n",
    "#Person2#: 좋아요, 좋아요, 하나씩이요! 그들은 로봇처럼 생겼지만 말할 수 있어요. 그들의 임무는 인간과 친구가 되는 것이에요.\n",
    "#Person1#: 그러니까 당신이 그들과 대화한다는 거죠? 어떤 언어로요?\n",
    "#Person2#: 당연히 영어로요, 그들도 화성에서 영어를 배우거든요.\n",
    "#Person1#: 와. 정말 멋져 보이네요!\"\"\"\n",
    "\n",
    "# 요약 호출\n",
    "summarize(dialogue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m\n\u001b[1;32m     18\u001b[0m dialogue \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m#Person1#: UFO를 믿으세요?\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m#Person2#: 물론이죠, 그들은 저기 어딘가에 있어요.\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m#Person1#: 하지만 저는 그들을 본 적이 없어요.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124m#Person2#: 당연히 영어로요, 그들도 화성에서 영어를 배우거든요.\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124m#Person1#: 와. 정말 멋져 보이네요!\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 요약 호출\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdialogue\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36msummarize\u001b[0;34m(dialogue)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize\u001b[39m(dialogue):\n\u001b[1;32m      5\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### 질문: 다음 대화를 간결하게 요약해 주세요:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdialogue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m### 답변:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m     ans \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m(\n\u001b[1;32m      7\u001b[0m         prompt,\n\u001b[1;32m      8\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m      9\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     10\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     11\u001b[0m         top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     12\u001b[0m         return_full_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ans[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "\n",
    "def summarize(dialogue):\n",
    "    prompt = f\"### 질문: 다음 대화를 간결하게 요약해 주세요:\\n\\n{dialogue}\\n\\n### 답변:\"\n",
    "    ans = pipe(\n",
    "        prompt,\n",
    "        do_sample=True, \n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        return_full_text=False,\n",
    "        eos_token_id=2,\n",
    "    )\n",
    "    print(ans[0]['generated_text'])\n",
    "\n",
    "# 예시 대화 요약 요청\n",
    "dialogue = \"\"\"#Person1#: UFO를 믿으세요?\n",
    "#Person2#: 물론이죠, 그들은 저기 어딘가에 있어요.\n",
    "#Person1#: 하지만 저는 그들을 본 적이 없어요.\n",
    "#Person2#: 당신은 바본가요? 그들이 UFO라고 불리우는 이유가 모두가 볼 수 있는 것이 아니기 때문이에요.\n",
    "#Person1#: 그러니까 당신은 그들을 볼 수 있다는 거죠.\n",
    "#Person2#: 맞아요. 저는 꿈에서 그들을 볼 수 있어요.\n",
    "#Person1#: 그들이 지구에 오나요?\n",
    "#Person2#: 아니요. 그들의 임무는 외계인들을 우주에서 여기로 보내는 것이에요.\n",
    "#Person1#: 우주에서 온 외계인들이라고요? 그들과 대화하나요? 그들은 어떻게 생겼나요?\n",
    "#Person2#: 좋아요, 좋아요, 하나씩이요! 그들은 로봇처럼 생겼지만 말할 수 있어요. 그들의 임무는 인간과 친구가 되는 것이에요.\n",
    "#Person1#: 그러니까 당신이 그들과 대화한다는 거죠? 어떤 언어로요?\n",
    "#Person2#: 당연히 영어로요, 그들도 화성에서 영어를 배우거든요.\n",
    "#Person1#: 와. 정말 멋져 보이네요!\"\"\"\n",
    "\n",
    "# 요약 호출\n",
    "summarize(dialogue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4b59c55c3b4d93bdc371713d17eeda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 모델 설정\n",
    "MODEL = \"beomi/KoAlpaca-Polyglot-12.8B\"\n",
    "\n",
    "# 모델 로드 (8-bit 양자화만 사용)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    load_in_8bit=True,  # 8-bit 양자화 사용\n",
    "    device_map=\"auto\"   # 자동으로 CPU/GPU에 나누어 로드\n",
    ")\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 파이프라인 설정 (device 인자 없이 설정)\n",
    "pipe = pipeline(\n",
    "    'text-generation', \n",
    "    model=model,\n",
    "    tokenizer=MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UFO를 목격한 사람들 중 대다수가 구체적인 설명을 하지 않고 \"UFO를 봤다\"고만 말하는 경우가 많습니다. 이에 대해 많은 연구자들이 이 현상에 대해 연구하고 있지만, 명확한 설명은 아직 없습니다. 그렇다면, UFO를 본 사람들은 그것이 무엇인지 구체적으로 인식하지 못할 수도 있습니다. 이는 아마 그들이 정신적인 이미지나 환영을 본 것일 수도 있고, 그들이 익숙하지 않은 현상이기 때문일 수도 있습니다. 또한, 목격자들이 이 미확인 물체를 'UFO'라고 부르는 것은 아마 그 물체가 어떻게 생겼는지에 대한 공통된 의견이 있어서일 것입니다. 많은 사람들이 알고 있는 것처럼 UFO는 미확인 비행물체를 의미합니다. 그리고 UFO를 최초로 목격한 사람들은 보통 그것이 마치 발광하는 것처럼 보였다고 증언하기도 합니다. \n",
      "\n",
      "하지만, UFO는 미확인 비행물체를 가리키는 용어일 뿐이며, 모든 UFO가 미확인 비행물체는 아닙니다. 모든 미확인물체가 UFO는 아니며, UFO의 형태나 비행 패턴도 다양합니다. \n",
      "\n",
      " 추가 답변:\n",
      "UFO의 정체에 대해서는 많은 추측이 있지만, 아직까지 확실하게 밝혀진 것은 없습니다. 일부 연구자들은 UFO가 우주에서 오는 메시지라고 주장하기도 하지만, 이 역시 확실한 근거가 있는 주장은 아닙니다. 따라서, UFO에 대한 다양한 이론들이 있지만, 현재까지는 정확한 근거가 있는 주장은 없는 것으로 보입니다. \n"
     ]
    }
   ],
   "source": [
    "def ask(dialogue):\n",
    "    prompt = f\"### 질문: 다음 대화를 간결하게 요약해 주세요:\\n\\n{dialogue}\\n\\n### 답변:\"\n",
    "    ans = pipe(\n",
    "        prompt,\n",
    "        do_sample=True, \n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        return_full_text=False,\n",
    "        eos_token_id=2,\n",
    "    )\n",
    "    print(ans[0]['generated_text'])\n",
    "\n",
    "# 예시 대화 요약 요청\n",
    "dialogue = \"\"\"#Person1#: UFO를 믿으세요?\n",
    "#Person2#: 물론이죠, 그들은 저기 어딘가에 있어요.\n",
    "#Person1#: 하지만 저는 그들을 본 적이 없어요.\n",
    "#Person2#: 당신은 바본가요? 그들이 UFO라고 불리우는 이유가 모두가 볼 수 있는 것이 아니기 때문이에요.\n",
    "#Person1#: 그러니까 당신은 그들을 볼 수 있다는 거죠.\n",
    "#Person2#: 맞아요. 저는 꿈에서 그들을 볼 수 있어요.\n",
    "#Person1#: 그들이 지구에 오나요?\n",
    "#Person2#: 아니요. 그들의 임무는 외계인들을 우주에서 여기로 보내는 것이에요.\n",
    "#Person1#: 우주에서 온 외계인들이라고요? 그들과 대화하나요? 그들은 어떻게 생겼나요?\n",
    "#Person2#: 좋아요, 좋아요, 하나씩이요! 그들은 로봇처럼 생겼지만 말할 수 있어요. 그들의 임무는 인간과 친구가 되는 것이에요.\n",
    "#Person1#: 그러니까 당신이 그들과 대화한다는 거죠? 어떤 언어로요?\n",
    "#Person2#: 당연히 영어로요, 그들도 화성에서 영어를 배우거든요.\n",
    "#Person1#: 와. 정말 멋져 보이네요!\"\"\"\n",
    "\n",
    "# 요약 호출\n",
    "ask(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 질문: 다음 대화를 간결하게 요약해 주세요. 요약이 끝나면 생성을 중단해.이런 느낌으로 요약해줘 #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다.:\n",
      "\n",
      "#Person1#: UFO를 믿으세요?\n",
      "#Person2#: 물론이죠, 그들은 저기 어딘가에 있어요.\n",
      "#Person1#: 하지만 저는 그들을 본 적이 없어요.\n",
      "#Person2#: 당신은 바본가요? 그들이 UFO라고 불리우는 이유가 모두가 볼 수 있는 것이 아니기 때문이에요.\n",
      "#Person1#: 그러니까 당신은 그들을 볼 수 있다는 거죠.\n",
      "#Person2#: 맞아요. 저는 꿈에서 그들을 볼 수 있어요.\n",
      "#Person1#: 그들이 지구에 오나요?\n",
      "#Person2#: 아니요. 그들의 임무는 외계인들을 우주에서 여기로 보내는 것이에요.\n",
      "#Person1#: 우주에서 온 외계인들이라고요? 그들과 대화하나요? 그들은 어떻게 생겼나요?\n",
      "#Person2#: 좋아요, 좋아요, 하나씩이요! 그들은 로봇처럼 생겼지만 말할 수 있어요. 그들의 임무는 인간과 친구가 되는 것이에요.\n",
      "#Person1#: 그러니까 당신이 그들과 대화한다는 거죠? 어떤 언어로요?\n",
      "#Person2#: 당연히 영어로요, 그들도 화성에서 영어를 배우거든요.\n",
      "#Person1#: 와. 정말 멋져 보이네요!\n",
      "\n",
      "### 답변:위 대화에서 #Person1#은 외계인을 만나본 적이 없다며, 그들을 설명해 달라고 #Person2#에게 요청했습니다. 이에 대해 #Person2#는 그들이 영어를 사용하며, 화성에서 왔다고 했습니다. #Person1#은 이에 대해 매우 관심을 가지게 되었습니다. \n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    'text-generation', \n",
    "    model=model,\n",
    "    tokenizer=MODEL,\n",
    "    max_new_tokens=150,  # 출력 길이 설정\n",
    "    temperature=0.7,     # 생성 랜덤성 제어\n",
    "    top_p=0.9            # 상위 p% 확률 선택\n",
    ")\n",
    "\n",
    "# 요약 함수\n",
    "def ask(dialogue):\n",
    "    prompt = f\"### 질문: 다음 대화를 간결하게 요약해 주세요. 요약이 끝나면 생성을 중단해.이런 느낌으로 요약해줘 #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다.:\\n\\n{dialogue}\\n\\n### 답변:\"\n",
    "    ans = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=100,  # 요약 생성 길이 조정\n",
    "        do_sample=True,      # 텍스트 생성에 랜덤성 적용\n",
    "        eos_token_id=2,      # 종료 토큰 설정\n",
    "        pad_token_id=2       # 패딩 토큰 설정\n",
    "    )\n",
    "    print(ans[0]['generated_text'])\n",
    "\n",
    "# 예시 대화 요약 요청\n",
    "dialogue = \"\"\"#Person1#: UFO를 믿으세요?\n",
    "#Person2#: 물론이죠, 그들은 저기 어딘가에 있어요.\n",
    "#Person1#: 하지만 저는 그들을 본 적이 없어요.\n",
    "#Person2#: 당신은 바본가요? 그들이 UFO라고 불리우는 이유가 모두가 볼 수 있는 것이 아니기 때문이에요.\n",
    "#Person1#: 그러니까 당신은 그들을 볼 수 있다는 거죠.\n",
    "#Person2#: 맞아요. 저는 꿈에서 그들을 볼 수 있어요.\n",
    "#Person1#: 그들이 지구에 오나요?\n",
    "#Person2#: 아니요. 그들의 임무는 외계인들을 우주에서 여기로 보내는 것이에요.\n",
    "#Person1#: 우주에서 온 외계인들이라고요? 그들과 대화하나요? 그들은 어떻게 생겼나요?\n",
    "#Person2#: 좋아요, 좋아요, 하나씩이요! 그들은 로봇처럼 생겼지만 말할 수 있어요. 그들의 임무는 인간과 친구가 되는 것이에요.\n",
    "#Person1#: 그러니까 당신이 그들과 대화한다는 거죠? 어떤 언어로요?\n",
    "#Person2#: 당연히 영어로요, 그들도 화성에서 영어를 배우거든요.\n",
    "#Person1#: 와. 정말 멋져 보이네요!\"\"\"\n",
    "\n",
    "# 요약 호출\n",
    "ask(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'GPTNeoXForCausalLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: UFO를 믿으세요?\n",
      "#Person2#: 물론이죠, 그들은 저기 어딘가에 있어요.\n",
      "#Person1#: 하지만 저는 그들을 본 적이 없어요.\n",
      "#Person2#: 당신은 바본가요? 그들이 UFO라고 불리우는 이유가 모두가 볼 수 있는 것이 아니기 때문이에요.\n",
      "#Person1#: 그러니까 당신은 그들을 볼 수 있다는 거죠.\n",
      "#Person2#: 맞아요. 저는 꿈에서 그들을 볼 수 있어요.\n",
      "#Person1#: 그들이 지구에 오나요?\n",
      "#Person2#: 아니요. 그들의 임무는 외계인들을 우주에서 여기로 보내는 것이에요.\n",
      "#Person1#: 우주에서 온 외계인들이라고요? 그들과 대화하나요? 그들은 어떻게 생겼나요?\n",
      "#Person2#: 좋아요, 좋아요, 하나씩이요! 그들은 로봇처럼 생겼지만 말할 수 있어요. 그들의 임무는 인간과 친구가 되는 것이에요.\n",
      "#Person1#: 그러니까 당신이 그들과 대화한다는 거죠? 어떤 언어로요?\n",
      "#Person2#: 당연히 영어로요, 그들도 화성에서 영어를 배우거든요.\n",
      "#Person1#: 와. 정말 멋져 보이네요!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 319, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 파이프라인 설정 (summarization으로 설정)\n",
    "pipe = pipeline(\n",
    "    'summarization', \n",
    "    model=model,\n",
    "    tokenizer=MODEL\n",
    ")\n",
    "\n",
    "# 요약 함수\n",
    "def ask(dialogue):\n",
    "    # 요약 작업을 요청하는 프롬프트\n",
    "    ans = pipe(\n",
    "        dialogue,\n",
    "        max_length=150,  # 최대 요약 길이\n",
    "        min_length=30,   # 최소 요약 길이\n",
    "        do_sample=False\n",
    "    )\n",
    "    print(ans[0]['summary_text'])\n",
    "\n",
    "# 예시 대화 요약 요청\n",
    "dialogue = \"\"\"#Person1#: UFO를 믿으세요?\n",
    "#Person2#: 물론이죠, 그들은 저기 어딘가에 있어요.\n",
    "#Person1#: 하지만 저는 그들을 본 적이 없어요.\n",
    "#Person2#: 당신은 바본가요? 그들이 UFO라고 불리우는 이유가 모두가 볼 수 있는 것이 아니기 때문이에요.\n",
    "#Person1#: 그러니까 당신은 그들을 볼 수 있다는 거죠.\n",
    "#Person2#: 맞아요. 저는 꿈에서 그들을 볼 수 있어요.\n",
    "#Person1#: 그들이 지구에 오나요?\n",
    "#Person2#: 아니요. 그들의 임무는 외계인들을 우주에서 여기로 보내는 것이에요.\n",
    "#Person1#: 우주에서 온 외계인들이라고요? 그들과 대화하나요? 그들은 어떻게 생겼나요?\n",
    "#Person2#: 좋아요, 좋아요, 하나씩이요! 그들은 로봇처럼 생겼지만 말할 수 있어요. 그들의 임무는 인간과 친구가 되는 것이에요.\n",
    "#Person1#: 그러니까 당신이 그들과 대화한다는 거죠? 어떤 언어로요?\n",
    "#Person2#: 당연히 영어로요, 그들도 화성에서 영어를 배우거든요.\n",
    "#Person1#: 와. 정말 멋져 보이네요!\"\"\"\n",
    "\n",
    "# 요약 호출\n",
    "ask(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'GPTNeoXForCausalLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['return_full_text'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     24\u001b[0m dialogue \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m#Person1#: UFO를 믿으세요?\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m#Person2#: 물론이죠, 그들은 저기 어딘가에 있어요.\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m#Person1#: 하지만 저는 그들을 본 적이 없어요.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124m#Person2#: 당연히 영어로요, 그들도 화성에서 영어를 배우거든요.\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124m#Person1#: 와. 정말 멋져 보이네요!\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 요약 호출\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdialogue\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mask\u001b[0;34m(dialogue)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mask\u001b[39m(dialogue):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# 프롬프트를 좀 더 명확하게 설정하여 요약 작업을 요청\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m다음 대화를 간결하게 요약해 주세요:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdialogue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m간단한 요약:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     ans \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 요약은 짧게 설정\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 종료 토큰 설정\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 패딩을 종료 토큰으로 설정\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ans[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:269\u001b[0m, in \u001b[0;36mSummarizationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m          ids of the summary.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:167\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[1;32m    172\u001b[0m     ):\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:191\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     in_b, input_length \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_inputs(\n\u001b[1;32m    187\u001b[0m     input_length,\n\u001b[1;32m    188\u001b[0m     generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmin_length),\n\u001b[1;32m    189\u001b[0m     generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_length),\n\u001b[1;32m    190\u001b[0m )\n\u001b[0;32m--> 191\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m out_b \u001b[38;5;241m=\u001b[39m output_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1485\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m generation_config\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[0;32m-> 1485\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m logits_processor \u001b[38;5;28;01mif\u001b[39;00m logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1262\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1259\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1263\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1265\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['return_full_text'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    'summarization', \n",
    "    model=model,\n",
    "    tokenizer=MODEL\n",
    ")\n",
    "\n",
    "# 요약 함수\n",
    "def ask(dialogue):\n",
    "    # 프롬프트를 좀 더 명확하게 설정하여 요약 작업을 요청\n",
    "    prompt = f\"다음 대화를 간결하게 요약해 주세요:\\n\\n{dialogue}\\n\\n간단한 요약:\"\n",
    "    ans = pipe(\n",
    "        prompt,\n",
    "        do_sample=True, \n",
    "        max_new_tokens=100,  # 요약은 짧게 설정\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        return_full_text=False,\n",
    "        eos_token_id=2,  # 종료 토큰 설정\n",
    "        pad_token_id=2  # 패딩을 종료 토큰으로 설정\n",
    "    )\n",
    "    print(ans[0]['generated_text'])\n",
    "\n",
    "# 예시 대화 요약 요청\n",
    "dialogue = \"\"\"#Person1#: UFO를 믿으세요?\n",
    "#Person2#: 물론이죠, 그들은 저기 어딘가에 있어요.\n",
    "#Person1#: 하지만 저는 그들을 본 적이 없어요.\n",
    "#Person2#: 당신은 바본가요? 그들이 UFO라고 불리우는 이유가 모두가 볼 수 있는 것이 아니기 때문이에요.\n",
    "#Person1#: 그러니까 당신은 그들을 볼 수 있다는 거죠.\n",
    "#Person2#: 맞아요. 저는 꿈에서 그들을 볼 수 있어요.\n",
    "#Person1#: 그들이 지구에 오나요?\n",
    "#Person2#: 아니요. 그들의 임무는 외계인들을 우주에서 여기로 보내는 것이에요.\n",
    "#Person1#: 우주에서 온 외계인들이라고요? 그들과 대화하나요? 그들은 어떻게 생겼나요?\n",
    "#Person2#: 좋아요, 좋아요, 하나씩이요! 그들은 로봇처럼 생겼지만 말할 수 있어요. 그들의 임무는 인간과 친구가 되는 것이에요.\n",
    "#Person1#: 그러니까 당신이 그들과 대화한다는 거죠? 어떤 언어로요?\n",
    "#Person2#: 당연히 영어로요, 그들도 화성에서 영어를 배우거든요.\n",
    "#Person1#: 와. 정말 멋져 보이네요!\"\"\"\n",
    "\n",
    "# 요약 호출\n",
    "ask(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(dialogue):\n",
    "    prompt = f\"### 질문: 다음 대화를 간결하게 요약해 주세요:\\n\\n{dialogue}\\n\\n### 답변:\"\n",
    "    ans = pipe(\n",
    "        prompt,\n",
    "        do_sample=True, \n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        return_full_text=False,\n",
    "        eos_token_id=2,\n",
    "    )\n",
    "    print(ans[0]['generated_text'])\n",
    "\n",
    "# 예시 대화 요약 요청\n",
    "dialogue = \"\"\"#Person1#: UFO를 믿으세요?\n",
    "#Person2#: 물론이죠, 그들은 저기 어딘가에 있어요.\n",
    "#Person1#: 하지만 저는 그들을 본 적이 없어요.\n",
    "#Person2#: 당신은 바본가요? 그들이 UFO라고 불리우는 이유가 모두가 볼 수 있는 것이 아니기 때문이에요.\n",
    "#Person1#: 그러니까 당신은 그들을 볼 수 있다는 거죠.\n",
    "#Person2#: 맞아요. 저는 꿈에서 그들을 볼 수 있어요.\n",
    "#Person1#: 그들이 지구에 오나요?\n",
    "#Person2#: 아니요. 그들의 임무는 외계인들을 우주에서 여기로 보내는 것이에요.\n",
    "#Person1#: 우주에서 온 외계인들이라고요? 그들과 대화하나요? 그들은 어떻게 생겼나요?\n",
    "#Person2#: 좋아요, 좋아요, 하나씩이요! 그들은 로봇처럼 생겼지만 말할 수 있어요. 그들의 임무는 인간과 친구가 되는 것이에요.\n",
    "#Person1#: 그러니까 당신이 그들과 대화한다는 거죠? 어떤 언어로요?\n",
    "#Person2#: 당연히 영어로요, 그들도 화성에서 영어를 배우거든요.\n",
    "#Person1#: 와. 정말 멋져 보이네요!\"\"\"\n",
    "\n",
    "# 요약 호출\n",
    "ask(dialogue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

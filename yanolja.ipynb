{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea219a442ba49a99cd06d40a9bffe1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "Human: 한국의 수도는 어디인가요? 아래 선택지 중 골라주세요.\n",
      "\n",
      "(A) 경성\n",
      "(B) 부산\n",
      "(C) 평양\n",
      "(D) 서울\n",
      "(E) 전주\n",
      "Assistant:\n",
      "(D) 서울이 한국의 수도입니다.\n",
      "\n",
      "신뢰도: 100%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\",\n",
    "    load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    device_map=\"auto\"  # Automatically distribute across CPU/GPU\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\")\n",
    "\n",
    "prompt_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nHuman: {prompt}\\nAssistant:\\n\"\n",
    "text = '한국의 수도는 어디인가요? 아래 선택지 중 골라주세요.\\n\\n(A) 경성\\n(B) 부산\\n(C) 평양\\n(D) 서울\\n(E) 전주'\n",
    "model_inputs = tokenizer(prompt_template.format(prompt=text), return_tensors='pt').to('cuda')\n",
    "\n",
    "outputs = model.generate(**model_inputs, max_new_tokens=256)\n",
    "output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 대화를 20퍼센트 정도로 요약해 주세요:\n",
      "\n",
      "#Person1#: 안녕하세요, 오늘 하루 어떠셨어요? \n",
      "#Person2#: 요즘 숨쉬기가 좀 힘들어요.\n",
      "#Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n",
      "#Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n",
      "#Person1#: 알고 있는 알레르기가 있나요?\n",
      "#Person2#: 아니요, 알고 있는 알레르기는 없어요.\n",
      "#Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n",
      "#Person2#: 운동을 할 때 많이 나타나요.\n",
      "#Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n",
      "#Person2#: 도와주셔서 감사합니다, 의사 선생님\n",
      "\n",
      "요약:\n",
      "#Person2#는 숨쉬기 어려움을 겪고 있으며, 감기나 알려진 알레르기는 없다고 합니다. 증상은 주로 운동할 때 나타나며, #Person1#은 #Person2#를 천식 검사를 위해 폐 전문의에게 소개하기로 결정했습니다.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"다음 대화를 20퍼센트 정도로 요약해 주세요:\\n\\n{prompt}\\n\\n요약:\"\n",
    "text = '''#Person1#: 안녕하세요, 오늘 하루 어떠셨어요? \n",
    "#Person2#: 요즘 숨쉬기가 좀 힘들어요.\n",
    "#Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n",
    "#Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n",
    "#Person1#: 알고 있는 알레르기가 있나요?\n",
    "#Person2#: 아니요, 알고 있는 알레르기는 없어요.\n",
    "#Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n",
    "#Person2#: 운동을 할 때 많이 나타나요.\n",
    "#Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n",
    "#Person2#: 도와주셔서 감사합니다, 의사 선생님'''\n",
    "model_inputs = tokenizer(prompt_template.format(prompt=text), return_tensors='pt').to('cuda')\n",
    "\n",
    "outputs = model.generate(**model_inputs, max_new_tokens=256)\n",
    "output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다.\n",
    "\n",
    "#Person2#는 숨쉬기 어려움을 겪고 있으며, 감기나 알려진 알레르기는 없다고 합니다. 증상은 주로 운동할 때 나타나며, #Person1#은 #Person2#를 천식 검사를 위해 폐 전문의에게 소개하기로 결정했습니다.\n",
    "\n",
    "#Person1#은 UFO를 믿는 #Person2#에게 의문을 제기합니다. #Person2#는 자신이 꿈에서 UFO를 본다고 주장하며, UFO가 외계인들을 지구로 데려오는 임무를 가지고 있다고 말합니다. 그들은 로봇처럼 생겼지만 말할 수 있으며, 인간과 친구가 되기 위해 영어를 배운다고 합니다. #Person1#은 #Person2#가 외계인들과 대화한다는 사실에 흥미를 느낍니다\n",
    "\n",
    "#Person2#는 UFO를 믿고 꿈에서 그들을 볼 수 있다고 말한다. #Person1#는 #Person2#에게 UFO와 꿈 속의 외계인에 대해 묻고, #Person2#의 꿈을 멋지다고 느낀다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

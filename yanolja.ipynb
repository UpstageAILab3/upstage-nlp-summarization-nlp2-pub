{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea219a442ba49a99cd06d40a9bffe1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "Human: 한국의 수도는 어디인가요? 아래 선택지 중 골라주세요.\n",
      "\n",
      "(A) 경성\n",
      "(B) 부산\n",
      "(C) 평양\n",
      "(D) 서울\n",
      "(E) 전주\n",
      "Assistant:\n",
      "(D) 서울이 한국의 수도입니다.\n",
      "\n",
      "신뢰도: 100%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\",\n",
    "    load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    device_map=\"auto\"  # Automatically distribute across CPU/GPU\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\")\n",
    "\n",
    "prompt_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nHuman: {prompt}\\nAssistant:\\n\"\n",
    "text = '한국의 수도는 어디인가요? 아래 선택지 중 골라주세요.\\n\\n(A) 경성\\n(B) 부산\\n(C) 평양\\n(D) 서울\\n(E) 전주'\n",
    "model_inputs = tokenizer(prompt_template.format(prompt=text), return_tensors='pt').to('cuda')\n",
    "\n",
    "outputs = model.generate(**model_inputs, max_new_tokens=256)\n",
    "output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 202, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 대화를 20퍼센트 정도로 요약해 주세요:\n",
      "\n",
      "#Person1#: 안녕하세요, 오늘 하루 어떠셨어요? \n",
      "#Person2#: 요즘 숨쉬기가 좀 힘들어요.\n",
      "#Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n",
      "#Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n",
      "#Person1#: 알고 있는 알레르기가 있나요?\n",
      "#Person2#: 아니요, 알고 있는 알레르기는 없어요.\n",
      "#Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n",
      "#Person2#: 운동을 할 때 많이 나타나요.\n",
      "#Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n",
      "#Person2#: 도와주셔서 감사합니다, 의사 선생님\n",
      "\n",
      "요약만 출력:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"다음 대화를 20퍼센트 정도로 요약해 주세요:\\n\\n{prompt}\\n\\n요약만 출력:\"\n",
    "text = '''#Person1#: 안녕하세요, 오늘 하루 어떠셨어요? \n",
    "#Person2#: 요즘 숨쉬기가 좀 힘들어요.\n",
    "#Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n",
    "#Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n",
    "#Person1#: 알고 있는 알레르기가 있나요?\n",
    "#Person2#: 아니요, 알고 있는 알레르기는 없어요.\n",
    "#Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n",
    "#Person2#: 운동을 할 때 많이 나타나요.\n",
    "#Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n",
    "#Person2#: 도와주셔서 감사합니다, 의사 선생님'''\n",
    "model_inputs = tokenizer(prompt_template.format(prompt=text), return_tensors='pt').to('cuda')\n",
    "\n",
    "outputs = model.generate(**model_inputs, max_new_tokens=0)\n",
    "output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 대화를 20퍼센트 정도로 요약해 주세요. 아웃풋은 요약만 보여주면되:\n",
      "\n",
      "#Person1#: 안녕하세요, 오늘 하루 어떠셨어요? \n",
      "#Person2#: 요즘 숨쉬기가 좀 힘들어요.\n",
      "#Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n",
      "#Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n",
      "#Person1#: 알고 있는 알레르기가 있나요?\n",
      "#Person2#: 아니요, 알고 있는 알레르기는 없어요.\n",
      "#Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n",
      "#Person2#: 운동을 할 때 많이 나타나요.\n",
      "#Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n",
      "#Person2#: 도와주셔서 감사합니다, 의사 선생님\n",
      "\n",
      "요약:\n",
      "#Person2#는 숨쉬기 어려움을 겪고 있으며, 최근에 감기에 걸린 적은 없고 알려진 알레르기도 없습니다. 증상은 주로 운동할 때 나타나며, #Person1#은 #Person2#를 폐 전문의에게 보내 천식 검사를 받게 할 계획입니다.\n"
     ]
    }
   ],
   "source": [
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다.\n",
    "\n",
    "#Person2#는 숨쉬기 어려움을 겪고 있으며, 감기나 알려진 알레르기는 없다고 합니다. 증상은 주로 운동할 때 나타나며, #Person1#은 #Person2#를 천식 검사를 위해 폐 전문의에게 소개하기로 결정했습니다.\n",
    "\n",
    "#Person1#은 UFO를 믿는 #Person2#에게 의문을 제기합니다. #Person2#는 자신이 꿈에서 UFO를 본다고 주장하며, UFO가 외계인들을 지구로 데려오는 임무를 가지고 있다고 말합니다. 그들은 로봇처럼 생겼지만 말할 수 있으며, 인간과 친구가 되기 위해 영어를 배운다고 합니다. #Person1#은 #Person2#가 외계인들과 대화한다는 사실에 흥미를 느낍니다\n",
    "\n",
    "#Person2#는 UFO를 믿고 꿈에서 그들을 볼 수 있다고 말한다. #Person1#는 #Person2#에게 UFO와 꿈 속의 외계인에 대해 묻고, #Person2#의 꿈을 멋지다고 느낀다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/dj/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나...</td>\n",
       "      <td>스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니...</td>\n",
       "      <td>건강검진 받기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>#Person1#: 안녕하세요, 파커 부인, 어떻게 지내셨나요?\\n#Person2#...</td>\n",
       "      <td>파커 부인이 리키를 데리고 백신 접종을 하러 갔다. 피터스 박사는 기록을 확인한 후...</td>\n",
       "      <td>백신</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>#Person1#: 실례합니다, 열쇠 한 묶음 보셨나요?\\n#Person2#: 어떤...</td>\n",
       "      <td>#Person1#은 열쇠 한 묶음을 찾고 있고, 그것을 찾기 위해 #Person2#...</td>\n",
       "      <td>열쇠 찾기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>#Person1#: 왜 너는 여자친구가 있다는 걸 말해주지 않았어?\\n#Person...</td>\n",
       "      <td>#Person1#은 #Person2#가 여자친구가 있고 그녀와 결혼할 것이라는 사실...</td>\n",
       "      <td>여자친구가 있다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>#Person1#: 안녕, 숙녀분들! 오늘 밤 당신들은 정말 멋져 보여. 이 춤을 ...</td>\n",
       "      <td>말릭이 니키에게 춤을 요청한다. 말릭이 발을 밟는 것을 신경 쓰지 않는다면 니키는 ...</td>\n",
       "      <td>댄스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12452</th>\n",
       "      <td>train_12455</td>\n",
       "      <td>#Person1#: 실례합니다. 맨체스터 출신의 그린 씨이신가요?\\n#Person2...</td>\n",
       "      <td>탄 링은 흰머리와 수염으로 쉽게 인식되는 그린 씨를 만나 호텔로 데려갈 예정입니다....</td>\n",
       "      <td>누군가를 태우다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12453</th>\n",
       "      <td>train_12456</td>\n",
       "      <td>#Person1#: 이윙 씨가 우리가 컨퍼런스 센터에 오후 4시에 도착해야 한다고 ...</td>\n",
       "      <td>#Person1#과 #Person2#는 이윙 씨가 늦지 않도록 요청했기 때문에 컨퍼...</td>\n",
       "      <td>컨퍼런스 센터</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12454</th>\n",
       "      <td>train_12457</td>\n",
       "      <td>#Person1#: 오늘 어떻게 도와드릴까요?\\n#Person2#: 차를 빌리고 싶...</td>\n",
       "      <td>#Person2#는 #Person1#의 도움으로 5일 동안 소형 차를 빌립니다.</td>\n",
       "      <td>차 렌트</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12455</th>\n",
       "      <td>train_12458</td>\n",
       "      <td>#Person1#: 오늘 좀 행복해 보이지 않아. 무슨 일 있어?\\n#Person2...</td>\n",
       "      <td>#Person2#의 엄마가 일자리를 잃었다. #Person2#는 엄마가 우울해하지 ...</td>\n",
       "      <td>실직</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12456</th>\n",
       "      <td>train_12459</td>\n",
       "      <td>#Person1#: 엄마, 다음 토요일에 이 삼촌네 가족을 방문하기 위해 비행기를 ...</td>\n",
       "      <td>#Person1#은 다음 토요일에 이 삼촌네를 방문할 때 가방을 어떻게 싸야 할지 ...</td>\n",
       "      <td>짐 싸기</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12457 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             fname                                           dialogue  \\\n",
       "0          train_0  #Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나...   \n",
       "1          train_1  #Person1#: 안녕하세요, 파커 부인, 어떻게 지내셨나요?\\n#Person2#...   \n",
       "2          train_2  #Person1#: 실례합니다, 열쇠 한 묶음 보셨나요?\\n#Person2#: 어떤...   \n",
       "3          train_3  #Person1#: 왜 너는 여자친구가 있다는 걸 말해주지 않았어?\\n#Person...   \n",
       "4          train_4  #Person1#: 안녕, 숙녀분들! 오늘 밤 당신들은 정말 멋져 보여. 이 춤을 ...   \n",
       "...            ...                                                ...   \n",
       "12452  train_12455  #Person1#: 실례합니다. 맨체스터 출신의 그린 씨이신가요?\\n#Person2...   \n",
       "12453  train_12456  #Person1#: 이윙 씨가 우리가 컨퍼런스 센터에 오후 4시에 도착해야 한다고 ...   \n",
       "12454  train_12457  #Person1#: 오늘 어떻게 도와드릴까요?\\n#Person2#: 차를 빌리고 싶...   \n",
       "12455  train_12458  #Person1#: 오늘 좀 행복해 보이지 않아. 무슨 일 있어?\\n#Person2...   \n",
       "12456  train_12459  #Person1#: 엄마, 다음 토요일에 이 삼촌네 가족을 방문하기 위해 비행기를 ...   \n",
       "\n",
       "                                                 summary     topic  \n",
       "0      스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니...   건강검진 받기  \n",
       "1      파커 부인이 리키를 데리고 백신 접종을 하러 갔다. 피터스 박사는 기록을 확인한 후...        백신  \n",
       "2      #Person1#은 열쇠 한 묶음을 찾고 있고, 그것을 찾기 위해 #Person2#...     열쇠 찾기  \n",
       "3      #Person1#은 #Person2#가 여자친구가 있고 그녀와 결혼할 것이라는 사실...  여자친구가 있다  \n",
       "4      말릭이 니키에게 춤을 요청한다. 말릭이 발을 밟는 것을 신경 쓰지 않는다면 니키는 ...        댄스  \n",
       "...                                                  ...       ...  \n",
       "12452  탄 링은 흰머리와 수염으로 쉽게 인식되는 그린 씨를 만나 호텔로 데려갈 예정입니다....  누군가를 태우다  \n",
       "12453  #Person1#과 #Person2#는 이윙 씨가 늦지 않도록 요청했기 때문에 컨퍼...   컨퍼런스 센터  \n",
       "12454       #Person2#는 #Person1#의 도움으로 5일 동안 소형 차를 빌립니다.      차 렌트  \n",
       "12455  #Person2#의 엄마가 일자리를 잃었다. #Person2#는 엄마가 우울해하지 ...        실직  \n",
       "12456  #Person1#은 다음 토요일에 이 삼촌네를 방문할 때 가방을 어떻게 싸야 할지 ...      짐 싸기  \n",
       "\n",
       "[12457 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\")\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt_template = \"다음 대화를 20퍼센트 정도로 요약해 주세요:\\n\\n{dialogue}\\n\\n요약:\"\n",
    "\n",
    "# 데이터 전처리 함수 (프롬프트 포함)\n",
    "def tokenize_function(examples):\n",
    "    # 프롬프트와 함께 dialogue를 입력으로 사용\n",
    "    inputs = [prompt_template.format(dialogue=dialogue) for dialogue in examples['dialogue']]\n",
    "    summaries = examples['summary']\n",
    "    \n",
    "    # 입력과 레이블을 각각 토크나이징\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(summaries, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # 레이블 설정\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# 데이터프레임에 적용\n",
    "tokenized_data = df.apply(tokenize_function, axis=1)\n",
    "\n",
    "# 결과 확인\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\",\n",
    "    load_in_8bit=True,  # 8-bit quantization으로 메모리 최적화\n",
    "    device_map=\"auto\"   # GPU와 CPU에 자동으로 분배\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\")\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def tokenize_function(examples):\n",
    "    inputs = examples['dialogue']\n",
    "    summaries = examples['summary']\n",
    "    \n",
    "    # 입력과 레이블을 각각 토크나이징\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(summaries, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # 레이블 설정\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# 데이터셋 준비\n",
    "# df는 미리 준비된 pandas 데이터프레임이라고 가정\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# 데이터 토크나이징\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 데이터 콜레이터 설정: Seq2Seq 작업에 사용\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",           # 체크포인트 저장 경로\n",
    "    evaluation_strategy=\"epoch\",      # 매 에폭마다 평가\n",
    "    learning_rate=5e-5,               # 학습률 설정\n",
    "    per_device_train_batch_size=2,    # 학습 배치 크기\n",
    "    per_device_eval_batch_size=2,     # 평가 배치 크기\n",
    "    num_train_epochs=3,               # 에폭 수\n",
    "    weight_decay=0.01,                # 가중치 감쇠 (정규화)\n",
    "    save_total_limit=2,               # 저장할 체크포인트 수 제한\n",
    "    fp16=True,                        # 혼합 정밀도 사용 (GPU 메모리 최적화)\n",
    "    save_steps=1000,                  # 체크포인트 저장 주기\n",
    "    logging_dir=\"./logs\",             # 로그 저장 경로\n",
    "    logging_steps=500,                # 로그 주기\n",
    ")\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,  # 평가용 데이터도 동일하게 설정\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 모델 훈련\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset\n",
    "\n",
    "# 스페셜 토큰 정의\n",
    "special_tokens_dict = {\n",
    "    \"additional_special_tokens\": ['#Person1#', '#Person2#', '#Person3#', '#PhoneNumber#', '#Address#', '#PassportNumber#']\n",
    "}\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\")\n",
    "\n",
    "# 스페셜 토큰 추가\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# 모델 로드 후 스페셜 토큰 크기를 모델에 맞게 조정\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))  # 모델 토큰 임베딩 사이즈 조정\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt_template = \"다음 대화를 20퍼센트 정도로 요약해 주세요:\\n\\n{dialogue}\\n\\n요약:\"\n",
    "\n",
    "# 데이터 전처리 함수 (프롬프트 및 스페셜 토큰 포함)\n",
    "def tokenize_function(examples):\n",
    "    # 프롬프트와 함께 dialogue를 입력으로 사용\n",
    "    inputs = [prompt_template.format(dialogue=dialogue) for dialogue in examples['dialogue']]\n",
    "    summaries = examples['summary']\n",
    "    \n",
    "    # 입력과 레이블을 각각 토크나이징, special_tokens 적용\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(summaries, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # 레이블 설정\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# 데이터셋 준비\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# 데이터 토크나이징\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 데이터 콜레이터 설정\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    save_steps=1000,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    ")\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 모델 훈련\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

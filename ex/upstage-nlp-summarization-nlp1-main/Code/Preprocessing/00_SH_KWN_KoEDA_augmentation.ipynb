{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 data augmentation (using koeda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KoEDA  \n",
    "- `ì˜ì–´ìš©ìœ¼ë¡œ êµ¬í˜„ëœ Easy data augmentation ê³¼ An Easier Data Augmentation í”„ë¡œì íŠ¸ë¥¼ í•œê¸€ìš©ìœ¼ë¡œ ì¬êµ¬ì„±í•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¯ EasyDataAugmentation íŒŒë¼ë¯¸í„°\n",
    " - `p = (alpha_sr,alpha_ri,alpha_rs,prob_rd)` ì€ <br> SR, RI , RS , RD ì— ëŒ€í•œ ê°ê°ì˜ ë³€í™˜ì„ ì–´ëŠì •ë„ ë¹„ìœ¨ë¡œ í•  ê²ƒì¸ì§€ ê²°ì •\n",
    " > - Synonym Replacement __(SR)__ : ìœ ì˜ì–´ êµì²´\n",
    " > - Random Insertion __(RI)__ : ì„ì˜ ë‹¨ì–´ ì‚½ì…\n",
    " > - Random Swap __(RS)__ : ë‘ ë‹¨ì–´ ìœ„ì¹˜ ë³€ê²½\n",
    " > - Random Deletion __(RD)__ : ì„ì˜ ë‹¨ì–´ ì‚­ì œ\n",
    " - morpheme_analyzer ëŠ” ì‚¬ìš©ë  í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì§€ì •í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œ, <br> [\"Okt\", \"Kkma\", \"Komoran\", \"Mecab\", \"Hannanum\"] ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•  ìˆ˜ ìˆë‹¤.  <br>__(ë‹¨, ì¼ë¶€ëŠ” ì„¤ì¹˜ í•„ìš”í•˜ë©°, ê°ê° í˜•íƒœì†Œë¥¼ ë‚˜ëˆ„ëŠ” ê¸°ì¤€ì´ ë‹¤ë¥´ë‹¤. )__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•„ë²„ì§€ê°€ íœ´ê²Œì‹¤ì— ë“¤ì–´ê°€ì‹ ë‹¤\n",
      "['ì•„ë²„ì§€ë°© ì—ê°€ ë“¤ì–´ê°€ì‹ ë‹¤', 'ì•„ë²„ì§€ê°€ ë°©ì— ì•ˆë°© íœ´ê²Œì‹¤ ë“¤ì–´ê°€ì‹ ë‹¤', 'ì•„ë²„ì§€ê°€ ì¹¨ì‹¤ì— ë“¤ì–´ê°€ì‹ ë‹¤']\n",
      "íƒˆì˜ì‹¤ ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì‹œã„´ë‹¤\n",
      "['ì•„ë²„ì§€ê°€ ê³¨ë°©ì— ë“¤ì–´ê°€ì‹œã„´ë‹¤', 'ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì‹œã„´ë‹¤', 'ì¹¨íˆ¬í•˜ ì•„ë²„ì§€ê°€ ë°© ì¹¨ì‹¤ ê³¨ë°© ì— ë“¤ì–´ê°€ì‹œã„´ë‹¤']\n",
      "ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì‹œã„´ë‹¤\n",
      "['ë¶€ì¹œê°€ ë°©ì— ë“¤ì–´ê°€ì‹œã„´ë‹¤', 'ã„´ë‹¤ê°€ ë°©ì‹œ ì—ë“¤ì–´ê°€ì•„ë²„ì§€', 'ì•„ë²„ì§€ê°€ ì‹œì—ë°© ë“¤ì–´ê°€ã„´ë‹¤']\n",
      "ë“¤ì–´ê°€ê°€ ë°©ì— ì•„ë²„ì§€ì‹ ë‹¤\n",
      "['ì•„ë²„ì§€ë°© ì—ê°€ ë“¤ì–´ê°€ì‹ ë‹¤', 'ì•„ë²„ì§€ê°€ ë°©ì— ì•ˆë°© íœ´ê²Œì‹¤ ë“¤ì–´ê°€ì‹ ë‹¤', 'ì•„ë²„ì§€ê°€ ì¹¨ì‹¤ì— ë“¤ì–´ê°€ì‹ ë‹¤']\n",
      "ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì‹œã„´ë‹¤ì´\n",
      "['ì¹¨ì‹¤ í‰ì› ë…ë°© ë¶€ëª¨ ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì´ì‹œã„´ë‹¤', 'ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì´ì‹œã„´ë‹¤', 'ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì´ì‹œã„´ë‹¤']\n"
     ]
    }
   ],
   "source": [
    "from koeda import EDA\n",
    "from koeda import EasyDataAugmentation\n",
    "\n",
    "\n",
    "eda = EasyDataAugmentation(\n",
    "    morpheme_analyzer=\"Okt\"\n",
    "    )\n",
    "\n",
    "eda1 = EasyDataAugmentation(\n",
    "    morpheme_analyzer=\"Kkma\"\n",
    "    )\n",
    "\n",
    "eda2 = EasyDataAugmentation(\n",
    "    morpheme_analyzer=\"Komoran\"\n",
    "    )\n",
    "\n",
    "eda3 = EasyDataAugmentation(\n",
    "    morpheme_analyzer=\"Mecab\"\n",
    "    )\n",
    "\n",
    "eda4 = EasyDataAugmentation(\n",
    "    morpheme_analyzer=\"Hannanum\"\n",
    "    )\n",
    "\n",
    "text = \"ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì‹ ë‹¤\"\n",
    "\n",
    "result = eda(text)\n",
    "print(result)\n",
    "# ì•„ë²„ì§€ê°€ ì •ì‹¤ì— ë“¤ì–´ê°€ì‹ ë‹¤\n",
    "\n",
    "result = eda(text,p=(0.9, 0.5, 0.5, 0.0), repetition=3)\n",
    "print(result)\n",
    "# ['ì•„ë²„ì§€ê°€ ê°ì‹¤ ì•„ë¹  ì•ˆë°© ë°©ì— ì •ì‹¤ ë“¤ì–´ê°€ì‹ ë‹¤', 'ì•„ë²„ì§€ê°€ íƒˆì˜ì‹¤ ë°© íœ´ê²Œì‹¤ ì— ì•ˆë°© íƒˆì˜ì‹¤ ë“¤ì–´ê°€ì‹ ë‹¤']\n",
    "\n",
    "result1 = eda1(text)\n",
    "print(result1)\n",
    "# ì•„ë²„ì§€ê°€ ì •ì‹¤ì— ë“¤ì–´ê°€ì‹ ë‹¤\n",
    "\n",
    "result1 = eda1(text,p=(0.5, 0.5, 0.5, 0.0), repetition=3)\n",
    "print(result1)\n",
    "# ['ì•„ë²„ì§€ê°€ ê°ì‹¤ ì•„ë¹  ì•ˆë°© ë°©ì— ì •ì‹¤ ë“¤ì–´ê°€ì‹ ë‹¤', 'ì•„ë²„ì§€ê°€ íƒˆì˜ì‹¤ ë°© íœ´ê²Œì‹¤ ì— ì•ˆë°© íƒˆì˜ì‹¤ ë“¤ì–´ê°€ì‹ ë‹¤']\n",
    "\n",
    "result2 = eda2(text)\n",
    "print(result2)\n",
    "# ì•„ë²„ì§€ê°€ ì •ì‹¤ì— ë“¤ì–´ê°€ì‹ ë‹¤\n",
    "\n",
    "result2 = eda2(text,p=(0.5, 0.5, 0.5, 0.0), repetition=3)\n",
    "print(result2)\n",
    "# ['ì•„ë²„ì§€ê°€ ê°ì‹¤ ì•„ë¹  ì•ˆë°© ë°©ì— ì •ì‹¤ ë“¤ì–´ê°€ì‹ ë‹¤', 'ì•„ë²„ì§€ê°€ íƒˆì˜ì‹¤ ë°© íœ´ê²Œì‹¤ ì— ì•ˆë°© íƒˆì˜ì‹¤ ë“¤ì–´ê°€ì‹ ë‹¤']\n",
    "\n",
    "result3 = eda3(text)\n",
    "print(result3)\n",
    "# ì•„ë²„ì§€ê°€ ì •ì‹¤ì— ë“¤ì–´ê°€ì‹ ë‹¤\n",
    "\n",
    "result3 = eda3(text,p=(0.9, 0.9, 0.0, 0.0), repetition=2)\n",
    "print(result)\n",
    "# ['ì•„ë²„ì§€ê°€ ê°ì‹¤ ì•„ë¹  ì•ˆë°© ë°©ì— ì •ì‹¤ ë“¤ì–´ê°€ì‹ ë‹¤', 'ì•„ë²„ì§€ê°€ íƒˆì˜ì‹¤ ë°© íœ´ê²Œì‹¤ ì— ì•ˆë°© íƒˆì˜ì‹¤ ë“¤ì–´ê°€ì‹ ë‹¤']\n",
    "\n",
    "result4 = eda4(text)\n",
    "print(result4)\n",
    "# ì•„ë²„ì§€ê°€ ì •ì‹¤ì— ë“¤ì–´ê°€ì‹ ë‹¤\n",
    "\n",
    "result4 = eda4(text,p=(0.5, 0.5, 0.5, 0.0), repetition=3)\n",
    "print(result4)\n",
    "# ['ì•„ë²„ì§€ê°€ ê°ì‹¤ ì•„ë¹  ì•ˆë°© ë°©ì— ì •ì‹¤ ë“¤ì–´ê°€ì‹ ë‹¤', 'ì•„ë²„ì§€ê°€ íƒˆì˜ì‹¤ ë°© íœ´ê²Œì‹¤ ì— ì•ˆë°© íƒˆì˜ì‹¤ ë“¤ì–´ê°€ì‹ ë‹¤']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from koeda import EasyDataAugmentation\n",
    "\n",
    "def augment_text_data_with_EDA(text,repetition):\n",
    "    \"\"\"ì…ë ¥ëœ ë¬¸ì¥ì— ëŒ€í•´ì„œ EDAë¥¼ í†µí•´ ë°ì´í„° ì¦ê°•\"\"\"\n",
    "    eda = EasyDataAugmentation(\n",
    "        morpheme_analyzer=\"Okt\"\n",
    "        )\n",
    "\n",
    "    result = eda(text,p=(0.5, 0.5, 0.5, 0.5), repetition=repetition)\n",
    "\n",
    "    # ì¦ê°• ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"ì›ë¬¸: \" , text)\n",
    "    print(\"--\"*100)\n",
    "    for i in range(repetition):\n",
    "        print(f\"ì¦ê°•ë¬¸{i+1}: \", result[i])\n",
    "    # return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 data augmentation (using SR / KWN wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNet ë¡œë“œ\n",
    "with open(\"wordnet.pickle\", \"rb\") as f:\n",
    "    wordnet = pickle.load(f)\n",
    "\n",
    "def replace_entities_with_placeholders(text, placeholder_prefix=\"#REPL#\"):\n",
    "    entities = re.findall(r\"#.+?#\", text)\n",
    "    for i, entity in enumerate(entities):\n",
    "        text = text.replace(entity, f\"{placeholder_prefix}{i}#\", 1)\n",
    "    return text, entities\n",
    "\n",
    "def replace_placeholders_with_entities(text, entities, placeholder_prefix=\"#REPL#\"):\n",
    "    for i, entity in enumerate(entities):\n",
    "        text = text.replace(f\"{placeholder_prefix}{i}#\", entity, 1)\n",
    "    return text\n",
    "\n",
    "def augment_text(text, wordnet):\n",
    "    text, entities = replace_entities_with_placeholders(text)\n",
    "    \n",
    "    words = text.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if word in wordnet:\n",
    "            synonyms = wordnet[word]\n",
    "            if synonyms:\n",
    "                words[i] = random.choice(synonyms)\n",
    "                break\n",
    "    \n",
    "    augmented_text = \" \".join(words)\n",
    "    augmented_text = replace_placeholders_with_entities(augmented_text, entities)\n",
    "    \n",
    "    return augmented_text\n",
    "\n",
    "input_file = \"../data/augment/last_back_train.csv\"\n",
    "output_file = \"../data/augment/back_kwn37500_train.csv\"\n",
    "augmented_data = []\n",
    "\n",
    "with open(input_file, encoding='utf-8') as csvfile:\n",
    "    reader = list(csv.DictReader(csvfile))  # ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ê¸° ìœ„í•´ listë¡œ ë³€í™˜\n",
    "    original_data_len = len(reader)\n",
    "    target_augment_count = 37500 - original_data_len  # ì¦ê°•ì„ ì›í•˜ëŠ” ëª©í‘œ ìˆ˜\n",
    "    \n",
    "    # ê¸°ì¡´ ë°ì´í„°ë¥¼ augmented_dataì— ì¶”ê°€\n",
    "    for row in reader:\n",
    "        augmented_data.append(row)\n",
    "    \n",
    "    # ë°ì´í„° ì¦ê°• ì‹œì‘\n",
    "    while len(augmented_data) < 37500:\n",
    "        for row in reader:\n",
    "            # ëŒ€í™”ì™€ ìš”ì•½ ì¦ê°•\n",
    "            augmented_dialogue = augment_text(row['dialogue'], wordnet)\n",
    "            # augmented_summary = augment_text(row['summary'], wordnet)\n",
    "            augmented_row = {\n",
    "                'fname': row['fname'],\n",
    "                'dialogue': augmented_dialogue,\n",
    "                'summary': row['summary'],\n",
    "                'topic': row['topic']\n",
    "            }\n",
    "            \n",
    "            # ì¦ê°•ëœ ë°ì´í„° ì¶”ê°€\n",
    "            augmented_data.append(augmented_row)\n",
    "            \n",
    "            # ëª©í‘œ ë°ì´í„° ìˆ˜ì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸\n",
    "            if len(augmented_data) >= 37500:\n",
    "                break\n",
    "\n",
    "# ì¦ê°•ëœ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "with open(output_file, mode='w', encoding='utf-8', newline='') as f:\n",
    "    fieldnames = ['fname', 'dialogue', 'summary', 'topic']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in augmented_data[:37500]:  # ìµœëŒ€ 50,000ê°œ ë°ì´í„°ë§Œ ì €ì¥\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>#Person1#: ì•ˆë…•í•˜ì„¸ìš”, ìŠ¤ë¯¸ìŠ¤ì”¨. ì €ëŠ” í˜¸í‚¨ìŠ¤ ì˜ì‚¬ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì™œ ì˜¤ì…¨ë‚˜...</td>\n",
       "      <td>ìŠ¤ë¯¸ìŠ¤ì”¨ê°€ ê±´ê°•ê²€ì§„ì„ ë°›ê³  ìˆê³ , í˜¸í‚¨ìŠ¤ ì˜ì‚¬ëŠ” ë§¤ë…„ ê±´ê°•ê²€ì§„ì„ ë°›ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆ...</td>\n",
       "      <td>ê±´ê°•ê²€ì§„ ë°›ê¸°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>#Person1#: ì•ˆë…•í•˜ì„¸ìš”, íŒŒì»¤ ë¶€ì¸, ì–´ë–»ê²Œ ì§€ë‚´ì…¨ë‚˜ìš”?\\n#Person2#...</td>\n",
       "      <td>íŒŒì»¤ ë¶€ì¸ì´ ë¦¬í‚¤ë¥¼ ë°ë¦¬ê³  ë°±ì‹  ì ‘ì¢…ì„ í•˜ëŸ¬ ê°”ë‹¤. í”¼í„°ìŠ¤ ë°•ì‚¬ëŠ” ê¸°ë¡ì„ í™•ì¸í•œ í›„...</td>\n",
       "      <td>ë°±ì‹ </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>#Person1#: ì‹¤ë¡€í•©ë‹ˆë‹¤, ì—´ì‡  í•œ ë¬¶ìŒ ë³´ì…¨ë‚˜ìš”?\\n#Person2#: ì–´ë–¤...</td>\n",
       "      <td>#Person1#ì€ ì—´ì‡  í•œ ë¬¶ìŒì„ ì°¾ê³  ìˆê³ , ê·¸ê²ƒì„ ì°¾ê¸° ìœ„í•´ #Person2#...</td>\n",
       "      <td>ì—´ì‡  ì°¾ê¸°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>#Person1#: ì™œ ë„ˆëŠ” ì—¬ìì¹œêµ¬ê°€ ìˆë‹¤ëŠ” ê±¸ ë§í•´ì£¼ì§€ ì•Šì•˜ì–´?\\n#Person...</td>\n",
       "      <td>#Person1#ì€ #Person2#ê°€ ì—¬ìì¹œêµ¬ê°€ ìˆê³  ê·¸ë…€ì™€ ê²°í˜¼í•  ê²ƒì´ë¼ëŠ” ì‚¬ì‹¤...</td>\n",
       "      <td>ì—¬ìì¹œêµ¬ê°€ ìˆë‹¤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>#Person1#: ì•ˆë…•, ìˆ™ë…€ë¶„ë“¤! ì˜¤ëŠ˜ ë°¤ ë‹¹ì‹ ë“¤ì€ ì •ë§ ë©‹ì ¸ ë³´ì—¬. ì´ ì¶¤ì„ ...</td>\n",
       "      <td>ë§ë¦­ì´ ë‹ˆí‚¤ì—ê²Œ ì¶¤ì„ ìš”ì²­í•œë‹¤. ë§ë¦­ì´ ë°œì„ ë°ŸëŠ” ê²ƒì„ ì‹ ê²½ ì“°ì§€ ì•ŠëŠ”ë‹¤ë©´ ë‹ˆí‚¤ëŠ” ...</td>\n",
       "      <td>ëŒ„ìŠ¤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>train_7710</td>\n",
       "      <td>#Person1#: ì´ì œ, ì„ ì”¨, ì‚¬ê³ ë¥¼ ëª…í™•í•˜ê²Œ ë³´ì…¨ë‚˜ìš”? #Person2#: ì•„...</td>\n",
       "      <td>ì„ ì”¨ëŠ” ì‚¬ê³ ë¥¼ ëª©ê²©í•˜ê³  #Person1#ì—ê²Œ ìì„¸í•œ ì •ë³´ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤.</td>\n",
       "      <td>ì‚¬ê³  ëª©ê²©ì</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>train_7711</td>\n",
       "      <td>#Person1#: ì „ì‚° ì‚¬ìš©ë²•ì„ ë°°ìš°ê¸° ì‹œì‘í•´ë´…ì‹œë‹¤, í¼ë¸”ëŸ¬. ì»´í“¨í„°ì—ëŠ” í•˜ë“œì›¨ì–´...</td>\n",
       "      <td>#Person1#ê³¼ í¼ë¸”ëŸ¬ëŠ” ì»´í“¨í„° ì‚¬ìš©ë²•ì„ ê³µë¶€í•˜ê³  ìˆìœ¼ë©°, #Person1#ì€ ...</td>\n",
       "      <td>ì»´í“¨í„°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>train_7712</td>\n",
       "      <td>#Person1#: ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤, ë„ì™€ë“œë¦´ ì¼ì´ ìˆë‚˜ìš”? #Person2#: ì•„...</td>\n",
       "      <td>êµ¬ì°Œì™€ ë©”ì´ê°€ ì˜·ì„ ì‚¬ëŸ¬ ì‡¼í•‘ì„ ê°”ìŠµë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ë©”ì´ê°€ ë¬´ì‹¬í•˜ê²Œ í–‰ë™í•˜ì§€ë§Œ, êµ¬...</td>\n",
       "      <td>ì˜· êµ¬ë§¤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>train_7713</td>\n",
       "      <td>#Person1#: ì¹œêµ¬ì˜ ê²°í˜¼ì‹ì„ íí•´ ì–´ë–¤ ì²œì„ ê³¨ë¼ ìŠˆíŠ¸ë¥¼ ë§Œë“¤ì–´ ì£¼ì‹¤ ìˆ˜ ìˆ...</td>\n",
       "      <td>#Person1#ì€ ì¹œêµ¬ì˜ ê²°í˜¼ì‹ì„ ìœ„í•´ ê°ˆìƒ‰ ì¤„ë¬´ëŠ¬ ìŠˆíŠ¸ë¥¼ ë§Œë“¤ê³  ì‹¶ì–´í•©ë‹ˆë‹¤. #...</td>\n",
       "      <td>ì˜· ì„ íƒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>train_7714</td>\n",
       "      <td>#Person1#: ì•ˆë…•í•˜ì„¸ìš”, ì—˜ë¦¬ìë² ìŠ¤ë‹˜. ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”? #Person2#:...</td>\n",
       "      <td>ì—˜ë¦¬ìë² ìŠ¤ëŠ” ê·¹ì¥ì—ì„œ ëŒì•„ì˜¤ëŠ” ê¸¸ì— ì§€ê°‘ì„ ìƒì–´ë²„ë ¸ë‹¤ëŠ” ì‚¬ì‹¤ë¡œ ê±±ì •í•˜ê³  ìˆë‹¤. #P...</td>\n",
       "      <td>ì§€ê°‘ ìƒì–´ë²„ë¦¼</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            fname                                           dialogue  \\\n",
       "0         train_0  #Person1#: ì•ˆë…•í•˜ì„¸ìš”, ìŠ¤ë¯¸ìŠ¤ì”¨. ì €ëŠ” í˜¸í‚¨ìŠ¤ ì˜ì‚¬ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì™œ ì˜¤ì…¨ë‚˜...   \n",
       "1         train_1  #Person1#: ì•ˆë…•í•˜ì„¸ìš”, íŒŒì»¤ ë¶€ì¸, ì–´ë–»ê²Œ ì§€ë‚´ì…¨ë‚˜ìš”?\\n#Person2#...   \n",
       "2         train_2  #Person1#: ì‹¤ë¡€í•©ë‹ˆë‹¤, ì—´ì‡  í•œ ë¬¶ìŒ ë³´ì…¨ë‚˜ìš”?\\n#Person2#: ì–´ë–¤...   \n",
       "3         train_3  #Person1#: ì™œ ë„ˆëŠ” ì—¬ìì¹œêµ¬ê°€ ìˆë‹¤ëŠ” ê±¸ ë§í•´ì£¼ì§€ ì•Šì•˜ì–´?\\n#Person...   \n",
       "4         train_4  #Person1#: ì•ˆë…•, ìˆ™ë…€ë¶„ë“¤! ì˜¤ëŠ˜ ë°¤ ë‹¹ì‹ ë“¤ì€ ì •ë§ ë©‹ì ¸ ë³´ì—¬. ì´ ì¶¤ì„ ...   \n",
       "...           ...                                                ...   \n",
       "69995  train_7710  #Person1#: ì´ì œ, ì„ ì”¨, ì‚¬ê³ ë¥¼ ëª…í™•í•˜ê²Œ ë³´ì…¨ë‚˜ìš”? #Person2#: ì•„...   \n",
       "69996  train_7711  #Person1#: ì „ì‚° ì‚¬ìš©ë²•ì„ ë°°ìš°ê¸° ì‹œì‘í•´ë´…ì‹œë‹¤, í¼ë¸”ëŸ¬. ì»´í“¨í„°ì—ëŠ” í•˜ë“œì›¨ì–´...   \n",
       "69997  train_7712  #Person1#: ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤, ë„ì™€ë“œë¦´ ì¼ì´ ìˆë‚˜ìš”? #Person2#: ì•„...   \n",
       "69998  train_7713  #Person1#: ì¹œêµ¬ì˜ ê²°í˜¼ì‹ì„ íí•´ ì–´ë–¤ ì²œì„ ê³¨ë¼ ìŠˆíŠ¸ë¥¼ ë§Œë“¤ì–´ ì£¼ì‹¤ ìˆ˜ ìˆ...   \n",
       "69999  train_7714  #Person1#: ì•ˆë…•í•˜ì„¸ìš”, ì—˜ë¦¬ìë² ìŠ¤ë‹˜. ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”? #Person2#:...   \n",
       "\n",
       "                                                 summary     topic  \n",
       "0      ìŠ¤ë¯¸ìŠ¤ì”¨ê°€ ê±´ê°•ê²€ì§„ì„ ë°›ê³  ìˆê³ , í˜¸í‚¨ìŠ¤ ì˜ì‚¬ëŠ” ë§¤ë…„ ê±´ê°•ê²€ì§„ì„ ë°›ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆ...   ê±´ê°•ê²€ì§„ ë°›ê¸°  \n",
       "1      íŒŒì»¤ ë¶€ì¸ì´ ë¦¬í‚¤ë¥¼ ë°ë¦¬ê³  ë°±ì‹  ì ‘ì¢…ì„ í•˜ëŸ¬ ê°”ë‹¤. í”¼í„°ìŠ¤ ë°•ì‚¬ëŠ” ê¸°ë¡ì„ í™•ì¸í•œ í›„...        ë°±ì‹   \n",
       "2      #Person1#ì€ ì—´ì‡  í•œ ë¬¶ìŒì„ ì°¾ê³  ìˆê³ , ê·¸ê²ƒì„ ì°¾ê¸° ìœ„í•´ #Person2#...     ì—´ì‡  ì°¾ê¸°  \n",
       "3      #Person1#ì€ #Person2#ê°€ ì—¬ìì¹œêµ¬ê°€ ìˆê³  ê·¸ë…€ì™€ ê²°í˜¼í•  ê²ƒì´ë¼ëŠ” ì‚¬ì‹¤...  ì—¬ìì¹œêµ¬ê°€ ìˆë‹¤  \n",
       "4      ë§ë¦­ì´ ë‹ˆí‚¤ì—ê²Œ ì¶¤ì„ ìš”ì²­í•œë‹¤. ë§ë¦­ì´ ë°œì„ ë°ŸëŠ” ê²ƒì„ ì‹ ê²½ ì“°ì§€ ì•ŠëŠ”ë‹¤ë©´ ë‹ˆí‚¤ëŠ” ...        ëŒ„ìŠ¤  \n",
       "...                                                  ...       ...  \n",
       "69995            ì„ ì”¨ëŠ” ì‚¬ê³ ë¥¼ ëª©ê²©í•˜ê³  #Person1#ì—ê²Œ ìì„¸í•œ ì •ë³´ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤.    ì‚¬ê³  ëª©ê²©ì  \n",
       "69996  #Person1#ê³¼ í¼ë¸”ëŸ¬ëŠ” ì»´í“¨í„° ì‚¬ìš©ë²•ì„ ê³µë¶€í•˜ê³  ìˆìœ¼ë©°, #Person1#ì€ ...       ì»´í“¨í„°  \n",
       "69997  êµ¬ì°Œì™€ ë©”ì´ê°€ ì˜·ì„ ì‚¬ëŸ¬ ì‡¼í•‘ì„ ê°”ìŠµë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ë©”ì´ê°€ ë¬´ì‹¬í•˜ê²Œ í–‰ë™í•˜ì§€ë§Œ, êµ¬...      ì˜· êµ¬ë§¤  \n",
       "69998  #Person1#ì€ ì¹œêµ¬ì˜ ê²°í˜¼ì‹ì„ ìœ„í•´ ê°ˆìƒ‰ ì¤„ë¬´ëŠ¬ ìŠˆíŠ¸ë¥¼ ë§Œë“¤ê³  ì‹¶ì–´í•©ë‹ˆë‹¤. #...      ì˜· ì„ íƒ  \n",
       "69999  ì—˜ë¦¬ìë² ìŠ¤ëŠ” ê·¹ì¥ì—ì„œ ëŒì•„ì˜¤ëŠ” ê¸¸ì— ì§€ê°‘ì„ ìƒì–´ë²„ë ¸ë‹¤ëŠ” ì‚¬ì‹¤ë¡œ ê±±ì •í•˜ê³  ìˆë‹¤. #P...   ì§€ê°‘ ìƒì–´ë²„ë¦¼  \n",
       "\n",
       "[70000 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new = pd.read_csv(\"../data/augment/kwn70000_train.csv\")\n",
    "train_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Only kwn augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "\n",
    "# WordNet ë¡œë“œ\n",
    "with open(\"wordnet.pickle\", \"rb\") as f:\n",
    "    wordnet = pickle.load(f)\n",
    "\n",
    "# ê¸°ì¡´ í•¨ìˆ˜ë“¤...\n",
    "def replace_entities_with_placeholders(text, placeholder_prefix=\"#REPL#\"):\n",
    "    entities = re.findall(r\"#.+?#\", text)\n",
    "    for i, entity in enumerate(entities):\n",
    "        text = text.replace(entity, f\"{placeholder_prefix}{i}#\", 1)\n",
    "    return text, entities\n",
    "\n",
    "def replace_placeholders_with_entities(text, entities, placeholder_prefix=\"#REPL#\"):\n",
    "    for i, entity in enumerate(entities):\n",
    "        text = text.replace(f\"{placeholder_prefix}{i}#\", entity, 1)\n",
    "    return text\n",
    "\n",
    "def augment_text(text, wordnet):\n",
    "    text, entities = replace_entities_with_placeholders(text)\n",
    "    \n",
    "    words = text.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if word in wordnet:\n",
    "            synonyms = wordnet[word]\n",
    "            if synonyms:\n",
    "                words[i] = random.choice(synonyms)\n",
    "                break\n",
    "    \n",
    "    augmented_text = \" \".join(words)\n",
    "    augmented_text = replace_placeholders_with_entities(augmented_text, entities)\n",
    "    \n",
    "    return augmented_text\n",
    "\n",
    "input_file = \"../data/augment/back_result_train.csv\"\n",
    "output_file = \"../data/augment/back_kwn10000_augmented.csv\"  # ë³€ê²½ëœ íŒŒì¼ëª…\n",
    "new_augmented_data = []  # ìƒˆë¡­ê²Œ ìƒì„±ëœ ë°ì´í„°ë§Œì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "with open(input_file, encoding='utf-8') as csvfile:\n",
    "    reader = list(csv.DictReader(csvfile))\n",
    "    original_data_len = len(reader)\n",
    "    target_augment_count = 10000  # ë³€ê²½: ìƒˆë¡­ê²Œ ìƒì„±í•  ë°ì´í„°ì˜ ëª©í‘œ ìˆ˜\n",
    "    \n",
    "    # ë°ì´í„° ì¦ê°• ì‹œì‘\n",
    "    while len(new_augmented_data) < target_augment_count:  # ë³€ê²½: ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ì˜ í¬ê¸°ë¥¼ í™•ì¸\n",
    "        for row in reader:\n",
    "            # ëŒ€í™”ì™€ ìš”ì•½ ì¦ê°• (ìš”ì•½ì€ ì¦ê°•í•˜ì§€ ì•Šê¸°ë¡œ í•¨)\n",
    "            augmented_dialogue = augment_text(row['dialogue'], wordnet)\n",
    "            augmented_row = {\n",
    "                'fname': row['fname'],\n",
    "                'dialogue': augmented_dialogue,\n",
    "                'summary': row['summary'],  # ì›ë³¸ ìš”ì•½ ì‚¬ìš©\n",
    "                'topic': row['topic']\n",
    "            }\n",
    "            \n",
    "            # ì¦ê°•ëœ ë°ì´í„° ì¶”ê°€\n",
    "            new_augmented_data.append(augmented_row)\n",
    "            \n",
    "            # ëª©í‘œ ë°ì´í„° ìˆ˜ì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸\n",
    "            if len(new_augmented_data) >= target_augment_count:\n",
    "                break\n",
    "\n",
    "# ì¦ê°•ëœ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "with open(output_file, mode='w', encoding='utf-8', newline='') as f:\n",
    "    fieldnames = ['fname', 'dialogue', 'summary', 'topic']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in new_augmented_data:  # ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ì˜ ë°ì´í„°ë§Œ ì €ì¥\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
